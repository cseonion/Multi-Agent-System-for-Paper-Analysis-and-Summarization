{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test case 1: Single Paper Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 06:54:12 | main                 | INFO     | üöÄ Task Paper Processing Started. Cache directory: cache/20250818_065412/\n",
      "2025-08-18 06:54:12 | main                 | INFO     | üìã Process logs are being recorded\n",
      "2025-08-18 06:54:12 | main                 | INFO     | üìÇ Input path: ['test/case1/deepseek r1.pdf']\n",
      "2025-08-18 06:54:12 | main                 | INFO     | üìã Log file: cache/20250818_065412/process.log\n",
      "2025-08-18 06:54:12 | src.tracking         | INFO     | üìä Ïã§Ìñâ Ï∂îÏ†Å ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî: cache/20250818_065412/\n",
      "2025-08-18 06:54:12 | src.graph            | INFO     | üîß Send API Í∏∞Î∞ò Î≥ëÎ†¨ ÏõåÌÅ¨ÌîåÎ°úÏö∞ Íµ¨ÏÑ± ÏãúÏûë...\n",
      "2025-08-18 06:54:12 | src.graph            | INFO     | ‚úÖ Send API Í∏∞Î∞ò Î≥ëÎ†¨ ÏõåÌÅ¨ÌîåÎ°úÏö∞ Íµ¨ÏÑ± ÏôÑÎ£å\n",
      "2025-08-18 06:54:12 | main                 | INFO     | ‚öôÔ∏è  Starting workflow execution...\n",
      "2025-08-18 06:54:12 | src.load_doc         | INFO     | üîÑ Document extraction process started\n",
      "2025-08-18 06:54:12 | src.load_doc         | INFO     | üìÇ Processing paths: ['test/case1/deepseek r1.pdf']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2bbaa90a054292b3977c1e64857a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 06:54:20 | src.load_doc         | INFO     | üìÑ Loaded 70 documents\n",
      "2025-08-18 06:54:20 | src.load_doc         | INFO     | üìä Found papers: ['DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning']\n",
      "2025-08-18 06:54:20 | src.load_doc         | INFO     | üîß Starting vectorstore creation\n",
      "/Users/onion/Documents/·Ñé·Ö±·Ñã·Ö•·Ü∏/·Ñå·Öµ·Ñã·ÖØ·Ü´/·Ñã·Öµ·Ü´·Ñê·Ö•·Ü´_·ÑÉ·Öµ·Ü∏·Ñã·Ö©·Ñê·Ö©/·Ñã·Öµ·Ü∏·Ñâ·Ö°·ÑÄ·Ö™·Ñå·Ö¶/code/v3(final)/task_paper/src/load_doc.py:206: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(\n",
      "2025-08-18 06:54:25 | sentence_transformers.SentenceTransformer | INFO     | Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-08-18 06:54:28 | src.load_doc         | INFO     | ‚úÖ Embedder initialized with model: all-MiniLM-L6-v2\n",
      "2025-08-18 06:54:28 | src.load_doc         | INFO     | üîÑ Creating vectorstore for: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n",
      "2025-08-18 06:54:29 | faiss.loader         | INFO     | Loading faiss.\n",
      "2025-08-18 06:54:29 | faiss.loader         | INFO     | Successfully loaded faiss.\n",
      "2025-08-18 06:54:29 | faiss                | INFO     | Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
      "2025-08-18 06:54:29 | src.load_doc         | INFO     | üíæ Vectorstore for 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning' saved to cache/20250818_065412/DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning/vectorstore/\n",
      "2025-08-18 06:54:29 | src.load_doc         | INFO     | ‚úÖ All vectorstores created successfully (1 papers)\n",
      "2025-08-18 06:54:29 | src.load_doc         | INFO     | ‚úÖ Document extraction completed successfully\n",
      "2025-08-18 06:54:29 | src.graph            | INFO     | üöÄ 1Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú Î≥ëÎ†¨ ÏöîÏïΩ ÏûëÏóÖ ÏãúÏûë...\n",
      "2025-08-18 06:54:29 | src.graph            | INFO     |   üì§ 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning' ‚Üí Summary SubgraphÎ°ú Ï†ÑÏÜ° (ÏÑπÏÖò 20Í∞ú)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 'DeepSeek-R1: Incentivizing Reasoning Capability in...' ÏÑπÏÖò Ï∂îÏ∂ú ÏôÑÎ£å (20Í∞ú ÏÑπÏÖò)\n",
      "‚è≥ 'DeepSeek-R1: Incentivizing Reasoning Capability in...' ÏÑπÏÖò ÏöîÏïΩ Ï§ë... (20Í∞ú ÏÑπÏÖò)\n",
      "      üìë Ï†ÑÏ≤¥ ÏöîÏïΩ Ïù∏Îç±Ïä§ Ï†ÄÏû•Îê®: 00 Index_All_Summaries.txt\n",
      "‚úÖ 'DeepSeek-R1: Incentivizing Reasoning Capability in...' ÏÑπÏÖòÎ≥Ñ ÏöîÏïΩ ÏôÑÎ£å (20Í∞ú ÏÑπÏÖò)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 06:56:32 | agents.domain_agent  | INFO     | üîç ÎèÑÎ©îÏù∏ Î∂ÑÏÑù ÏãúÏûë... Î∂ÑÏÑù ÎåÄÏÉÅ: 1Í∞ú ÎÖºÎ¨∏\n",
      "2025-08-18 06:56:32 | agents.domain_agent  | INFO     | üìÑ Î∂ÑÏÑù ÎÖºÎ¨∏: ['DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning']\n",
      "2025-08-18 06:56:32 | agents.domain_agent  | INFO     | üìë ÎèÑÎ©îÏù∏ Î∂ÑÏÑù Ï§ë...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      üíæ ÏµúÏ¢Ö ÏöîÏïΩ Ï†ÄÏû•Îê®: Final_Summary.txt\n",
      "‚úÖ 'DeepSeek-R1: Incentivizing Reasoning Capability in...' ÏµúÏ¢Ö ÏöîÏïΩ ÏôÑÎ£å\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 06:56:33 | agents.domain_agent  | INFO     |    ‚úÖ DeepSeek-R1: Incentivizing Rea... Î∂ÑÏïº: {'main_field': ['Computer Science'], 'sub_field': ['Natural Language Processing', 'Reinforcement Learning', 'Large Language Models', 'Machine Learning']}\n",
      "2025-08-18 06:56:33 | agents.domain_agent  | INFO     | ‚úÖ ÎèÑÎ©îÏù∏ Î∂ÑÏÑù ÏôÑÎ£å!\n",
      "2025-08-18 06:56:33 | agents.analysis_plan_router | INFO     | üéØ Î∂ÑÏÑù Í≥ÑÌöç ÏÉùÏÑ± Ï§ë... ÎÖºÎ¨∏ Ïàò: 1\n",
      "2025-08-18 06:56:33 | agents.analysis_plan_router | INFO     | üìñ Îã®Ïùº ÎÖºÎ¨∏ Î∂ÑÏÑù: 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning'\n",
      "2025-08-18 06:56:34 | agents.analysis_plan_router | INFO     | ‚úÖ Î∂ÑÏÑù Í≥ÑÌöç Í≤∞Ï†ï: Îã®Ïùº ÎèÑÎ©îÏù∏ (single)\n",
      "2025-08-18 06:56:34 | src.graph            | INFO     | üß≠ Î∂ÑÏÑù ÌîåÎûú ÌôïÏ†ï: single\n",
      "2025-08-18 06:56:34 | agents.tools.python_repl | INFO     | Python REPL Tool enabled\n",
      "2025-08-18 06:56:34 | agents.write_agent   | INFO     | üîß Python REPL Tool ÌôúÏÑ±Ìôî\n",
      "2025-08-18 06:56:34 | agents.write_agent   | INFO     | üìù Write agent Ïã§Ìñâ (Î∏îÎ°úÍ∑∏ Ïä§ÌÉÄÏùº ÏÉùÏÑ±, Markdown Ï∂úÎ†•)...\n",
      "2025-08-18 06:57:47 | agents.write_agent   | INFO     | ‚úÖ Î∏îÎ°úÍ∑∏ Ìè¨Ïä§Ìä∏ ÏÉùÏÑ± ÏôÑÎ£å\n",
      "2025-08-18 06:57:47 | src.tracking         | INFO     | üéØ Ï†ÑÏ≤¥ ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏôÑÎ£å\n",
      "2025-08-18 06:57:47 | src.tracking         | INFO     | ‚è±Ô∏è  Ï¥ù Ïã§Ìñâ ÏãúÍ∞Ñ: 215.80Ï¥à\n",
      "2025-08-18 06:57:47 | src.tracking         | INFO     | üí∞ Ï¥ù ÌÜ†ÌÅ∞ ÏÇ¨Ïö©: 0Í∞ú\n",
      "2025-08-18 06:57:47 | src.tracking         | INFO     | üí≥ Ï¥ù ÏòàÏÉÅ ÎπÑÏö©: $0.0000\n",
      "2025-08-18 06:57:47 | main                 | INFO     | üìä Ïã§Ìñâ Ï∂îÏ†Å ÏôÑÎ£å - ÏóêÏù¥Ï†ÑÌä∏ 0Í∞ú, Ï¥ù ÏãúÍ∞Ñ 215.80Ï¥à\n",
      "2025-08-18 06:57:47 | main                 | INFO     | analysis_report ÏóÜÏùå - Ï†ÄÏû• Í±¥ÎÑàÎúÄ\n",
      "2025-08-18 06:57:47 | main                 | INFO     | üíæ Ï†ÄÏû•Îê®: cache/20250818_065412/DeepSeek-R1_Incentivizing_Reasoning_Capability_in_LLMs_via_Reinforcement_Learnin_final_report_20250818_065747.md\n",
      "2025-08-18 06:57:47 | main                 | INFO     | ‚úÖ Task Paper Processing Completed Successfully\n"
     ]
    }
   ],
   "source": [
    "from main import run\n",
    "\n",
    "pdf_path = [\"test/case1/deepseek r1.pdf\"]\n",
    "result, app, workflow = run(pdf_path, print_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': ['test/case1/deepseek r1.pdf'],\n",
       " 'paper_title': ['DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning'],\n",
       " 'paper_sections': {'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning': [('1.',\n",
       "    'Introduction'),\n",
       "   ('1.2.', 'Summary of Evaluation Results'),\n",
       "   ('2.1.', 'Overview'),\n",
       "   ('2.2.', 'DeepSeek-R1-Zero: Reinforcement Learning on the Base Model'),\n",
       "   ('2.2.1.', 'Reinforcement Learning Algorithm'),\n",
       "   ('2.2.2.', 'Reward Modeling'),\n",
       "   ('2.2.3.', 'Training Template'),\n",
       "   ('2.2.4.',\n",
       "    'Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero'),\n",
       "   ('2.3.', 'DeepSeek-R1: Reinforcement Learning with Cold Start'),\n",
       "   ('2.3.1.', 'Cold Start'),\n",
       "   ('2.3.2.', 'Reasoning-oriented Reinforcement Learning'),\n",
       "   ('2.3.3.', 'Rejection Sampling and Supervised Fine-Tuning'),\n",
       "   ('2.3.4.', 'Reinforcement Learning for all Scenarios'),\n",
       "   ('2.4.', 'Distillation: Empower Small Models with Reasoning Capability'),\n",
       "   ('3.', 'Experiment'),\n",
       "   ('3.1.', 'DeepSeek-R1 Evaluation'),\n",
       "   ('3.2.', 'Distilled Model Evaluation'),\n",
       "   ('4.1.', 'Distillation v.s. Reinforcement Learning'),\n",
       "   ('4.2.', 'Unsuccessful Attempts'),\n",
       "   ('5.', 'Conclusion, Limitations, and Future Work')]},\n",
       " 'vectorstores': {'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning': <langchain_community.vectorstores.faiss.FAISS at 0x179f8edd0>},\n",
       " 'vectorstores_path': {'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning': 'cache/20250818_065412/DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning/vectorstore/'},\n",
       " 'cache_dir': 'cache/20250818_065412/',\n",
       " 'section_summaries': {'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning': ['The Introduction outlines the rapid advancements in Large Language Models (LLMs) and highlights the emerging importance of post-training techniques to enhance reasoning capabilities efficiently. It situates the work within ongoing efforts to improve reasoning through methods like inference-time scaling, process-based reward models, reinforcement learning (RL), and search algorithms, noting that none have yet matched the reasoning performance of OpenAI‚Äôs o1 series. The paper‚Äôs core contribution is pioneering the use of pure RL‚Äîwithout supervised fine-tuning (SFT)‚Äîto incentivize reasoning in LLMs, demonstrated through the development of DeepSeek-R1-Zero. This model exhibits advanced reasoning behaviors and achieves benchmark performance comparable to OpenAI-o1-0912. To address issues such as readability and language mixing, the authors introduce DeepSeek-R1, which integrates a multi-stage training pipeline combining cold-start SFT, RL, and further fine-tuning with newly generated and supervised data, ultimately matching OpenAI-o1-1217‚Äôs performance. The study further shows that reasoning capabilities learned by larger models can be effectively distilled into smaller dense models, yielding state-of-the-art results on reasoning benchmarks and outperforming existing open-source models. By open-sourcing these distilled models and their API, the work provides valuable resources for the research community, advancing the theme of enhancing LLM reasoning through innovative RL-driven methodologies without reliance on extensive supervised data.',\n",
       "   'Section 1.2 presents a comprehensive evaluation of DeepSeek-R1 across diverse reasoning, knowledge, and general language tasks, reinforcing the paper‚Äôs central theme of enhancing LLM reasoning capabilities via reinforcement learning. On reasoning benchmarks, DeepSeek-R1 attains a Pass@1 score of 79.8% on AIME 2024, slightly surpassing OpenAI-o1-1217, and achieves an impressive 97.3% on MATH-500, matching OpenAI-o1-1217 while outperforming other models. Its coding proficiency is demonstrated by a 2,029 Elo rating on Codeforces, outperforming over 96% of human competitors, and it shows incremental gains over its predecessor DeepSeek-V3 on engineering tasks, indicating practical utility for developers. In knowledge-intensive evaluations such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 significantly outperforms DeepSeek-V3, with scores of 90.8%, 84.0%, and 71.5% respectively, closely trailing OpenAI-o1-1217 and surpassing other closed-source models, thereby underscoring its competitive edge in educational and factual domains. Additionally, DeepSeek-R1 excels in creative writing, question answering, editing, and summarization, achieving high win-rates on AlpacaEval 2.0 (87.6%) and Are-naHard (92.3%), and demonstrates superior long-context understanding compared to DeepSeek-V3. These results collectively validate the effectiveness of the multi-stage RL-driven training pipeline introduced earlier, confirming that DeepSeek-R1 not only matches but in several areas surpasses leading models, thereby advancing the paper‚Äôs goal of incentivizing and realizing enhanced reasoning capabilities in LLMs without reliance on extensive supervised fine-tuning.',\n",
       "   'Section 2.1 builds upon the demonstrated effectiveness of DeepSeek-R1 by outlining the core methodological innovation of the paper: leveraging large-scale reinforcement learning (RL) to enhance reasoning capabilities in large language models without heavy dependence on supervised fine-tuning (SFT). This section introduces two key variants‚ÄîDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and DeepSeek-R1, which begins RL training from a checkpoint already fine-tuned on thousands of long Chain-of-Thought (CoT) examples. Additionally, the section highlights the strategy of distilling the enhanced reasoning abilities from the large DeepSeek-R1 model into smaller, dense models, aiming to broaden practical applicability. By emphasizing that substantial reasoning improvements can be achieved even without extensive supervised data, this overview reinforces the paper‚Äôs central theme of incentivizing reasoning through RL, complementing the prior section‚Äôs empirical validation of DeepSeek-R1‚Äôs superior performance across diverse reasoning and knowledge tasks.',\n",
       "   'Section 2.2 delves deeper into the DeepSeek-R1-Zero variant, emphasizing its novel approach of employing reinforcement learning (RL) directly on the base large language model without relying on any supervised data. Building on the prior section‚Äôs introduction of this variant, the authors highlight the motivation to overcome the limitations posed by the time-consuming collection of supervised datasets, which previous works depended on heavily. This section outlines the RL algorithm employed to enable the model‚Äôs self-evolution in reasoning capabilities purely through interaction and feedback, marking a significant methodological shift toward unsupervised enhancement. Preliminary results presented here demonstrate promising improvements in reasoning performance achieved solely via this RL-driven process, reinforcing the paper‚Äôs overarching theme that reasoning abilities in LLMs can be effectively incentivized and developed without extensive supervised fine-tuning. By showcasing the feasibility and potential of zero-shot RL training, this section strengthens the argument for scalable, data-efficient approaches to advancing LLM reasoning.',\n",
       "   'Section 2.2.1 introduces the specific reinforcement learning algorithm underpinning DeepSeek-R1-Zero‚Äôs training: Group Relative Policy Optimization (GRPO). This method is strategically chosen to reduce the substantial computational costs typically associated with RL, particularly by eliminating the need for a separate critic model that matches the policy model in size. Instead, GRPO estimates the baseline using group-level reward scores, which streamlines the optimization process. Concretely, for each reasoning question, GRPO samples a group of candidate outputs from the current policy and computes advantages based on the relative rewards within this group. The policy is then updated to maximize an objective function that incorporates these group-relative advantages, guided by hyperparameters controlling the update dynamics. This approach aligns seamlessly with the paper‚Äôs overarching goal of efficiently incentivizing reasoning capabilities in large language models without supervised data. By leveraging GRPO, the authors provide a practical and cost-effective RL framework that enables DeepSeek-R1-Zero to self-improve its reasoning through interaction and feedback, reinforcing the feasibility of scalable, unsupervised reasoning enhancement in LLMs.',\n",
       "   'Section 2.2.2 builds upon the reinforcement learning framework established in the previous section by detailing the reward modeling strategy critical to guiding DeepSeek-R1-Zero‚Äôs training. The authors implement a rule-based reward system composed of two main components: accuracy rewards and format rewards. Accuracy rewards provide a clear, verifiable training signal by checking the correctness of responses‚Äîsuch as verifying final answers in math problems through specified formatting or using compilers to validate LeetCode solutions against test cases. Complementing this, format rewards enforce structured reasoning by requiring the model to explicitly demarcate its thought process within designated tags (‚Äú<think>‚Äù and ‚Äú</think>‚Äù), thereby promoting transparency and interpretability in reasoning steps. Notably, the authors deliberately avoid neural reward models for both outcome and process evaluation due to concerns about reward hacking and the additional computational burden of retraining these models, which would complicate the training pipeline. This carefully designed reward modeling aligns with the paper‚Äôs overarching objective of efficiently incentivizing reasoning in large language models through reinforcement learning, ensuring that the optimization direction remains reliable and resource-conscious while fostering both accuracy and explicit reasoning behavior.',\n",
       "   'Section 2.2.3 advances the training methodology of DeepSeek-R1-Zero by introducing a carefully crafted training template that structures the model‚Äôs output into two sequential components: a reasoning process followed by the final answer, as illustrated in Table 1. This template serves as a minimal yet effective scaffold, deliberately avoiding any content-specific biases or prescriptive reasoning styles‚Äîsuch as enforcing reflective thinking or particular problem-solving heuristics‚Äîto allow the model‚Äôs inherent reasoning capabilities to emerge naturally during reinforcement learning. By constraining only the output format rather than the reasoning content, this approach complements the previously described reward modeling strategy, which emphasizes accuracy and explicit reasoning structure without relying on neural reward models. Together, these design choices reinforce the paper‚Äôs central theme of incentivizing transparent and accurate reasoning in large language models through a streamlined, interpretable, and resource-efficient reinforcement learning framework.',\n",
       "   'Section 2.2.4 evaluates the performance and autonomous development of DeepSeek-R1-Zero during reinforcement learning (RL) training, building directly on the structured output format and reward design introduced in the prior section. The model exhibits a marked and steady improvement on the AIME 2024 benchmark, with its average pass@1 score rising dramatically from 15.6% to 71.0%, ultimately matching the performance of OpenAI‚Äôs o1-0912 model without any supervised fine-tuning. This underscores the effectiveness of the RL approach in cultivating robust reasoning capabilities purely through interaction and reward incentives.\\n\\nFurther comparative analysis across diverse reasoning benchmarks confirms that DeepSeek-R1-Zero attains strong generalization and problem-solving skills solely via RL, a significant achievement given the absence of labeled fine-tuning data. The application of majority voting further boosts performance‚Äîfor example, elevating AIME pass@1 accuracy to 86.7%, surpassing the baseline OpenAI model‚Äîhighlighting the model‚Äôs solid foundational reasoning ability and potential for enhancement.\\n\\nA key contribution of this section is the detailed exposition of DeepSeek-R1-Zero‚Äôs self-evolution process. By initiating RL training directly from the base model, the authors isolate and observe the model‚Äôs intrinsic development of reasoning strategies over time. Notably, the model progressively increases its ‚Äúthinking time‚Äù during test-time inference, generating hundreds to thousands of reasoning tokens that enable deeper exploration and refinement of problem-solving approaches. This extended computation fosters emergent sophisticated behaviors such as reflection‚Äîreevaluating prior reasoning steps‚Äîand exploring alternative solution paths, which arise spontaneously from the RL environment rather than explicit programming.\\n\\nThe section also highlights a striking ‚Äúaha moment‚Äù during an intermediate training phase, where DeepSeek-R1-Zero autonomously learns to allocate more cognitive effort by reconsidering its initial approach to problems. This emergent behavior exemplifies the power of RL incentives to unlock advanced reasoning strategies without direct supervision, illustrating a pivotal step toward more autonomous and adaptive large language models.\\n\\nDespite these advances, the authors acknowledge limitations in DeepSeek-R1-Zero‚Äôs outputs, including issues with readability and language mixing, which motivate the subsequent development of DeepSeek-R1. This next iteration aims to enhance the interpretability and community accessibility of reasoning processes by incorporating human-friendly cold-start data within the RL framework.\\n\\nOverall, this section reinforces the paper‚Äôs central thesis that carefully designed reinforcement learning can effectively incentivize and cultivate sophisticated, transparent reasoning capabilities in large language models, enabling them to self-improve and discover complex problem-solving behaviors autonomously.',\n",
       "   'Section 2.3 builds directly on the promising autonomous reasoning improvements demonstrated by DeepSeek-R1-Zero by exploring how incorporating a small amount of high-quality, human-curated data as a cold start can further enhance performance and training efficiency. Motivated by two key questions‚Äîwhether reasoning capabilities and convergence speed can be improved with such data, and how to produce a user-friendly model that generates clear, coherent Chains of Thought (CoT) while maintaining strong generalization‚Äîthe authors propose a structured four-stage training pipeline for DeepSeek-R1. This pipeline strategically integrates cold-start data within the reinforcement learning framework to address the limitations observed in DeepSeek-R1-Zero, particularly regarding output readability and language consistency. By doing so, DeepSeek-R1 aims to combine the strengths of RL-driven autonomous reasoning with the interpretability and accessibility afforded by supervised guidance, advancing the paper‚Äôs overarching goal of incentivizing sophisticated, transparent reasoning in large language models through reinforcement learning.',\n",
       "   'Section 2.3.1, Cold Start, details the methodology for addressing the early instability in reinforcement learning (RL) training observed in DeepSeek-R1-Zero by introducing a carefully constructed cold-start dataset for DeepSeek-R1. Building on the previous section‚Äôs emphasis on integrating high-quality human-curated data to improve reasoning and readability, the authors collect thousands of long Chain-of-Thought (CoT) examples to fine-tune the base model before RL begins. This cold-start data is generated through a combination of few-shot prompting, direct model prompting for detailed reflective answers, harvesting outputs from DeepSeek-R1-Zero, and human post-processing to ensure clarity and coherence. Crucially, the cold-start responses follow a standardized, reader-friendly format‚Äîencapsulating the reasoning process and a concise summary‚Äîaddressing DeepSeek-R1-Zero‚Äôs limitations such as mixed languages and poor formatting. This approach not only enhances output readability but also leverages human priors to improve model performance and training stability. Thus, the cold-start phase serves as a foundational step that aligns with the paper‚Äôs overarching goal of incentivizing transparent and effective reasoning in large language models through a hybrid supervised and RL training paradigm.',\n",
       "   'Section 2.3.2, Reasoning-oriented Reinforcement Learning, builds directly on the cold-start fine-tuning described previously by applying large-scale reinforcement learning (RL) to further enhance DeepSeek-R1‚Äôs reasoning abilities. After initializing the model with high-quality, human-curated Chain-of-Thought (CoT) data, this phase targets reasoning-intensive domains such as coding, mathematics, science, and logic‚Äîareas characterized by well-defined problems and clear solutions. A key challenge identified during RL training is the tendency of CoT outputs to exhibit language mixing, especially when prompts involve multiple languages. To address this, the authors introduce a novel language consistency reward that quantifies the proportion of target language words in the CoT, thereby encouraging linguistic coherence aligned with human preferences for readability. Although ablation studies reveal a slight trade-off in raw task performance due to this alignment, the combined reward‚Äîsumming reasoning accuracy and language consistency‚Äîeffectively balances correctness with clarity. The RL training proceeds until convergence on reasoning tasks, reinforcing the paper‚Äôs central theme of incentivizing transparent, high-quality reasoning in large language models through a carefully designed reward structure that integrates both task accuracy and human-aligned output quality.',\n",
       "   'Section 2.3.3, Rejection Sampling and Supervised Fine-Tuning, extends the reasoning-oriented reinforcement learning (RL) phase by leveraging the converged RL checkpoint to generate a large, diverse supervised fine-tuning (SFT) dataset for subsequent model refinement. Building on the prior focus on reasoning tasks, this stage broadens the training scope to include non-reasoning domains such as writing, role-playing, factual question answering, self-cognition, and translation, thereby enhancing DeepSeek-R1‚Äôs general-purpose capabilities alongside its reasoning proficiency.\\n\\nMethodologically, the authors employ rejection sampling on the RL-trained checkpoint to curate high-quality reasoning trajectories, filtering out outputs with mixed languages, excessive length, or code blocks to maintain clarity and coherence. Unlike the earlier stage that relied solely on rule-based rewards, this expanded dataset incorporates generative reward judgments via DeepSeek-V3, which evaluates model predictions against ground truth to ensure correctness. Approximately 600,000 reasoning-related samples are collected through this process.\\n\\nFor non-reasoning data, the authors reuse portions of the DeepSeek-V3 SFT dataset and generate additional samples using the DeepSeek-V3 pipeline, sometimes prompting for chain-of-thought reasoning when appropriate. Simpler queries receive direct answers without CoT to maintain efficiency. This results in roughly 200,000 non-reasoning samples.\\n\\nThe combined dataset of about 800,000 samples is then used to fine-tune DeepSeek-V3-Base for two epochs, effectively consolidating improvements from RL with broader task coverage. This approach aligns with the paper‚Äôs overarching goal of incentivizing and enhancing reasoning capabilities in large language models while simultaneously improving their versatility and output quality across diverse tasks. By integrating rejection sampling and supervised fine-tuning, the authors ensure that DeepSeek-R1 not only produces accurate and linguistically consistent reasoning but also generalizes well to a wider range of applications.',\n",
       "   'Section 2.3.4 advances the training framework by introducing a secondary reinforcement learning stage designed to holistically align DeepSeek-R1 with human preferences, emphasizing not only enhanced reasoning capabilities but also improved helpfulness and harmlessness. Building on the diverse and high-quality dataset curated through rejection sampling and supervised fine-tuning in the previous section, this stage integrates multiple reward signals tailored to different data types. For reasoning-focused data, the approach follows the DeepSeek-R1-Zero methodology, employing rule-based rewards to reinforce accuracy in math, code, and logical reasoning tasks. For more general, nuanced scenarios, reward models derived from the DeepSeek-V3 pipeline capture complex human preferences through preference pairs and varied prompt distributions.\\n\\nCrucially, the training distinguishes between helpfulness and harmlessness assessments: helpfulness is evaluated solely on the final summary to ensure responses are relevant and useful without disrupting the reasoning process, whereas harmlessness considers the entire model output‚Äîincluding reasoning steps and summaries‚Äîto detect and mitigate potential biases or harmful content. This dual-focus reinforcement learning strategy effectively balances the model‚Äôs reasoning proficiency with practical utility and ethical safety, thereby advancing the paper‚Äôs central objective of incentivizing robust reasoning in large language models while maintaining alignment with human values and real-world applicability.',\n",
       "   'Section 2.4 extends the paper‚Äôs overarching goal of enhancing reasoning capabilities in large language models by focusing on empowering smaller, more efficient models through distillation. Building on the sophisticated reinforcement learning framework established in Section 2.3.4, which holistically aligns DeepSeek-R1 with human preferences, this section explores a more accessible approach: directly fine-tuning open-source smaller models such as Qwen and Llama using the 800k high-quality reasoning samples curated by DeepSeek-R1. The authors demonstrate that this straightforward supervised fine-tuning (SFT) distillation method substantially improves the reasoning abilities of these smaller models without incorporating the reinforcement learning stage. By applying this technique to a range of model sizes‚Äîfrom 1.5B to 70B parameters‚Äîthe study highlights the practical potential of distillation to transfer advanced reasoning skills efficiently. Although the reinforcement learning phase could further enhance performance, the authors intentionally limit this section to distillation to showcase its standalone effectiveness and encourage future research to explore RL integration. This approach aligns with the paper‚Äôs central theme of incentivizing and disseminating robust reasoning capabilities across model scales while balancing complexity and accessibility.',\n",
       "   'Section 3 advances the paper‚Äôs central objective of incentivizing reasoning capabilities in large language models by rigorously evaluating DeepSeek-R1 across a diverse and comprehensive suite of benchmarks. Building on the distillation strategies detailed in Section 2.4, this section systematically assesses DeepSeek-R1‚Äôs performance on a wide array of challenging tasks spanning knowledge-intensive, reasoning-heavy, code generation, and long-context question answering domains. The evaluation leverages established benchmarks such as MMLU variants, C-Eval, GPQA Diamond, and newly introduced datasets like LiveCodeBench and SWE-Bench Verified, alongside open-ended generation tasks judged via GPT-4-Turbo-1106 to ensure robust and nuanced performance measurement.\\n\\nMethodologically, the authors adopt a zero-shot evaluation paradigm for most benchmarks, carefully adjusting prompt formats to avoid few-shot chain-of-thought biases that could disadvantage DeepSeek-R1. They employ a pass@1 metric with stochastic sampling (temperature 0.6, top-p 0.95) to mitigate repetition and variability issues inherent in greedy decoding of long outputs, thereby providing more reliable performance estimates. For distilled smaller models, representative results on select benchmarks demonstrate the transferability of reasoning improvements.\\n\\nEmpirically, DeepSeek-R1 consistently outperforms its predecessor DeepSeek-V3 and several strong baselines‚Äîincluding Claude-Sonnet-3.5, GPT-4o, and OpenAI‚Äôs latest models‚Äîparticularly excelling in STEM-related and long-context reasoning tasks such as FRAMES and AIME 2024. These gains underscore the effectiveness of large-scale reinforcement learning in enhancing reasoning accuracy and document analysis capabilities, reinforcing the paper‚Äôs thesis that incentivized reasoning via RL can substantially elevate LLM performance. Notably, DeepSeek-R1 also shows superior factual query handling on SimpleQA, although safety-driven reinforcement learning introduces some conservative behavior on the Chinese SimpleQA benchmark, slightly reducing accuracy.\\n\\nOverall, Section 3 substantiates the methodological innovations and training strategies introduced earlier by demonstrating DeepSeek-R1‚Äôs strong, empirically validated reasoning capabilities across diverse, real-world benchmarks. This comprehensive evaluation confirms the paper‚Äôs contribution in advancing reasoning proficiency in LLMs through reinforcement learning and distillation, while highlighting practical considerations such as prompt design, evaluation protocols, and safety trade-offs.',\n",
       "   'Section 3.1 DeepSeek-R1 Evaluation further substantiates the paper‚Äôs core claim that large-scale reinforcement learning effectively incentivizes reasoning capabilities in LLMs by demonstrating DeepSeek-R1‚Äôs strong generalization across multiple specialized benchmarks. Building on the broad evaluation framework established previously, this subsection highlights DeepSeek-R1‚Äôs notable success on IF-Eval, a benchmark assessing adherence to format instructions, which is attributed to the targeted inclusion of instruction-following data during late-stage supervised fine-tuning and RL training. The model‚Äôs superior performance on AlpacaEval 2.0 and ArenaHard benchmarks underscores its enhanced proficiency in writing tasks and open-domain question answering, significantly outperforming its predecessor DeepSeek-V3 and illustrating the broad domain transfer benefits of RL-driven reasoning improvements.\\n\\nImportantly, DeepSeek-R1 maintains concise output lengths across tasks, avoiding length bias in GPT-based evaluations and reinforcing its robustness in generating high-quality, efficient responses. On STEM-related challenges, DeepSeek-R1 matches the performance of OpenAI-o1-1217 in math tasks and leads other models by a wide margin, while also excelling in reasoning-intensive coding benchmarks like LiveCodeBench and Codeforces. Although OpenAI-o1-1217 currently outperforms DeepSeek-R1 on certain engineering-focused coding tasks such as Aider, comparable results on SWE Verified suggest promising potential for future iterations as RL training data in this domain expands.\\n\\nOverall, Section 3.1 complements the comprehensive evaluation strategy detailed earlier by providing concrete evidence that DeepSeek-R1‚Äôs reinforcement learning enhancements not only elevate reasoning accuracy but also improve instruction adherence, writing quality, and coding proficiency across diverse domains. This reinforces the paper‚Äôs overarching thesis that incentivizing reasoning via large-scale RL yields multifaceted performance gains, while also identifying avenues for continued refinement in engineering-oriented tasks.',\n",
       "   'Section 3.2 Distilled Model Evaluation builds directly on the strong performance of DeepSeek-R1 established in Section 3.1 by examining the efficacy of knowledge distillation as a means to create more efficient yet highly capable reasoning models. The authors demonstrate that distilling the outputs of DeepSeek-R1 produces smaller models‚Äîsuch as DeepSeek-R1-7B‚Äîthat consistently outperform larger, non-reasoning-focused baselines like GPT-4o-0513 across a wide range of benchmarks including MATH-500, GPQA, LiveCode, and Codeforces. Notably, the distilled DeepSeek-R1-14B model surpasses the QwQ-32B-Preview on all evaluation metrics, while even larger distilled variants (32B and 70B) significantly outperform the o1-mini baseline on most tasks. These findings underscore the strong potential of distillation to preserve and transfer the enhanced reasoning capabilities incentivized by reinforcement learning into more parameter-efficient models.\\n\\nFurthermore, the section highlights preliminary evidence that applying reinforcement learning to these distilled models can yield additional performance gains, suggesting a promising direction for future work. However, the current results focus on simple supervised fine-tuning (SFT) distilled models to establish a baseline. This evaluation of distilled models complements the prior section‚Äôs emphasis on large-scale RL training by illustrating a practical pathway to deploy reasoning-optimized LLMs in more resource-constrained settings without sacrificing accuracy or generalization. Overall, Section 3.2 reinforces the paper‚Äôs central thesis that incentivizing reasoning through RL not only improves raw model capabilities but also enables effective downstream compression and efficiency improvements via distillation, thereby broadening the applicability of DeepSeek-R1‚Äôs reasoning advancements.',\n",
       "   'Section 4.1 builds upon the promising results of distilled models presented in Section 3.2 by directly comparing the effectiveness of knowledge distillation against large-scale reinforcement learning (RL) training without distillation. The authors investigate whether training a large base model (Qwen-32B-Base) solely through extensive RL on math, code, and STEM datasets‚Äîresulting in DeepSeek-R1-Zero-Qwen-32B‚Äîcan match the performance of distilled variants. Experimental results reveal that while RL training alone achieves performance comparable to the QwQ-32B-Preview baseline, it falls short of the significantly superior results attained by DeepSeek-R1-Distill-Qwen-32B, which benefits from distillation of a more powerful model.\\n\\nThis comparison leads to two key conclusions aligned with the paper‚Äôs overarching theme of incentivizing reasoning in LLMs: first, distillation effectively transfers enhanced reasoning capabilities into smaller, more efficient models with less computational cost; second, although large-scale RL training can improve base models, achieving breakthroughs beyond current intelligence boundaries likely requires both more powerful foundational models and extensive RL efforts. Thus, Section 4.1 reinforces the paper‚Äôs argument that distillation is a practical and resource-efficient strategy to capitalize on reasoning improvements induced by RL, while also acknowledging the continued importance of large-scale RL for future advances.',\n",
       "   'Section 4.2 details the unsuccessful attempts encountered during the development of DeepSeek-R1, providing critical insights into the challenges of incentivizing reasoning capabilities in large language models (LLMs) via reinforcement learning. Building on the previous section‚Äôs exploration of large-scale RL training versus distillation, this section examines two alternative methodologies‚ÄîPreference-based Reward Modeling (PRM) and Monte Carlo Tree Search (MCTS)‚Äîthat were investigated but ultimately found limited in effectiveness within this context.\\n\\nThe authors first analyze PRM, a method aimed at guiding models toward improved reasoning by leveraging preferences to rank intermediate reasoning steps. Despite its theoretical appeal and prior successes in related work, PRM faces three major obstacles: the difficulty of defining fine-grained reasoning steps, challenges in accurately annotating intermediate correctness (with automated and manual approaches both falling short), and the risk of reward hacking that complicates training and demands additional resources. Consequently, while PRM can assist in reranking candidate responses or guiding search, its computational overhead and scalability issues diminish its practical value for large-scale RL in DeepSeek-R1.\\n\\nNext, inspired by AlphaGo and AlphaZero, the authors explored MCTS to enhance test-time reasoning by decomposing answers into smaller reasoning steps and systematically searching the solution space. This approach involves training a value model to guide the search and iteratively refining both actor and value models through self-play-like procedures. However, scaling MCTS to token generation tasks proved problematic due to the exponentially larger search space compared to structured games like chess, leading to local optima and difficulties in training a reliable value model. These challenges hindered the iterative improvement process that underpins AlphaGo‚Äôs success, limiting MCTS‚Äôs utility to inference-time performance gains rather than sustained model enhancement.\\n\\nIn summary, Section 4.2 underscores the complexity of effectively incentivizing reasoning in LLMs through RL by highlighting the practical limitations of PRM and MCTS approaches. These findings complement the previous section‚Äôs conclusions by illustrating that while large-scale RL and distillation remain promising, alternative strategies to directly guide reasoning steps or leverage search-based methods face significant scalability and training challenges. This reinforces the paper‚Äôs overarching theme that advancing reasoning capabilities in LLMs requires carefully balanced methodologies that consider both computational feasibility and the nuanced nature of reasoning tasks.',\n",
       "   'Section 5 concludes the paper by reflecting on the successful development and evaluation of DeepSeek-R1, a reinforcement learning (RL)-based framework designed to enhance reasoning capabilities in large language models (LLMs). Building on the challenges and limitations of alternative methods such as Preference-based Reward Modeling and Monte Carlo Tree Search discussed in Section 4.2, the authors emphasize that DeepSeek-R1-Zero‚Äîan RL approach without cold-start data‚Äîalready achieves strong performance across diverse reasoning tasks. The more advanced DeepSeek-R1 model, which integrates cold-start data with iterative RL fine-tuning, attains performance on par with leading commercial models like OpenAI-o1-1217, underscoring the efficacy of their RL-centric methodology.\\n\\nA key contribution highlighted is the successful distillation of DeepSeek-R1‚Äôs reasoning ability into smaller dense models. By generating a large-scale dataset of 800K samples from the teacher model, the authors fine-tune compact models such as DeepSeek-R1-Distill-Qwen-1.5B, which notably surpasses GPT-4o and Claude-3.5-Sonnet on challenging math benchmarks. This demonstrates the practical value of their approach in producing efficient yet powerful models, addressing scalability and deployment concerns raised in earlier sections.\\n\\nLooking forward, the authors candidly acknowledge current limitations and outline future research directions aligned with the paper‚Äôs overarching goal of robustly incentivizing reasoning in LLMs. These include enhancing general capabilities to better handle complex tasks like function calling and multi-turn interactions, mitigating language mixing issues beyond Chinese and English, refining prompt engineering strategies to optimize zero-shot performance, and improving RL efficiency for software engineering tasks through techniques like rejection sampling and asynchronous evaluation.\\n\\nOverall, this concluding section ties together the paper‚Äôs methodological innovations, empirical successes, and encountered challenges, reinforcing the central thesis that carefully designed reinforcement learning frameworks‚Äîcomplemented by strategic distillation‚Äîcan meaningfully advance reasoning capabilities in large language models while highlighting avenues for continued improvement.']},\n",
       " 'final_summary': {'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning': '**Comprehensive Final Summary of \"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\"**\\n\\n---\\n\\n### 1. Research Objective and Background\\n\\nThe paper addresses a central challenge in the development of Large Language Models (LLMs): how to efficiently and robustly enhance their reasoning capabilities beyond what is achievable through traditional supervised fine-tuning (SFT). While recent advances in LLMs have yielded impressive results, the reasoning performance of open-source models has lagged behind leading commercial offerings such as OpenAI‚Äôs o1 series. Existing post-training techniques‚Äîincluding inference-time scaling, process-based reward models, reinforcement learning (RL), and search algorithms‚Äîhave not fully closed this gap. The core objective of this work is to pioneer and rigorously evaluate a reinforcement learning-centric approach that incentivizes and cultivates advanced reasoning in LLMs, minimizing reliance on extensive supervised data. The study introduces DeepSeek-R1 and its variants, aiming to match or surpass state-of-the-art reasoning benchmarks while ensuring practical scalability, interpretability, and accessibility for the broader research community.\\n\\n---\\n\\n### 2. Key Methodology\\n\\nThe methodological innovation of the paper centers on leveraging large-scale reinforcement learning to directly incentivize reasoning in LLMs. The approach unfolds in several stages:\\n\\n- **DeepSeek-R1-Zero:** This variant applies RL directly to a base LLM without any supervised fine-tuning, using a novel Group Relative Policy Optimization (GRPO) algorithm. GRPO efficiently estimates policy advantages by comparing groups of sampled outputs, eliminating the need for a separate critic model and reducing computational overhead.\\n\\n- **Reward Modeling:** The RL process is guided by a rule-based reward system that combines accuracy rewards (e.g., correctness of math answers or code solutions) and format rewards (e.g., explicit demarcation of reasoning steps using tags). This ensures both verifiable correctness and transparent, interpretable reasoning without the pitfalls of neural reward models.\\n\\n- **Structured Output Templates:** Outputs are scaffolded into a reasoning process followed by a final answer, promoting clarity and minimizing content-specific biases.\\n\\n- **Self-Evolution and Emergent Behaviors:** RL training from scratch enables the model to autonomously develop sophisticated reasoning strategies, such as increased ‚Äúthinking time,‚Äù reflection, and exploration of alternative solutions.\\n\\n- **DeepSeek-R1 (Full Pipeline):** To address limitations in readability and language consistency observed in DeepSeek-R1-Zero, the authors introduce a multi-stage pipeline:\\n    1. **Cold-Start SFT:** Fine-tuning on thousands of high-quality, human-curated Chain-of-Thought (CoT) examples to stabilize early RL training and improve output clarity.\\n    2. **Reasoning-Oriented RL:** Large-scale RL with additional language consistency rewards to ensure outputs are both accurate and linguistically coherent.\\n    3. **Rejection Sampling and SFT:** Generating a large, diverse dataset via rejection sampling from the RL-trained model, followed by supervised fine-tuning to broaden general capabilities.\\n    4. **Secondary RL Alignment:** Further RL to holistically align the model with human preferences for helpfulness and harmlessness, using both rule-based and preference-based rewards.\\n\\n- **Distillation:** The enhanced reasoning abilities of DeepSeek-R1 are distilled into smaller, dense models (e.g., Qwen, Llama) via supervised fine-tuning on the curated dataset, enabling efficient deployment without the need for RL in the smaller models.\\n\\n---\\n\\n### 3. Key Findings\\n\\n- **Autonomous Reasoning via RL:** DeepSeek-R1-Zero, trained solely with RL and no supervised data, achieves dramatic improvements on reasoning benchmarks (e.g., AIME 2024 pass@1 rising from 15.6% to 71.0%), matching the performance of OpenAI‚Äôs o1-0912 model. Emergent behaviors such as reflection and extended reasoning steps arise naturally from the RL environment.\\n\\n- **State-of-the-Art Performance:** DeepSeek-R1, with its multi-stage pipeline, attains or surpasses leading commercial models on a wide range of benchmarks:\\n    - **Reasoning:** 79.8% pass@1 on AIME 2024, 97.3% on MATH-500.\\n    - **Coding:** 2,029 Elo on Codeforces, outperforming 96% of human competitors.\\n    - **Knowledge:** 90.8% on MMLU, 84.0% on MMLU-Pro, 71.5% on GPQA Diamond.\\n    - **General Tasks:** High win-rates on AlpacaEval 2.0 (87.6%) and ArenaHard (92.3%), with superior long-context understanding and concise, high-quality outputs.\\n\\n- **Distillation Success:** Distilled models (e.g., DeepSeek-R1-7B, 14B, 32B, 70B) consistently outperform larger, non-reasoning-focused baselines (e.g., GPT-4o-0513, QwQ-32B-Preview) on math, code, and general reasoning tasks, demonstrating the effectiveness of transferring reasoning skills to smaller, more efficient models.\\n\\n- **Empirical Validation:** Comprehensive evaluations across diverse benchmarks‚Äîincluding STEM, code generation, knowledge-intensive, and open-ended tasks‚Äîconfirm the robustness and generalizability of DeepSeek-R1‚Äôs reasoning improvements.\\n\\n---\\n\\n### 4. Implications and Significance\\n\\nThis work marks a significant advance in the field of LLMs by demonstrating that carefully designed reinforcement learning frameworks can autonomously cultivate sophisticated reasoning capabilities, even in the absence of extensive supervised data. The introduction of GRPO and rule-based reward modeling offers a scalable, resource-efficient alternative to traditional RL approaches. The multi-stage training pipeline, combining cold-start SFT, RL, and distillation, not only achieves state-of-the-art performance but also ensures outputs are interpretable, linguistically consistent, and aligned with human preferences.\\n\\nBy open-sourcing both the distilled models and their APIs, the authors provide valuable resources that democratize access to high-performing reasoning models, fostering further research and practical applications. The successful distillation of reasoning abilities into smaller models broadens the accessibility and deployability of advanced LLMs, making them viable for resource-constrained environments.\\n\\nMoreover, the paper‚Äôs rigorous evaluation protocols and transparent discussion of unsuccessful approaches (e.g., PRM, MCTS) offer critical insights for the community, guiding future research toward more effective and scalable reasoning enhancement strategies.\\n\\n---\\n\\n### 5. Limitations\\n\\nDespite its successes, the paper acknowledges several limitations:\\n\\n- **Language Mixing and Readability:** Early RL-only models exhibited issues with mixed-language outputs and poor readability, necessitating the integration of cold-start SFT and language consistency rewards.\\n- **General Capabilities:** While reasoning and coding performance are strong, further improvements are needed in complex tasks such as function calling, multi-turn interactions, and broader general-purpose abilities.\\n- **Prompt Engineering:** The effectiveness of zero-shot performance is sensitive to prompt design, indicating a need for further refinement in prompt engineering strategies.\\n- **RL Efficiency in Engineering Tasks:** RL training for software engineering tasks remains less efficient, with ongoing work needed to optimize reward modeling and evaluation (e.g., through rejection sampling and asynchronous evaluation).\\n- **Scalability of Alternative Methods:** Preference-based Reward Modeling and Monte Carlo Tree Search, while theoretically appealing, proved impractical at scale due to annotation challenges, reward hacking, and computational complexity.\\n\\n---\\n\\n**Conclusion**\\n\\n\"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\" presents a comprehensive, empirically validated framework for advancing reasoning in large language models through innovative RL methodologies and strategic distillation. The work not only achieves state-of-the-art results across a spectrum of reasoning and general tasks but also provides practical pathways for scalable, interpretable, and accessible LLM deployment. By candidly addressing limitations and open-sourcing their models, the authors set a new benchmark for research in LLM reasoning, offering both immediate tools and a roadmap for future advancements in the field.'},\n",
       " 'paper_domain': {'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning': {'main_field': ['Computer Science'],\n",
       "   'sub_field': ['Natural Language Processing',\n",
       "    'Reinforcement Learning',\n",
       "    'Large Language Models',\n",
       "    'Machine Learning']}},\n",
       " 'analysis_plan': 'single',\n",
       " 'final_report': '# DeepSeek-R1: How Reinforcement Learning Supercharged LLM Reasoning (Without Mountains of Labels)\\n\\nLarge Language Models are great talkers‚Äîbut not always great thinkers. Traditional supervised fine-tuning (SFT) helps, but it struggles to unlock robust multi-step reasoning at scale. DeepSeek-R1 takes a different path: it uses reinforcement learning (RL) to directly incentivize reasoning, then distills that skill into smaller, faster models.\\n\\nIn this post, you‚Äôll learn what makes DeepSeek-R1 different, how its training pipeline works (including the GRPO algorithm), the results it achieves, and how to prototype similar ideas with a small Python snippet.\\n\\n## What Is DeepSeek-R1?\\n\\nDeepSeek-R1 is a reinforcement learning‚Äìcentric training framework for reasoning-focused LLMs. It comes in two flavors:\\n\\n- DeepSeek-R1-Zero: trains purely via RL from a base model (no SFT), using Group Relative Policy Optimization (GRPO).\\n- DeepSeek-R1 (full pipeline): adds a practical multi-stage pipeline around RL for readability, consistency, and alignment.\\n\\nThe core idea: reward models not only for being correct, but also for showing their work in a structured, interpretable way.\\n\\n## Why RL for Reasoning?\\n\\nMost open-source LLMs lag behind commercial models on reasoning-heavy tasks. Prior post-training methods (process reward models, search algorithms like MCTS, or inference-time scaling) didn‚Äôt fully close the gap. RL lets the model discover longer, richer reasoning chains on its own‚Äîleading to emergent behaviors like reflection, trying alternatives, and ‚Äúthinking longer‚Äù when needed.\\n\\n## The Key Ingredient: GRPO (Group Relative Policy Optimization)\\n\\nGRPO is a lightweight alternative to PPO that avoids training a separate critic. Instead, the model:\\n\\n1. Samples multiple candidate answers per prompt.\\n2. Scores them with rule-based rewards (accuracy, format).\\n3. Computes advantages relative to the group average.\\n4. Updates the policy toward better-than-average samples.\\n\\nThis keeps training simple and stable while scaling to large batches.\\n\\n## Rewards That Encourage Thinking\\n\\nThe paper uses rule-based rewards you can verify automatically:\\n\\n- Accuracy reward: Did the final answer match the ground truth (e.g., math result or code output)?\\n- Format reward: Did the model separate reasoning from the final answer using explicit tags (e.g., <think> ... </think>, <answer> ... </answer>)?\\n\\nThis approach reduces reward hacking and avoids fragile neural reward models.\\n\\n## Structured Outputs Make Reasoning Clear\\n\\nThe model is prompted to produce two clearly separated sections:\\n- A reasoning block (<think>...</think>)\\n- A final answer block (<answer>...</answer>)\\n\\nThis both improves interpretability and gives the reward function something concrete to validate.\\n\\n## From Scratch to State of the Art: The Full Training Pipeline\\n\\nWhile DeepSeek-R1-Zero was strong, early outputs sometimes mixed languages or were hard to read. The full DeepSeek-R1 pipeline addresses that:\\n\\n1. Cold-Start SFT: Thousands of high-quality, human-curated Chain-of-Thought examples to stabilize early RL and improve clarity.\\n2. Reasoning-Oriented RL: Large-scale RL with extra language-consistency rewards to keep outputs coherent and readable.\\n3. Rejection Sampling + SFT: Generate diverse data from the RL model, keep the best samples, and do supervised fine-tuning to broaden general skills.\\n4. Secondary RL Alignment: A final RL pass for helpfulness/harmlessness using rule-based and preference-based rewards.\\n\\n## Distillation: Make It Fast and Accessible\\n\\nAfter training a strong reasoning model, the team distills its skills into smaller dense models (e.g., Qwen, Llama variants). These distilled models retain much of the reasoning power without needing RL at deployment time‚Äîgreat for teams with realistic budgets and latency requirements.\\n\\n## What Did It Achieve?\\n\\nHighlights from the paper‚Äôs evaluations:\\n\\n- DeepSeek-R1-Zero (RL only) showed dramatic gains:\\n  - AIME 2024 pass@1: 15.6% ‚Üí 71.0%, matching OpenAI‚Äôs o1-0912\\n  - Emergent behaviors: reflection, longer reasoning steps\\n\\n- DeepSeek-R1 (full pipeline) achieved or surpassed SOTA on diverse benchmarks:\\n  - Reasoning: 79.8% (AIME 2024 pass@1), 97.3% (MATH-500)\\n  - Coding: 2,029 Elo on Codeforces (better than 96% of human competitors)\\n  - Knowledge: 90.8% (MMLU), 84.0% (MMLU-Pro), 71.5% (GPQA Diamond)\\n  - General tasks: 87.6% win rate (AlpacaEval 2.0), 92.3% (ArenaHard), strong long-context and concise outputs\\n\\n- Distilled models (7B‚Äì70B) outperformed larger non-reasoning baselines across math, code, and general reasoning.\\n\\n## What Didn‚Äôt Work (and Why It Matters)\\n\\n- PRM/MCTS were impractical at scale due to annotation needs, reward hacking, and compute complexity.\\n- RL-only models initially had language mixing issues‚Äîfixed via SFT and language consistency rewards.\\n- Zero-shot results were sensitive to prompt design‚Äîprompt engineering still matters.\\n- RL for software engineering tasks remains less efficient; better evaluation and reward shaping are ongoing work.\\n\\n## Mini GRPO Prototype: Rule-Based Reward and Structured Outputs\\n\\nThe snippet below illustrates a toy GRPO loop with:\\n- Group sampling per prompt\\n- Rule-based rewards (accuracy + format)\\n- Group-relative advantages\\n\\nIt‚Äôs simplified for readability and runs as pseudocode (you can adapt it with your favorite library).\\n\\n```python\\nimport random\\nfrom typing import List, Tuple\\n\\nTHINK_L, THINK_R = \"<think>\", \"</think>\"\\nANS_L, ANS_R = \"<answer>\", \"</answer>\"\\n\\nclass TinyPolicy:\\n    def __init__(self):\\n        self.params = {\"verbosity\": 0.5}  # toy param\\n\\n    def generate(self, prompt: str, n: int) -> List[str]:\\n        # Pretend we sample different \"reasoning lengths\"\\n        outs = []\\n        for _ in range(n):\\n            thoughts = \" ...\" * random.randint(1, int(1 + 5 * self.params[\"verbosity\"]))\\n            # pretend it solves x+y from prompt \"x y\"\\n            try:\\n                x, y = map(int, prompt.strip().split())\\n                ans = str(x + y)\\n            except Exception:\\n                ans = \"0\"\\n            outs.append(f\"{THINK_L}Let me compute{x+y if \\' \\' in prompt else \\'\\'}{thoughts}{THINK_R}{ANS_L}{ans}{ANS_R}\")\\n        return outs\\n\\n    def update(self, samples: List[str], advantages: List[float]):\\n        # Toy update: increase verbosity if longer thoughts had higher advantage\\n        avg_len = sum(s.count(\"...\") for s in samples) / max(1, len(samples))\\n        avg_adv = sum(advantages) / max(1, len(advantages))\\n        if avg_adv > 0:\\n            self.params[\"verbosity\"] = min(1.0, self.params[\"verbosity\"] + 0.05 * avg_len)\\n        else:\\n            self.params[\"verbosity\"] = max(0.0, self.params[\"verbosity\"] - 0.02)\\n\\ndef extract_between(text: str, left: str, right: str) -> str:\\n    if left in text and right in text:\\n        return text.split(left, 1)[1].split(right, 1)[0].strip()\\n    return \"\"\\n\\ndef format_reward(out: str) -> float:\\n    has_think = THINK_L in out and THINK_R in out\\n    has_ans = ANS_L in out and ANS_R in out\\n    return 0.5 * has_think + 0.5 * has_ans  # in [0,1]\\n\\ndef accuracy_reward(prompt: str, out: str) -> float:\\n    gt = None\\n    try:\\n        x, y = map(int, prompt.strip().split())\\n        gt = x + y\\n    except Exception:\\n        return 0.0\\n    ans = extract_between(out, ANS_L, ANS_R)\\n    try:\\n        return 1.0 if int(ans) == gt else 0.0\\n    except Exception:\\n        return 0.0\\n\\ndef total_reward(prompt: str, out: str) -> float:\\n    return 0.7 * accuracy_reward(prompt, out) + 0.3 * format_reward(out)\\n\\ndef grpo_step(policy: TinyPolicy, prompt: str, group_size: int = 8) -> Tuple[float, float]:\\n    samples = policy.generate(prompt, n=group_size)\\n    rewards = [total_reward(prompt, s) for s in samples]\\n    baseline = sum(rewards) / max(1, len(rewards))\\n    advantages = [r - baseline for r in rewards]\\n    policy.update(samples, advantages)\\n    return sum(rewards) / len(rewards), sum(advantages) / len(advantages)\\n\\n# Train on toy addition prompts\\npolicy = TinyPolicy()\\nprompts = [\"1 2\", \"5 7\", \"10 15\", \"3 3\", \"100 23\"]\\n\\nfor step in range(50):\\n    p = random.choice(prompts)\\n    avg_r, avg_adv = grpo_step(policy, p, group_size=6)\\n    if step % 10 == 0:\\n        print(f\"Step {step:02d} | avg_reward={avg_r:.2f} | verbosity={policy.params[\\'verbosity\\']:.2f}\")\\n\\n# Inference with structured output\\ntest = \"8 9\"\\nprint(policy.generate(test, n=1)[0])\\n```\\n\\nWhat this shows:\\n- You can structure outputs and reward them without a neural critic.\\n- Group-relative advantages let you improve the policy using only rule-based signals.\\n- Even a toy ‚Äúverbosity‚Äù parameter can learn to ‚Äúthink longer‚Äù if longer chains correlate with higher rewards.\\n\\n## Practical Tips If You‚Äôre Trying This\\n\\n- Start with rule-based rewards you can verify automatically (math, code tests, format checks).\\n- Use structured tags to keep reasoning trace and answer separate for easy evaluation.\\n- Warm-start with a small set of human CoT examples if outputs are messy or multilingual.\\n- Consider rejection sampling to bootstrap a higher-quality SFT dataset from your RL model.\\n- Add a final alignment stage to balance helpfulness, safety, and brevity.\\n\\n## Limitations to Keep in Mind\\n\\n- Early RL-only runs may mix languages or be hard to read‚Äîlanguage consistency rewards help.\\n- Zero-shot results are prompt-sensitive‚Äîthoughtful prompts matter.\\n- General-purpose interactive skills (function calling, multi-turn flows) may need extra data and objectives.\\n- RL for software engineering is still compute- and evaluation-heavy; invest in good test harnesses.\\n\\n## Why This Matters\\n\\nDeepSeek-R1 shows that with the right reward signals and training pipeline, LLMs can autonomously grow strong reasoning behaviors‚Äîthen pass those skills down to smaller, cheaper models. It‚Äôs a practical blueprint for teams who want better reasoning without massive supervised datasets.\\n\\n## Key Takeaways\\n\\n- Reinforcement learning can directly incentivize reasoning in LLMs.\\n- GRPO removes the critic, enabling scalable, stable training with group-relative advantages.\\n- Rule-based rewards (accuracy + format) reduce reward hacking and increase interpretability.\\n- A multi-stage pipeline (SFT ‚Üí RL ‚Üí rejection sampling SFT ‚Üí RL alignment) yields state-of-the-art results.\\n- Distillation makes reasoning models deployable in resource-constrained environments.\\n\\n## Potential Applications\\n\\n- STEM tutoring, problem-solving assistants, and math-heavy analytics\\n- Competitive programming and code generation with automatic test rewards\\n- Scientific reasoning and long-context document analysis\\n- Decision support systems that require transparent chains of thought\\n- On-device or low-latency reasoning via distilled small models',\n",
       " 'tracking_summary': {'workflow_id': '20250818_065412',\n",
       "  'total_duration_seconds': 215.798646,\n",
       "  'total_agents': 0,\n",
       "  'successful_agents': 0,\n",
       "  'failed_agents': 0,\n",
       "  'total_tokens': 0,\n",
       "  'total_cost_usd': 0.0,\n",
       "  'agent_details': []}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x119eced50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# DeepSeek-R1: How Reinforcement Learning Supercharged LLM Reasoning (Without Mountains of Labels)\n",
      "\n",
      "Large Language Models are great talkers‚Äîbut not always great thinkers. Traditional supervised fine-tuning (SFT) helps, but it struggles to unlock robust multi-step reasoning at scale. DeepSeek-R1 takes a different path: it uses reinforcement learning (RL) to directly incentivize reasoning, then distills that skill into smaller, faster models.\n",
      "\n",
      "In this post, you‚Äôll learn what makes DeepSeek-R1 different, how its training pipeline works (including the GRPO algorithm), the results it achieves, and how to prototype similar ideas with a small Python snippet.\n",
      "\n",
      "## What Is DeepSeek-R1?\n",
      "\n",
      "DeepSeek-R1 is a reinforcement learning‚Äìcentric training framework for reasoning-focused LLMs. It comes in two flavors:\n",
      "\n",
      "- DeepSeek-R1-Zero: trains purely via RL from a base model (no SFT), using Group Relative Policy Optimization (GRPO).\n",
      "- DeepSeek-R1 (full pipeline): adds a practical multi-stage pipeline around RL for readability, consistency, and alignment.\n",
      "\n",
      "The core idea: reward models not only for being correct, but also for showing their work in a structured, interpretable way.\n",
      "\n",
      "## Why RL for Reasoning?\n",
      "\n",
      "Most open-source LLMs lag behind commercial models on reasoning-heavy tasks. Prior post-training methods (process reward models, search algorithms like MCTS, or inference-time scaling) didn‚Äôt fully close the gap. RL lets the model discover longer, richer reasoning chains on its own‚Äîleading to emergent behaviors like reflection, trying alternatives, and ‚Äúthinking longer‚Äù when needed.\n",
      "\n",
      "## The Key Ingredient: GRPO (Group Relative Policy Optimization)\n",
      "\n",
      "GRPO is a lightweight alternative to PPO that avoids training a separate critic. Instead, the model:\n",
      "\n",
      "1. Samples multiple candidate answers per prompt.\n",
      "2. Scores them with rule-based rewards (accuracy, format).\n",
      "3. Computes advantages relative to the group average.\n",
      "4. Updates the policy toward better-than-average samples.\n",
      "\n",
      "This keeps training simple and stable while scaling to large batches.\n",
      "\n",
      "## Rewards That Encourage Thinking\n",
      "\n",
      "The paper uses rule-based rewards you can verify automatically:\n",
      "\n",
      "- Accuracy reward: Did the final answer match the ground truth (e.g., math result or code output)?\n",
      "- Format reward: Did the model separate reasoning from the final answer using explicit tags (e.g., <think> ... </think>, <answer> ... </answer>)?\n",
      "\n",
      "This approach reduces reward hacking and avoids fragile neural reward models.\n",
      "\n",
      "## Structured Outputs Make Reasoning Clear\n",
      "\n",
      "The model is prompted to produce two clearly separated sections:\n",
      "- A reasoning block (<think>...</think>)\n",
      "- A final answer block (<answer>...</answer>)\n",
      "\n",
      "This both improves interpretability and gives the reward function something concrete to validate.\n",
      "\n",
      "## From Scratch to State of the Art: The Full Training Pipeline\n",
      "\n",
      "While DeepSeek-R1-Zero was strong, early outputs sometimes mixed languages or were hard to read. The full DeepSeek-R1 pipeline addresses that:\n",
      "\n",
      "1. Cold-Start SFT: Thousands of high-quality, human-curated Chain-of-Thought examples to stabilize early RL and improve clarity.\n",
      "2. Reasoning-Oriented RL: Large-scale RL with extra language-consistency rewards to keep outputs coherent and readable.\n",
      "3. Rejection Sampling + SFT: Generate diverse data from the RL model, keep the best samples, and do supervised fine-tuning to broaden general skills.\n",
      "4. Secondary RL Alignment: A final RL pass for helpfulness/harmlessness using rule-based and preference-based rewards.\n",
      "\n",
      "## Distillation: Make It Fast and Accessible\n",
      "\n",
      "After training a strong reasoning model, the team distills its skills into smaller dense models (e.g., Qwen, Llama variants). These distilled models retain much of the reasoning power without needing RL at deployment time‚Äîgreat for teams with realistic budgets and latency requirements.\n",
      "\n",
      "## What Did It Achieve?\n",
      "\n",
      "Highlights from the paper‚Äôs evaluations:\n",
      "\n",
      "- DeepSeek-R1-Zero (RL only) showed dramatic gains:\n",
      "  - AIME 2024 pass@1: 15.6% ‚Üí 71.0%, matching OpenAI‚Äôs o1-0912\n",
      "  - Emergent behaviors: reflection, longer reasoning steps\n",
      "\n",
      "- DeepSeek-R1 (full pipeline) achieved or surpassed SOTA on diverse benchmarks:\n",
      "  - Reasoning: 79.8% (AIME 2024 pass@1), 97.3% (MATH-500)\n",
      "  - Coding: 2,029 Elo on Codeforces (better than 96% of human competitors)\n",
      "  - Knowledge: 90.8% (MMLU), 84.0% (MMLU-Pro), 71.5% (GPQA Diamond)\n",
      "  - General tasks: 87.6% win rate (AlpacaEval 2.0), 92.3% (ArenaHard), strong long-context and concise outputs\n",
      "\n",
      "- Distilled models (7B‚Äì70B) outperformed larger non-reasoning baselines across math, code, and general reasoning.\n",
      "\n",
      "## What Didn‚Äôt Work (and Why It Matters)\n",
      "\n",
      "- PRM/MCTS were impractical at scale due to annotation needs, reward hacking, and compute complexity.\n",
      "- RL-only models initially had language mixing issues‚Äîfixed via SFT and language consistency rewards.\n",
      "- Zero-shot results were sensitive to prompt design‚Äîprompt engineering still matters.\n",
      "- RL for software engineering tasks remains less efficient; better evaluation and reward shaping are ongoing work.\n",
      "\n",
      "## Mini GRPO Prototype: Rule-Based Reward and Structured Outputs\n",
      "\n",
      "The snippet below illustrates a toy GRPO loop with:\n",
      "- Group sampling per prompt\n",
      "- Rule-based rewards (accuracy + format)\n",
      "- Group-relative advantages\n",
      "\n",
      "It‚Äôs simplified for readability and runs as pseudocode (you can adapt it with your favorite library).\n",
      "\n",
      "```python\n",
      "import random\n",
      "from typing import List, Tuple\n",
      "\n",
      "THINK_L, THINK_R = \"<think>\", \"</think>\"\n",
      "ANS_L, ANS_R = \"<answer>\", \"</answer>\"\n",
      "\n",
      "class TinyPolicy:\n",
      "    def __init__(self):\n",
      "        self.params = {\"verbosity\": 0.5}  # toy param\n",
      "\n",
      "    def generate(self, prompt: str, n: int) -> List[str]:\n",
      "        # Pretend we sample different \"reasoning lengths\"\n",
      "        outs = []\n",
      "        for _ in range(n):\n",
      "            thoughts = \" ...\" * random.randint(1, int(1 + 5 * self.params[\"verbosity\"]))\n",
      "            # pretend it solves x+y from prompt \"x y\"\n",
      "            try:\n",
      "                x, y = map(int, prompt.strip().split())\n",
      "                ans = str(x + y)\n",
      "            except Exception:\n",
      "                ans = \"0\"\n",
      "            outs.append(f\"{THINK_L}Let me compute{x+y if ' ' in prompt else ''}{thoughts}{THINK_R}{ANS_L}{ans}{ANS_R}\")\n",
      "        return outs\n",
      "\n",
      "    def update(self, samples: List[str], advantages: List[float]):\n",
      "        # Toy update: increase verbosity if longer thoughts had higher advantage\n",
      "        avg_len = sum(s.count(\"...\") for s in samples) / max(1, len(samples))\n",
      "        avg_adv = sum(advantages) / max(1, len(advantages))\n",
      "        if avg_adv > 0:\n",
      "            self.params[\"verbosity\"] = min(1.0, self.params[\"verbosity\"] + 0.05 * avg_len)\n",
      "        else:\n",
      "            self.params[\"verbosity\"] = max(0.0, self.params[\"verbosity\"] - 0.02)\n",
      "\n",
      "def extract_between(text: str, left: str, right: str) -> str:\n",
      "    if left in text and right in text:\n",
      "        return text.split(left, 1)[1].split(right, 1)[0].strip()\n",
      "    return \"\"\n",
      "\n",
      "def format_reward(out: str) -> float:\n",
      "    has_think = THINK_L in out and THINK_R in out\n",
      "    has_ans = ANS_L in out and ANS_R in out\n",
      "    return 0.5 * has_think + 0.5 * has_ans  # in [0,1]\n",
      "\n",
      "def accuracy_reward(prompt: str, out: str) -> float:\n",
      "    gt = None\n",
      "    try:\n",
      "        x, y = map(int, prompt.strip().split())\n",
      "        gt = x + y\n",
      "    except Exception:\n",
      "        return 0.0\n",
      "    ans = extract_between(out, ANS_L, ANS_R)\n",
      "    try:\n",
      "        return 1.0 if int(ans) == gt else 0.0\n",
      "    except Exception:\n",
      "        return 0.0\n",
      "\n",
      "def total_reward(prompt: str, out: str) -> float:\n",
      "    return 0.7 * accuracy_reward(prompt, out) + 0.3 * format_reward(out)\n",
      "\n",
      "def grpo_step(policy: TinyPolicy, prompt: str, group_size: int = 8) -> Tuple[float, float]:\n",
      "    samples = policy.generate(prompt, n=group_size)\n",
      "    rewards = [total_reward(prompt, s) for s in samples]\n",
      "    baseline = sum(rewards) / max(1, len(rewards))\n",
      "    advantages = [r - baseline for r in rewards]\n",
      "    policy.update(samples, advantages)\n",
      "    return sum(rewards) / len(rewards), sum(advantages) / len(advantages)\n",
      "\n",
      "# Train on toy addition prompts\n",
      "policy = TinyPolicy()\n",
      "prompts = [\"1 2\", \"5 7\", \"10 15\", \"3 3\", \"100 23\"]\n",
      "\n",
      "for step in range(50):\n",
      "    p = random.choice(prompts)\n",
      "    avg_r, avg_adv = grpo_step(policy, p, group_size=6)\n",
      "    if step % 10 == 0:\n",
      "        print(f\"Step {step:02d} | avg_reward={avg_r:.2f} | verbosity={policy.params['verbosity']:.2f}\")\n",
      "\n",
      "# Inference with structured output\n",
      "test = \"8 9\"\n",
      "print(policy.generate(test, n=1)[0])\n",
      "```\n",
      "\n",
      "What this shows:\n",
      "- You can structure outputs and reward them without a neural critic.\n",
      "- Group-relative advantages let you improve the policy using only rule-based signals.\n",
      "- Even a toy ‚Äúverbosity‚Äù parameter can learn to ‚Äúthink longer‚Äù if longer chains correlate with higher rewards.\n",
      "\n",
      "## Practical Tips If You‚Äôre Trying This\n",
      "\n",
      "- Start with rule-based rewards you can verify automatically (math, code tests, format checks).\n",
      "- Use structured tags to keep reasoning trace and answer separate for easy evaluation.\n",
      "- Warm-start with a small set of human CoT examples if outputs are messy or multilingual.\n",
      "- Consider rejection sampling to bootstrap a higher-quality SFT dataset from your RL model.\n",
      "- Add a final alignment stage to balance helpfulness, safety, and brevity.\n",
      "\n",
      "## Limitations to Keep in Mind\n",
      "\n",
      "- Early RL-only runs may mix languages or be hard to read‚Äîlanguage consistency rewards help.\n",
      "- Zero-shot results are prompt-sensitive‚Äîthoughtful prompts matter.\n",
      "- General-purpose interactive skills (function calling, multi-turn flows) may need extra data and objectives.\n",
      "- RL for software engineering is still compute- and evaluation-heavy; invest in good test harnesses.\n",
      "\n",
      "## Why This Matters\n",
      "\n",
      "DeepSeek-R1 shows that with the right reward signals and training pipeline, LLMs can autonomously grow strong reasoning behaviors‚Äîthen pass those skills down to smaller, cheaper models. It‚Äôs a practical blueprint for teams who want better reasoning without massive supervised datasets.\n",
      "\n",
      "## Key Takeaways\n",
      "\n",
      "- Reinforcement learning can directly incentivize reasoning in LLMs.\n",
      "- GRPO removes the critic, enabling scalable, stable training with group-relative advantages.\n",
      "- Rule-based rewards (accuracy + format) reduce reward hacking and increase interpretability.\n",
      "- A multi-stage pipeline (SFT ‚Üí RL ‚Üí rejection sampling SFT ‚Üí RL alignment) yields state-of-the-art results.\n",
      "- Distillation makes reasoning models deployable in resource-constrained environments.\n",
      "\n",
      "## Potential Applications\n",
      "\n",
      "- STEM tutoring, problem-solving assistants, and math-heavy analytics\n",
      "- Competitive programming and code generation with automatic test rewards\n",
      "- Scientific reasoning and long-context document analysis\n",
      "- Decision support systems that require transparent chains of thought\n",
      "- On-device or low-latency reasoning via distilled small models\n"
     ]
    }
   ],
   "source": [
    "print(result['final_report'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
