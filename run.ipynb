{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set path/directory\n",
    "- support for single file. Input file path.\n",
    "    - ex. `path = \"test/case1/deepseek r1.pdf\"`\n",
    "- support for multiple files. Input folder directory.\n",
    "    - ex. `path = \"test/case2/\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = [\"test/case1/deepseek r1.pdf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run system\n",
    "- `run()` requires 3 parameters\n",
    "    1. `path`: file or folder path\n",
    "    2. `print_log`: (bool) to print logs or not\n",
    "    3. `eval`: (bool) to run evaluations(LLM-based) or not \n",
    "- `run()` returns 3 outputs\n",
    "    1. final state\n",
    "    2. main graph visualization\n",
    "    3. sub graph visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 07:09:37 | main                 | INFO     | ğŸš€ Task Paper Processing Started. Cache directory: cache/20250819_070937/\n",
      "2025-08-19 07:09:37 | main                 | INFO     | ğŸ“‹ Process logs are being recorded\n",
      "2025-08-19 07:09:37 | main                 | INFO     | ğŸ“‚ Input path: ['test/case1/deepseek r1.pdf']\n",
      "2025-08-19 07:09:37 | main                 | INFO     | ğŸ“‹ Log file: cache/20250819_070937/process.log\n",
      "2025-08-19 07:09:37 | src.tracking         | INFO     | ğŸ“Š ì‹¤í–‰ ì¶”ì  ì‹œìŠ¤í…œ ì´ˆê¸°í™”: cache/20250819_070937/\n",
      "2025-08-19 07:09:37 | src.graph            | INFO     | ğŸ”§ Send API ê¸°ë°˜ ë³‘ë ¬ ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì‹œì‘...\n",
      "2025-08-19 07:09:37 | src.graph            | INFO     | âœ… Send API ê¸°ë°˜ ë³‘ë ¬ ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì™„ë£Œ\n",
      "2025-08-19 07:09:37 | main                 | INFO     | âš™ï¸  Starting workflow execution...\n",
      "2025-08-19 07:09:37 | src.tracking         | INFO     | ğŸ [extract] ì‹¤í–‰ ì‹œì‘: 07:09:37\n",
      "2025-08-19 07:09:37 | src.load_doc         | INFO     | ğŸ”„ Document extraction process started\n",
      "2025-08-19 07:09:37 | src.load_doc         | INFO     | ğŸ“‚ Processing paths: ['test/case1/deepseek r1.pdf']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba4ef14d45f4e508bfdec085a9cbe2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 07:09:46 | src.load_doc         | INFO     | ğŸ“„ Loaded 70 documents\n",
      "2025-08-19 07:09:46 | src.load_doc         | INFO     | ğŸ“Š Found papers: ['DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning']\n",
      "2025-08-19 07:09:46 | src.load_doc         | INFO     | ğŸ”§ Starting vectorstore creation\n",
      "2025-08-19 07:09:46 | sentence_transformers.SentenceTransformer | INFO     | Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-08-19 07:09:51 | src.load_doc         | INFO     | âœ… Embedder initialized with model: all-MiniLM-L6-v2\n",
      "2025-08-19 07:09:51 | src.load_doc         | INFO     | ğŸ”„ Creating vectorstore for: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n",
      "2025-08-19 07:09:52 | src.load_doc         | INFO     | ğŸ’¾ Vectorstore for 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning' saved to cache/20250819_070937/DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning/vectorstore/\n",
      "2025-08-19 07:09:52 | src.load_doc         | INFO     | âœ… All vectorstores created successfully (1 papers)\n",
      "2025-08-19 07:09:52 | src.load_doc         | INFO     | âœ… Document extraction completed successfully\n",
      "2025-08-19 07:09:52 | src.tracking         | INFO     | ğŸ [extract] ì‹¤í–‰ ì™„ë£Œ: 14.43ì´ˆ\n",
      "2025-08-19 07:09:52 | src.graph            | INFO     | ğŸš€ 1ê°œ ë…¼ë¬¸ì— ëŒ€í•œ ë³‘ë ¬ ìš”ì•½ ì‘ì—… ì‹œì‘...\n",
      "2025-08-19 07:09:52 | src.graph            | INFO     |   ğŸ“¤ 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning' â†’ Summary Subgraphë¡œ ì „ì†¡ (ì„¹ì…˜ 20ê°œ)\n",
      "2025-08-19 07:09:52 | src.tracking         | INFO     | ğŸ [extract_sections] ì‹¤í–‰ ì‹œì‘: 07:09:52\n",
      "2025-08-19 07:09:52 | src.tracking         | INFO     | ğŸ [extract_sections] ì‹¤í–‰ ì™„ë£Œ: 0.23ì´ˆ\n",
      "2025-08-19 07:09:52 | src.tracking         | INFO     | ğŸ [summarize_sections] ì‹¤í–‰ ì‹œì‘: 07:09:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 'DeepSeek-R1: Incentivizing Reasoning Capability in...' ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ (20ê°œ ì„¹ì…˜)\n",
      "â³ 'DeepSeek-R1: Incentivizing Reasoning Capability in...' ì„¹ì…˜ ìš”ì•½ ì¤‘... (20ê°œ ì„¹ì…˜)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 07:13:17 | src.tracking         | INFO     | ğŸ [summarize_sections] ì‹¤í–‰ ì™„ë£Œ: 204.61ì´ˆ\n",
      "2025-08-19 07:13:17 | src.tracking         | INFO     | ğŸ’° [summarize_sections] í† í° ì‚¬ìš©: 63896ê°œ (ì…ë ¥: 23243, ì¶œë ¥: 40653)\n",
      "2025-08-19 07:13:17 | src.tracking         | INFO     | ğŸ [create_final_summary] ì‹¤í–‰ ì‹œì‘: 07:13:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ğŸ“‘ ì „ì²´ ìš”ì•½ ì¸ë±ìŠ¤ ì €ì¥ë¨: 00 Index_All_Summaries.txt\n",
      "âœ… 'DeepSeek-R1: Incentivizing Reasoning Capability in...' ì„¹ì…˜ë³„ ìš”ì•½ ì™„ë£Œ (20ê°œ ì„¹ì…˜)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 07:14:10 | src.tracking         | INFO     | ğŸ [create_final_summary] ì‹¤í–‰ ì™„ë£Œ: 52.83ì´ˆ\n",
      "2025-08-19 07:14:10 | src.tracking         | INFO     | ğŸ’° [create_final_summary] í† í° ì‚¬ìš©: 14193ê°œ (ì…ë ¥: 12467, ì¶œë ¥: 1726)\n",
      "2025-08-19 07:14:10 | src.tracking         | INFO     | ğŸ [domain_agent] ì‹¤í–‰ ì‹œì‘: 07:14:10\n",
      "2025-08-19 07:14:10 | agents.domain_agent  | INFO     | ğŸ” ë„ë©”ì¸ ë¶„ì„ ì‹œì‘... ë¶„ì„ ëŒ€ìƒ: 1ê°œ ë…¼ë¬¸\n",
      "2025-08-19 07:14:10 | agents.domain_agent  | INFO     | ğŸ“„ ë¶„ì„ ë…¼ë¬¸: ['DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning']\n",
      "2025-08-19 07:14:10 | agents.domain_agent  | INFO     | ğŸ“‘ ë„ë©”ì¸ ë¶„ì„ ì¤‘...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ğŸ’¾ ìµœì¢… ìš”ì•½ ì €ì¥ë¨: Final_Summary.txt\n",
      "âœ… 'DeepSeek-R1: Incentivizing Reasoning Capability in...' ìµœì¢… ìš”ì•½ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 07:14:16 | agents.domain_agent  | INFO     |    âœ… DeepSeek-R1: Incentivizing Rea... ë¶„ì•¼: {'main_field': ['Computer Science'], 'sub_field': ['Artificial Intelligence', 'Machine Learning', 'Natural Language Processing', 'Reinforcement Learning', 'Knowledge Distillation', 'Open-Source AI']}\n",
      "2025-08-19 07:14:16 | agents.domain_agent  | INFO     | âœ… ë„ë©”ì¸ ë¶„ì„ ì™„ë£Œ!\n",
      "2025-08-19 07:14:16 | src.tracking         | INFO     | ğŸ [domain_agent] ì‹¤í–‰ ì™„ë£Œ: 6.23ì´ˆ\n",
      "2025-08-19 07:14:16 | src.tracking         | INFO     | ğŸ [analysis_planner] ì‹¤í–‰ ì‹œì‘: 07:14:16\n",
      "2025-08-19 07:14:16 | agents.analysis_plan_router | INFO     | ğŸ¯ ë¶„ì„ ê³„íš ìƒì„± ì¤‘... ë…¼ë¬¸ ìˆ˜: 1\n",
      "2025-08-19 07:14:16 | agents.analysis_plan_router | INFO     | ğŸ“– ë‹¨ì¼ ë…¼ë¬¸ ë¶„ì„: 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning'\n",
      "2025-08-19 07:14:19 | agents.analysis_plan_router | INFO     | âœ… ë¶„ì„ ê³„íš ê²°ì •: ë‹¨ì¼ ë„ë©”ì¸ (single)\n",
      "2025-08-19 07:14:19 | src.tracking         | INFO     | ğŸ [analysis_planner] ì‹¤í–‰ ì™„ë£Œ: 3.53ì´ˆ\n",
      "2025-08-19 07:14:19 | src.graph            | INFO     | ğŸ§­ ë¶„ì„ í”Œëœ í™•ì •: single\n",
      "2025-08-19 07:14:19 | src.tracking         | INFO     | ğŸ [write_agent] ì‹¤í–‰ ì‹œì‘: 07:14:19\n",
      "2025-08-19 07:14:19 | agents.tools.python_repl | INFO     | Python REPL Tool enabled\n",
      "2025-08-19 07:14:19 | agents.write_agent   | INFO     | ğŸ”§ Python REPL Tool í™œì„±í™”\n",
      "2025-08-19 07:14:19 | agents.write_agent   | INFO     | ğŸ“ Write agent ì‹¤í–‰ (ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼ ìƒì„±, Markdown ì¶œë ¥)...\n",
      "2025-08-19 07:15:24 | agents.write_agent   | INFO     | âœ… ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ\n",
      "2025-08-19 07:15:24 | src.tracking         | INFO     | ğŸ [write_agent] ì‹¤í–‰ ì™„ë£Œ: 64.78ì´ˆ\n",
      "2025-08-19 07:15:24 | src.tracking         | INFO     | ğŸ¯ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ\n",
      "2025-08-19 07:15:24 | src.tracking         | INFO     | â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: 347.02ì´ˆ\n",
      "2025-08-19 07:15:24 | src.tracking         | INFO     | ğŸ’° ì´ í† í° ì‚¬ìš©: 78089ê°œ\n",
      "2025-08-19 07:15:24 | src.tracking         | INFO     | ğŸ’³ ì´ ì˜ˆìƒ ë¹„ìš©: $0.0000\n",
      "2025-08-19 07:15:24 | main                 | INFO     | ğŸ“Š ì‹¤í–‰ ì¶”ì  ì™„ë£Œ - ì—ì´ì „íŠ¸ 7ê°œ, ì´ ì‹œê°„ 347.02ì´ˆ\n",
      "2025-08-19 07:15:24 | main                 | INFO     | analysis_report ì—†ìŒ - ì €ì¥ ê±´ë„ˆëœ€\n",
      "2025-08-19 07:15:24 | main                 | INFO     | ğŸ’¾ ì €ì¥ë¨: cache/20250819_070937/report.md\n",
      "2025-08-19 07:15:24 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:15:27 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:15:29 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:15:32 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:15:37 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:15:40 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:15:48 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:15:50 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:15:56 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:15:59 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:03 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:06 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:10 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:12 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:17 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:20 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:25 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:28 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:33 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:36 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:41 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:44 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:48 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:50 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:54 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:58 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:02 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:06 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:09 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:13 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:15 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:19 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:22 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:25 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:29 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:31 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:35 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:38 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:42 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:44 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:48 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:50 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:54 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:56 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:18:00 | config.agent_llm     | INFO     | ğŸ”§ eval_judge LLM ìƒì„±: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:18:03 | main                 | INFO     | âœ… í‰ê°€ ì™„ë£Œ\n",
      "2025-08-19 07:18:03 | main                 | INFO     | âœ… Task Paper Processing Completed Successfully\n"
     ]
    }
   ],
   "source": [
    "from main import run\n",
    "\n",
    "result, main_vis, sub_vis = run(path, print_log=True, eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results after execution\n",
    "\n",
    "All results will be stored in `cache` folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
