{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set path/directory\n",
    "- support for single file. Input file path.\n",
    "    - ex. `path = \"test/case1/deepseek r1.pdf\"`\n",
    "- support for multiple files. Input folder directory.\n",
    "    - ex. `path = \"test/case2/\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = [\"test/case1/deepseek r1.pdf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run system\n",
    "- `run()` requires 3 parameters\n",
    "    1. `path`: file or folder path\n",
    "    2. `print_log`: (bool) to print logs or not\n",
    "    3. `eval`: (bool) to run evaluations(LLM-based) or not \n",
    "- `run()` returns 3 outputs\n",
    "    1. final state\n",
    "    2. main graph visualization\n",
    "    3. sub graph visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 07:09:37 | main                 | INFO     | 🚀 Task Paper Processing Started. Cache directory: cache/20250819_070937/\n",
      "2025-08-19 07:09:37 | main                 | INFO     | 📋 Process logs are being recorded\n",
      "2025-08-19 07:09:37 | main                 | INFO     | 📂 Input path: ['test/case1/deepseek r1.pdf']\n",
      "2025-08-19 07:09:37 | main                 | INFO     | 📋 Log file: cache/20250819_070937/process.log\n",
      "2025-08-19 07:09:37 | src.tracking         | INFO     | 📊 실행 추적 시스템 초기화: cache/20250819_070937/\n",
      "2025-08-19 07:09:37 | src.graph            | INFO     | 🔧 Send API 기반 병렬 워크플로우 구성 시작...\n",
      "2025-08-19 07:09:37 | src.graph            | INFO     | ✅ Send API 기반 병렬 워크플로우 구성 완료\n",
      "2025-08-19 07:09:37 | main                 | INFO     | ⚙️  Starting workflow execution...\n",
      "2025-08-19 07:09:37 | src.tracking         | INFO     | 🏁 [extract] 실행 시작: 07:09:37\n",
      "2025-08-19 07:09:37 | src.load_doc         | INFO     | 🔄 Document extraction process started\n",
      "2025-08-19 07:09:37 | src.load_doc         | INFO     | 📂 Processing paths: ['test/case1/deepseek r1.pdf']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba4ef14d45f4e508bfdec085a9cbe2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 07:09:46 | src.load_doc         | INFO     | 📄 Loaded 70 documents\n",
      "2025-08-19 07:09:46 | src.load_doc         | INFO     | 📊 Found papers: ['DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning']\n",
      "2025-08-19 07:09:46 | src.load_doc         | INFO     | 🔧 Starting vectorstore creation\n",
      "2025-08-19 07:09:46 | sentence_transformers.SentenceTransformer | INFO     | Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-08-19 07:09:51 | src.load_doc         | INFO     | ✅ Embedder initialized with model: all-MiniLM-L6-v2\n",
      "2025-08-19 07:09:51 | src.load_doc         | INFO     | 🔄 Creating vectorstore for: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n",
      "2025-08-19 07:09:52 | src.load_doc         | INFO     | 💾 Vectorstore for 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning' saved to cache/20250819_070937/DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning/vectorstore/\n",
      "2025-08-19 07:09:52 | src.load_doc         | INFO     | ✅ All vectorstores created successfully (1 papers)\n",
      "2025-08-19 07:09:52 | src.load_doc         | INFO     | ✅ Document extraction completed successfully\n",
      "2025-08-19 07:09:52 | src.tracking         | INFO     | 🏁 [extract] 실행 완료: 14.43초\n",
      "2025-08-19 07:09:52 | src.graph            | INFO     | 🚀 1개 논문에 대한 병렬 요약 작업 시작...\n",
      "2025-08-19 07:09:52 | src.graph            | INFO     |   📤 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning' → Summary Subgraph로 전송 (섹션 20개)\n",
      "2025-08-19 07:09:52 | src.tracking         | INFO     | 🏁 [extract_sections] 실행 시작: 07:09:52\n",
      "2025-08-19 07:09:52 | src.tracking         | INFO     | 🏁 [extract_sections] 실행 완료: 0.23초\n",
      "2025-08-19 07:09:52 | src.tracking         | INFO     | 🏁 [summarize_sections] 실행 시작: 07:09:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'DeepSeek-R1: Incentivizing Reasoning Capability in...' 섹션 추출 완료 (20개 섹션)\n",
      "⏳ 'DeepSeek-R1: Incentivizing Reasoning Capability in...' 섹션 요약 중... (20개 섹션)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 07:13:17 | src.tracking         | INFO     | 🏁 [summarize_sections] 실행 완료: 204.61초\n",
      "2025-08-19 07:13:17 | src.tracking         | INFO     | 💰 [summarize_sections] 토큰 사용: 63896개 (입력: 23243, 출력: 40653)\n",
      "2025-08-19 07:13:17 | src.tracking         | INFO     | 🏁 [create_final_summary] 실행 시작: 07:13:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      📑 전체 요약 인덱스 저장됨: 00 Index_All_Summaries.txt\n",
      "✅ 'DeepSeek-R1: Incentivizing Reasoning Capability in...' 섹션별 요약 완료 (20개 섹션)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 07:14:10 | src.tracking         | INFO     | 🏁 [create_final_summary] 실행 완료: 52.83초\n",
      "2025-08-19 07:14:10 | src.tracking         | INFO     | 💰 [create_final_summary] 토큰 사용: 14193개 (입력: 12467, 출력: 1726)\n",
      "2025-08-19 07:14:10 | src.tracking         | INFO     | 🏁 [domain_agent] 실행 시작: 07:14:10\n",
      "2025-08-19 07:14:10 | agents.domain_agent  | INFO     | 🔍 도메인 분석 시작... 분석 대상: 1개 논문\n",
      "2025-08-19 07:14:10 | agents.domain_agent  | INFO     | 📄 분석 논문: ['DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning']\n",
      "2025-08-19 07:14:10 | agents.domain_agent  | INFO     | 📑 도메인 분석 중...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      💾 최종 요약 저장됨: Final_Summary.txt\n",
      "✅ 'DeepSeek-R1: Incentivizing Reasoning Capability in...' 최종 요약 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 07:14:16 | agents.domain_agent  | INFO     |    ✅ DeepSeek-R1: Incentivizing Rea... 분야: {'main_field': ['Computer Science'], 'sub_field': ['Artificial Intelligence', 'Machine Learning', 'Natural Language Processing', 'Reinforcement Learning', 'Knowledge Distillation', 'Open-Source AI']}\n",
      "2025-08-19 07:14:16 | agents.domain_agent  | INFO     | ✅ 도메인 분석 완료!\n",
      "2025-08-19 07:14:16 | src.tracking         | INFO     | 🏁 [domain_agent] 실행 완료: 6.23초\n",
      "2025-08-19 07:14:16 | src.tracking         | INFO     | 🏁 [analysis_planner] 실행 시작: 07:14:16\n",
      "2025-08-19 07:14:16 | agents.analysis_plan_router | INFO     | 🎯 분석 계획 생성 중... 논문 수: 1\n",
      "2025-08-19 07:14:16 | agents.analysis_plan_router | INFO     | 📖 단일 논문 분석: 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning'\n",
      "2025-08-19 07:14:19 | agents.analysis_plan_router | INFO     | ✅ 분석 계획 결정: 단일 도메인 (single)\n",
      "2025-08-19 07:14:19 | src.tracking         | INFO     | 🏁 [analysis_planner] 실행 완료: 3.53초\n",
      "2025-08-19 07:14:19 | src.graph            | INFO     | 🧭 분석 플랜 확정: single\n",
      "2025-08-19 07:14:19 | src.tracking         | INFO     | 🏁 [write_agent] 실행 시작: 07:14:19\n",
      "2025-08-19 07:14:19 | agents.tools.python_repl | INFO     | Python REPL Tool enabled\n",
      "2025-08-19 07:14:19 | agents.write_agent   | INFO     | 🔧 Python REPL Tool 활성화\n",
      "2025-08-19 07:14:19 | agents.write_agent   | INFO     | 📝 Write agent 실행 (블로그 스타일 생성, Markdown 출력)...\n",
      "2025-08-19 07:15:24 | agents.write_agent   | INFO     | ✅ 블로그 포스트 생성 완료\n",
      "2025-08-19 07:15:24 | src.tracking         | INFO     | 🏁 [write_agent] 실행 완료: 64.78초\n",
      "2025-08-19 07:15:24 | src.tracking         | INFO     | 🎯 전체 워크플로우 완료\n",
      "2025-08-19 07:15:24 | src.tracking         | INFO     | ⏱️  총 실행 시간: 347.02초\n",
      "2025-08-19 07:15:24 | src.tracking         | INFO     | 💰 총 토큰 사용: 78089개\n",
      "2025-08-19 07:15:24 | src.tracking         | INFO     | 💳 총 예상 비용: $0.0000\n",
      "2025-08-19 07:15:24 | main                 | INFO     | 📊 실행 추적 완료 - 에이전트 7개, 총 시간 347.02초\n",
      "2025-08-19 07:15:24 | main                 | INFO     | analysis_report 없음 - 저장 건너뜀\n",
      "2025-08-19 07:15:24 | main                 | INFO     | 💾 저장됨: cache/20250819_070937/report.md\n",
      "2025-08-19 07:15:24 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:15:27 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:15:29 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:15:32 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:15:37 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:15:40 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:15:48 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:15:50 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:15:56 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:15:59 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:03 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:06 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:10 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:12 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:17 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:20 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:25 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:28 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:33 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:36 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:41 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:44 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:48 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:50 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:16:54 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:16:58 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:02 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:06 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:09 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:13 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:15 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:19 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:22 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:25 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:29 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:31 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:35 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:38 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:42 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:44 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:48 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:50 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:17:54 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:17:56 | absl                 | INFO     | Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-19 07:18:00 | config.agent_llm     | INFO     | 🔧 eval_judge LLM 생성: {'model': 'gpt-4.1-mini', 'temperature': 0}\n",
      "2025-08-19 07:18:03 | main                 | INFO     | ✅ 평가 완료\n",
      "2025-08-19 07:18:03 | main                 | INFO     | ✅ Task Paper Processing Completed Successfully\n"
     ]
    }
   ],
   "source": [
    "from main import run\n",
    "\n",
    "result, main_vis, sub_vis = run(path, print_log=True, eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results after execution\n",
    "\n",
    "All results will be stored in `cache` folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
