# Automated Research Paper Analysis and Summarization

ë³¸ ê³¼ì œëŠ” LangGraphë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¤„ì¡ŒìŠµë‹ˆë‹¤.
ì œì•ˆëœ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì— ëŒ€í•œ ê²°ê³¼ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## Repo

```
main.py     				# run
run.ipynb   				# run(recommend)
ğŸ“‚[agents]					# LangGraph agents
	analysis_comparison_agent.py
	analysis_cross_domain_agent.py
	analysis_lit_review_agent.py
	analysis_plan_router.py
	domain_agent.py
	summary_agent.py
	write_agent.py
	ğŸ“‚[tools] 					# tools for agents
		arxiv.py      				# arXiv searching tool
		python_repl.py  			# pythonREPL tool
		vectorstore.py  			# vectorstore retriever tool
		web_search.py   			# web search tool
ğŸ“‚[cache]   				# results after execution
	(...)
ğŸ“‚[config]
	agent_config.json   		# agents llm config
	agent_llm.py 				# agents llm calling
	logging_config.py   		# logger config
ğŸ“‚[src]
	graph.py      				# graph(LangGraph)
	state.py      				# state(LangGraph)
	load_doc.py   				# document(pdf) loader
	tracking.py   				# input/output token tracking
ğŸ“‚[test]       	  
	ğŸ“‚[case1]     				# paper for "single-paper analysis"
	ğŸ“‚[case2]     				# papers for "multi-paper comparison"
	ğŸ“‚[case3]     				# papers for "literature review synthesis"
	ğŸ“‚[caseE]     				# paper for "cross-domain paper"
ğŸ“‚[test_output] 			# example outputs
	(...)
```

## Topology

```
[PDF Ingest]
   â†’ (Extract)
      â””â†’ (Documant Parsing) â†’ (Paragraph Chunking) â†’ (Embedding)
   â†’ [Summary Subgraph]
	  â””â†’ (Summary: section) â†’ (Summary: paper)
   â†’ [Domain Identity Agent]
   â†’ [Analysis Plan Router] â”€â”¬â”€â†’ [Comparison Agent]
                          	 â”œâ”€â†’ [LitReview Agent]
							 â””â”€â†’ [Cross-Domain Agent]
   â†’ [Writing Agent]
```

![graph workflow](./image/graph.png)

## Agent descriptions

ìì„¸í•œ ë‚´ìš©ì€ `agents/Info.md`ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.


---

# Installation

## 1. GROBID ì„¤ì¹˜

**DockerëŠ” í•„ìˆ˜ì ìœ¼ë¡œ ì„¤ì¹˜ ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.** ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´, ë‹¤ìŒì„ ì°¸ê³ í•˜ì„¸ìš”. [ë§í¬](https://docs.docker.com/get-started/docker-overview/)
GROBIDëŠ” scientific paper parsingì— íŠ¹í™”ëœ ML ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ë³¸ ì‹œìŠ¤í…œì—ì„œëŠ” ì…ë ¥ëœ pdfì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” toolë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
    - GROBID ì„¤ì¹˜ì— ëŒ€í•œ ì„¸ë¶€ ì •ë³´ëŠ” ë‹¤ìŒì„ ì°¸ê³ í•˜ì„¸ìš”. [ë§í¬](https://grobid.readthedocs.io/en/latest/Grobid-docker/)
- ì•„ë˜ ë‘ ì˜µì…˜ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ í„°ë¯¸ë„ì—ì„œ ì„¤ì¹˜ ë° êµ¬ë™í•˜ì—¬ docker containerê°€ ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

### âŒ opt1. **Deep Learning and CRF image**

- ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ê°•ë ¥í•œ ì¶”ì¶œ ì„±ëŠ¥ì„ ë‚´ëŠ” ë²„ì „ì…ë‹ˆë‹¤.
- ì„¤ì¹˜ì— 10GBì˜ ê³µê°„ì´ í•„ìš”í•©ë‹ˆë‹¤.
- ë‹¨, í˜„ì¬(25.08.16 ê¸°ì¤€) Linux OSì—ì„œë§Œ ì§€ì›ë©ë‹ˆë‹¤.
    - ë³¸ ê³¼ì œì—ì„œ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

```
docker pull grobid/grobid:0.8.2
```

```
docker run --rm --gpus all --init --ulimit core=0 -p 8070:8070 grobid/grobid:0.8.2
```

### âœ… opt2. **CRF-only image**

- CRF ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì¶œì„ ìˆ˜í–‰í•˜ëŠ” lightweight ë²„ì „ì…ë‹ˆë‹¤.
- ì„¤ì¹˜ì— 300MBì˜ ê³µê°„ì´ í•„ìš”í•©ë‹ˆë‹¤.
- Linux/Windows/Mac OSì—ì„œ ì‘ë™ë©ë‹ˆë‹¤.
    - ë³¸ ê³¼ì œëŠ” í•´ë‹¹ imageë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.

```
docker pull lfoppiano/grobid:0.8.2
```

```
docker run --rm --init --ulimit core=0 -p 8070:8070 lfoppiano/grobid:0.8.2
```

## 2. ê°€ìƒí™˜ê²½ ì„¸íŒ…

- ê°€ìƒí™˜ê²½ ì„¤ì •
    
    ```
    conda create -n taskpaper python=3.11 
    conda activate taskpaper
    ```
    
- íŒ¨í‚¤ì§€ ì„¤ì¹˜
    
    ```
    pip install -r requirments.txt
    ```
    

---

# Execution

- `run.ipynb`ë¥¼ í™•ì¸í•˜ì„¸ìš”.

# Results

ì‹¤í–‰ ë° ë¶„ì„ì´ ì™„ë£Œë˜ë©´ `cache` í´ë”ì— ê²°ê³¼ê°€ ì €ì¥ë©ë‹ˆë‹¤.
```
ğŸ“[cache]
	ğŸ“[(timestamp)]
		ğŸ“[paper_1]
			ğŸ“[vectorstore]		# raw text vector stored in local
			ğŸ“[summaries]		# all summary included
				(...)
		ğŸ“[paper_2]
			(...)
		ğŸ“(...)
		eval.json				# evaluation results
		process.log				# log
		execution_tracking.json # tracking results
		report.md				# final output of system
```
