import logging
import os
from datetime import datetime
from src.graph import GraphWorkflow
from src.tracking import init_tracking, finalize_tracking
import re
from src.eval import evaluate_with_judge  
from src.eval import rouge_l, bertscore_f1_single 
import json
from typing import Any, List 


def setup_logging(cache_dir: str = None, print_log: bool = True):
    """
    Ï†ÑÏ≤¥ ÌîÑÎ°úÏ†ùÌä∏Ïùò Î°úÍπÖÏùÑ ÏÑ§Ï†ïÌïòÎäî Ìï®Ïàò
    
    Args:
        cache_dir (str): Î°úÍ∑∏ ÌååÏùºÏùÑ Ï†ÄÏû•Ìï† Ï∫êÏãú ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú
        print_log (bool): ÏΩòÏÜîÏóê Î°úÍ∑∏Î•º Ï∂úÎ†•Ìï†ÏßÄ Ïó¨Î∂Ä
    
    Returns:
        str: Î°úÍ∑∏ ÌååÏùº Í≤ΩÎ°ú (ÌååÏùº Î°úÍ∑∏Í∞Ä ÌôúÏÑ±ÌôîÎêú Í≤ΩÏö∞)
    """
    # Î°úÍ∑∏ Ìè¨Îß∑ ÏÑ§Ï†ï
    formatter = logging.Formatter(
        fmt='%(asctime)s | %(name)-20s | %(levelname)-8s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Î£®Ìä∏ Î°úÍ±∞ ÏÑ§Ï†ï
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
    # Î™®Îì† Ìï∏Îì§Îü¨ ÏôÑÏ†Ñ Ï†úÍ±∞
    root_logger.handlers.clear()
    
    log_file_path = None
    
    # ÌååÏùº Î°úÍπÖ ÏÑ§Ï†ï (Ìï≠ÏÉÅ Ï∂îÍ∞Ä)
    if cache_dir:
        os.makedirs(cache_dir, exist_ok=True)
        log_file_path = os.path.join(cache_dir, "process.log")
        
        file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
        file_handler.setFormatter(formatter)
        file_handler.setLevel(logging.INFO)
        root_logger.addHandler(file_handler)
    
    # ÏΩòÏÜî Î°úÍπÖÏùÄ print_log=TrueÏùº ÎïåÎßå Ï∂îÍ∞Ä
    if print_log:
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        console_handler.setLevel(logging.INFO)
        root_logger.addHandler(console_handler)
        
    # Ï†ÑÌåå Î∞©ÏßÄ - Ïù¥Í≤å Ï§ëÏöî!
    root_logger.propagate = False
    
    # ÌååÏùº Î°úÍπÖ ÏÑ§Ï†ï (cache_dirÏù¥ Ï†úÍ≥µÎêú Í≤ΩÏö∞)
    if cache_dir:
        os.makedirs(cache_dir, exist_ok=True)
        log_file_path = os.path.join(cache_dir, "process.log")
        
        file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
        file_handler.setFormatter(formatter)
        file_handler.setLevel(logging.INFO)
        root_logger.addHandler(file_handler)
        
        if print_log:
            # Î°úÍ∑∏ ÌååÏùº Ï¥àÍ∏∞Ìôî Î©îÏãúÏßÄÎäî Ìïú Î≤àÎßå Ï∂úÎ†•
            pass
    
    return log_file_path
# output_language: enÍ≥º ko Ï§ëÏóêÏÑúÎßå ÏÑ†ÌÉù Í∞ÄÎä•

def _safe_name(base: str) -> str:
    base = re.sub(r"[^\w\- ]+", "", str(base)).strip().replace(" ", "_")
    return base[:80] if base else "output"


def _save_markdown(cache_dir: str, name_base: str, content: str, suffix: str) -> str:
    try:
        os.makedirs(cache_dir, exist_ok=True)
        filename = "report.md"
        path = os.path.join(cache_dir, filename)
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)
        logging.getLogger(__name__).info(f"üíæ Ï†ÄÏû•Îê®: {path}")
        return path
    except Exception as e:
        logging.getLogger(__name__).error(f"ÌååÏùº Ï†ÄÏû• Ïã§Ìå®({_safe_name(name_base)}): {e}")
        return ""


def _save_json(cache_dir: str, name_base: str, data: dict, suffix: str) -> str:
    """ÌèâÍ∞Ä Í≤∞Í≥º Ï†ÄÏû•(Î°úÍπÖ ÏµúÏÜåÌôî)."""
    try:
        os.makedirs(cache_dir, exist_ok=True)
        filename = "eval.json"
        path = os.path.join(cache_dir, filename)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return path
    except Exception:
        return ""


def run(path, print_log: bool = True, output_language: str = "en", eval: bool = False):
    """
    ÎÖºÎ¨∏ Ï≤òÎ¶¨ ÏõåÌÅ¨ÌîåÎ°úÏö∞Î•º Ïã§ÌñâÌïòÎäî Î©îÏù∏ Ìï®Ïàò
    
    Args:
        path: Ï≤òÎ¶¨Ìï† PDF Í≤ΩÎ°ú (Î¶¨Ïä§Ìä∏)
        print_log (bool): ÏΩòÏÜîÏóê Î°úÍ∑∏Î•º Ï∂úÎ†•Ìï†ÏßÄ Ïó¨Î∂Ä. TrueÏù¥Î©¥ ÏΩòÏÜîÍ≥º ÌååÏùº Î™®ÎëêÏóê Ï∂úÎ†•, FalseÏù¥Î©¥ ÌååÏùºÏóêÎßå Ï∂úÎ†•
        eval (bool): TrueÎ©¥ ÌèâÍ∞Ä ÏßÄÌëúÎ•º ÏÉùÏÑ±ÌïòÍ≥† cache ÎîîÎ†âÌÜ†Î¶¨Ïóê Ï†ÄÏû•
    
    Returns:
        dict: Ï≤òÎ¶¨ Í≤∞Í≥º
    """
    # Ï∫êÏãú ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ± (Î°úÍ∑∏ ÌååÏùºÏö©)
    cache_dir = "cache/" + f"{datetime.now().strftime('%Y%m%d_%H%M%S')}/"
    os.makedirs(cache_dir, exist_ok=True)
    # Î°úÍπÖ ÏÑ§Ï†ï
    log_file_path = setup_logging(cache_dir=cache_dir, print_log=print_log)
    
    # Î©îÏù∏ Î°úÍ±∞ ÏÉùÏÑ±
    logger = logging.getLogger(__name__)
    logger.info(f"üöÄ Task Paper Processing Started. Cache directory: {cache_dir}")
    logger.info(f"üìã Process logs are being recorded")
    logger.info(f"üìÇ Input path: {path}")
    
    if log_file_path:
        logger.info(f"üìã Log file: {log_file_path}")
    
    if not print_log:
        logger.info("üîá Console logging disabled - logs will only be saved to file")
    
    try:
        # Ï∂îÏ†Å ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî
        init_tracking(cache_dir)
        
        # ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïã§Ìñâ
        workflow = GraphWorkflow()
        app = workflow.build_workflow()
        
        logger.info("‚öôÔ∏è  Starting workflow execution...")
        # cache_dirÏùÑ stateÏóê Ìè¨Ìï®ÌïòÏó¨ Ï†ÑÎã¨
        initial_state = {
            "path": path,
            "cache_dir": cache_dir
        }
        result = app.invoke(initial_state)
        
        # Ï∂îÏ†Å Ï†ïÎ≥¥ ÎßàÎ¨¥Î¶¨ Î∞è ÏöîÏïΩ ÏÉùÏÑ±
        tracking_summary = finalize_tracking()
        if tracking_summary:
            logger.info(f"üìä Ïã§Ìñâ Ï∂îÏ†Å ÏôÑÎ£å - ÏóêÏù¥Ï†ÑÌä∏ {tracking_summary['total_agents']}Í∞ú, Ï¥ù ÏãúÍ∞Ñ {tracking_summary['total_duration_seconds']:.2f}Ï¥à")
            result["tracking_summary"] = tracking_summary
        
        # Í≤∞Í≥º Ï†ÄÏû•: analysis_reportÏôÄ final_reportÎ•º mdÎ°ú Ï†ÄÏû•
        analysis_plan = result.get("analysis_plan")
        # Ï†ÄÏû•Ïö© Ïù¥Î¶Ñ Î≤†Ïù¥Ïä§ Í≤∞Ï†ï
        if analysis_plan == "single":
            # Îã®ÏùºÏù∏ Í≤ΩÏö∞ summaryÏùò Ï≤´ ÎÖºÎ¨∏ Ï†úÎ™© ÏÇ¨Ïö©
            summaries = result.get("final_summary", {}) or {}
            name_base = next(iter(summaries.keys()), analysis_plan or "result")
        else:
            name_base = analysis_plan or "analysis"
        
        # analysis_report Ïö∞ÏÑ† Ï†ÄÏû•, ÏóÜÏúºÎ©¥ analysis_resultÎ°ú Ìè¥Î∞±
        analysis_text = result.get("analysis_report") or result.get("analysis_result")
        if analysis_text:
            _save_markdown(cache_dir, name_base, analysis_text, "analysis_report")
        else:
            logger.info("analysis_report ÏóÜÏùå - Ï†ÄÏû• Í±¥ÎÑàÎúÄ")
        
        if result.get("final_report"):
            _save_markdown(cache_dir, name_base, result["final_report"], "final_report")
        else:
            logger.info("final_report ÏóÜÏùå - Ï†ÄÏû• Í±¥ÎÑàÎúÄ")
        
        # ---------- ÌèâÍ∞Ä (ÏòµÏÖò) ----------
        if eval:
            eval_outputs: dict[str, dict[str, Any]] = {}

            # Í≥µÌÜµ evidence: Í∞Å ÎÖºÎ¨∏Ïùò ÏµúÏ¢Ö ÏöîÏïΩ
            final_summaries: dict = result.get("final_summary", {}) or {}
            final_summary_texts: List[str] = list(final_summaries.values()) if isinstance(final_summaries, dict) else []

            # 1) domain_agent (judge)
            if result.get("paper_domain"):
                candidate = json.dumps(result["paper_domain"], ensure_ascii=False)
                r = evaluate_with_judge("domain_agent", candidate=candidate, evidences=final_summary_texts)
                eval_outputs["domain_agent"] = {"target": r.target, "scores": r.scores, "details": r.details}

            # 2) analysis_plan_router (judge)
            if result.get("analysis_plan"):
                titles = list(final_summaries.keys())
                evidence_text = f"num_papers={len(titles)}; titles={', '.join(titles[:5])}"
                r = evaluate_with_judge("analysis_plan_router", candidate=result["analysis_plan"], evidences=[evidence_text])
                eval_outputs["analysis_plan_router"] = {"target": r.target, "scores": r.scores, "details": r.details}

            # 3) summary_agent(section): ÏÑπÏÖò ÏõêÎ¨∏Í≥º 1:1 ÎπÑÍµê, LLM Ìò∏Ï∂ú ÏóÜÏùå
            section_summaries: dict = result.get("section_summaries", {}) or {}
            section_texts: dict = result.get("section_texts", {}) or {}
            per_section_details: List[dict] = []
            per_section_avg_scores: List[float] = []

            def _sec_sort_key(k: str) -> int:
                m = re.match(r"^\s*(\d+)\.", k)
                return int(m.group(1)) if m else 10**9

            for paper_title, summaries in section_summaries.items():
                originals_map = section_texts.get(paper_title, {}) or {}
                if not originals_map:
                    continue
                keys_sorted = sorted(list(originals_map.keys()), key=_sec_sort_key)
                originals_seq = [originals_map[k] for k in keys_sorted]
                # Í∏∏Ïù¥ ÎßûÏ∂îÍ∏∞
                n = min(len(summaries), len(originals_seq))
                for i in range(n):
                    pred = summaries[i]
                    ref = originals_seq[i]
                    rl = rouge_l(pred, ref)
                    bf = bertscore_f1_single(pred, ref)
                    per_section_details.append({
                        "paper": paper_title,
                        "section": keys_sorted[i] if i < len(keys_sorted) else f"{i+1}",
                        "rouge_l": rl,
                        "bertscore_f1": bf,
                    })
                    per_section_avg_scores.append((rl + bf) / 2.0)

            eval_outputs["summary_agent(section)"] = {
                "target": "summary_agent(section)",
                "scores": {"avg_score": (sum(per_section_avg_scores) / len(per_section_avg_scores)) if per_section_avg_scores else 0.0},
                "details": {"per_section": per_section_details},
            }

            # 4) summary_agent(final): ÏµúÏ¢Ö ÏöîÏïΩ vs ÏÑπÏÖò ÏöîÏïΩ Ìï©Î≥∏, 1:1 ÎπÑÍµê (ÎÖºÎ¨∏Î≥Ñ ÏÇ∞Ï∂ú ÌõÑ ÌèâÍ∑†)
            per_paper_details: List[dict] = []
            rl_vals: List[float] = []
            bf_vals: List[float] = []
            for paper_title, final_sum in final_summaries.items():
                sec_sums = section_summaries.get(paper_title, [])
                ref = "\n\n".join(sec_sums)
                if not final_sum or not ref:
                    continue
                rl = rouge_l(final_sum, ref)
                bf = bertscore_f1_single(final_sum, ref)
                per_paper_details.append({"paper": paper_title, "rouge_l": rl, "bertscore_f1": bf})
                rl_vals.append(rl)
                bf_vals.append(bf)
            avg_rl = (sum(rl_vals) / len(rl_vals)) if rl_vals else 0.0
            avg_bf = (sum(bf_vals) / len(bf_vals)) if bf_vals else 0.0
            eval_outputs["summary_agent(final)"] = {
                "target": "summary_agent(final)",
                "scores": {"rouge_l": avg_rl, "bertscore_f1": avg_bf},
                "details": {"per_paper": per_paper_details},
            }

            # 5) Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏ (judge)
            analysis_agent_map = {
                "comparison": "comparison_agent",
                "cross_domain": "cross_domain_agent",
                "literature_review": "lit_review_agent",
            }
            analysis_plan = result.get("analysis_plan")
            if result.get("analysis_report") and analysis_plan in analysis_agent_map:
                agent_name = analysis_agent_map[analysis_plan]
                r = evaluate_with_judge(agent_name, candidate=result["analysis_report"], evidences=final_summary_texts)
                eval_outputs[agent_name] = {"target": r.target, "scores": r.scores, "details": r.details}

            # 6) ÏµúÏ¢Ö Í∏Ä(write_agent) (judge + Íµ¨Ï°∞/Í∞ÄÎèÖÏÑ±)
            if result.get("final_report"):
                evs = [result.get("analysis_report")] if result.get("analysis_report") else final_summary_texts
                r = evaluate_with_judge("write_agent", candidate=result["final_report"], evidences=evs, metrics=["structure", "readability"])
                eval_outputs["write_agent"] = {"target": r.target, "scores": r.scores, "details": r.details}

            # Ï†ÄÏû• (Í≤ΩÎ°úÎßå Î∞òÌôò, Ï∂îÍ∞Ä Î°úÍπÖ ÏóÜÏùå)
            _save_json(cache_dir, name_base, eval_outputs, "eval_results")
            # ÏôÑÎ£å Î°úÍπÖÎßå
            logger.info("‚úÖ ÌèâÍ∞Ä ÏôÑÎ£å")
        
        logger.info("‚úÖ Task Paper Processing Completed Successfully")
        return result, app, workflow
        
    except Exception as e:
        logger.error(f"‚ùå Processing failed: {str(e)}")
        logger.error(f"üîç Error details: {repr(e)}")
        raise e