This section describes the composition and evaluation framework for MusicEval, introducing 31 models drawn from 14 TTM and 7 TTA systems to cover broad architectural diversity; 25 systems are publicly accessible (with 6 providing only demo audio) and 6 are commercial while 25 are non-commercial, spanning 2022, 2023, and 2024 (14, 7, and 10 systems respectively) and varying in size. Most TTM systems generate music by decoding latent representations or discrete tokens into waveforms via autoregressive or non-autoregressive methods, with symbolic generation contributing limited timbre modeling opportunities; a few symbolic samples (ABC/MIDI) are included to enrich diversity. Evaluation uses objective metrics MSE, LCC, SRCC, and KTAU to compare predictions against ground truth. MusicEval comprises 384 prompts (80 manual, 20 from MusicCaps, 284 from system demos); 100 prompts are used with open-access models and 284 with demo-only systems. Prompts address emotion, structure, rhythm, theme, and instrumentation, with lengths showing broad short-to-mid ranges and high variance; focus genres are pop and classical to leverage expert reliability. The dataset spans 16.62 hours of mono audio, 2,748 clips, and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) over 384 prompts, with audio resampled to 16 kHz mono via ffmpeg and each clip scored by five conservatory experts; details of data collection and processing are provided in Sections III–IV. MusicEval is presented as the first expert-scored automatic evaluation dataset for generative music, alongside a CLAP-based baseline to predict both musical impression and textual alignment, using the official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, with a 3-layer MLP head trained by L1 loss on a single NVIDIA 4090 GPU with batch size 64 and SGD learning rate 0.0005. In Table I, utterance-level metrics for musical impression are U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461, and for textual alignment U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314, while system-level metrics show musical impression S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767 and textual alignment S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617 under an 85/15 train/test split. The results indicate strong alignment of utterance-level predictions with human judgments, but greater difficulty predicting alignment, justifying the CLAP-based approach and broader genre coverage for future work. To bolster reliability, two probing methods insert real AudioSet clips into batches to invalidate scores below 3 and interleave identical samples within batches to detect inconsistent raters; score distributions for musical impression and textual alignment are approximately normal with mid-range peaks, and cross-system variation confirms MusicEval’s breadth and the value of CLAP-based scoring. The section advocates a universal evaluation paradigm for TTM that couples automatic assessment with human-perception alignment across two dimensions—overall musical impression and alignment with the text prompt—captured by MusicEval and CLAP-based scoring with CLAP as the upstream feature extractor, a 3-layer MLP head, and L1 loss. MOS data were collected online from 14 raters totaling 13,740 ratings, with samples scored on two 5-point Likert scales by five evaluators each, released in batches with replayable samples; reliability checks include real-music clip insertion and repeat-sample checks, and Figures 1–3 illustrate score distributions and per-system variations, indicating intra-system consistency but notable inter-system differences. The paper details a two-dimension evaluation framework, the first expert-scored generative-music dataset, and a CLAP-based automatic scoring baseline, with future work targeting broader genres and more advanced automatic evaluation architectures.