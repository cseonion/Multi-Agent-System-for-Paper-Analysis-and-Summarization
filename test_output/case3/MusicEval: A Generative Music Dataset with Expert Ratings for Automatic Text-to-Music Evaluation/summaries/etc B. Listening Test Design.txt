We select 14 TTM systems [1]-[3], [18]-[22] and 7 text-to-audio (TTA) systems [23]-[29], some with multiple models differing in size or training data, yielding a total of 31 models that vary across four dimensions, as illustrated in Fig. 4. Twenty-five systems are publicly accessible and used to generate samples from designed prompts, while the remaining six provide only demo audio. In terms of commercialization, six systems are commercial and 25 are non-commercial. Temporally, 14 systems were developed in 2023, 7 in 2024, and 10 in 2022, with models spanning large-scale to smaller architectures, and a balanced mix ensures coverage of diverse characteristics. Most TTM systems generate music by producing latent representations or discrete tokens decoded into waveforms via autoregressive or non-autoregressive methods; symbolic generation exists but remains limited for TTM due to timbre modeling limitations, though a few samples from symbolic systems using ABC notation and MIDI are included to diversify. Evaluation relies on objective indicators MSE, LCC, SRCC, and KTAU. The MusicEval dataset comprises 384 prompts (80 manual, 20 from MusicCaps, 284 from system demos); among these, 100 prompts are used with open-access models and 284 with demo-only systems, with prompts crafted to address emotion, structure, rhythm, theme, and instrumentation; prompt lengths show a broad distribution with high variance, concentrated on short to mid-length prompts, and focus on pop and classical genres to leverage expert reliability. MusicEval totals 16.62 hours of mono audio across 2,748 clips and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) over 384 prompts; audio is resampled to 16 kHz mono via ffmpeg, with each clip scored by five conservatory experts. The dataset and scoring pipeline are described in Sections III–IV. MusicEval is the first expert-scored automatic evaluation dataset for generative music, and a CLAP-based baseline is proposed to predict both musical impression and textual alignment. The baseline uses the official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, with a 3-layer MLP head trained by L1 loss, on a single NVIDIA 4090 GPU with batch size 64 and SGD learning rate 0.0005. Table I reports utterance-level metrics (U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461) and textual alignment (U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314); and system-level metrics (Musical impression: S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767; Textual alignment: S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617), all under the 85/15 split. Results show utterance-level predictions align well with human judgments, while alignment predictions are harder, supporting CLAP-based scoring and motivating broader genre and architecture coverage in future work. To bolster reliability, two probing methods insert real AudioSet clips into batches to invalidate scores below 3 and interleave identical samples within a batch to detect inconsistent raters. Score distributions for musical impression and textual alignment are approximately normal with mid-range peaks, and cross-system variation confirms MusicEval’s breadth and the value of CLAP-based scoring. The section argues for a universal evaluation paradigm for TTM and advocates automatic, perception-aligned assessment across two dimensions—overall musical impression and alignment with the text prompt—captured by MusicEval and CLAP-based scoring, with CLAP as the upstream audio feature extractor, a 3-layer MLP head, and L1 loss; MOS data were collected online from 14 raters totaling 13,740 ratings, with samples scored on two 5-point Likert scales by five evaluators each, released in batches with replayable samples. Reliability checks include real-music clip insertion and repeat-sample checks; Figures 1–3 illustrate score distributions and per-system variations, indicating intra-system consistency but notable inter-system differences.