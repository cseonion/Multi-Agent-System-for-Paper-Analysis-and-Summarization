To ensure broad coverage, the study includes 14 TTM systems and 7 text-to-audio (TTA) systems, totaling 31 models from 21 systems and spanning four dimensions—accessibility, commercialization, year, and model size (Figure 4). Twenty-five systems are publicly accessible for sample generation, while the remaining six provide demo audio only; six are commercial and 25 non-commercial, with temporal development distributed as 14 systems in 2023, 7 in 2024, and 10 in 2022, and models vary in size to capture a wide range of capabilities. Most TTM systems generate music from latent representations or discrete tokens decoded into waveform via autoregressive or non-autoregressive methods; symbolic generation exists but is limited in TTM due to timbre modeling, though a few samples based on ABC notation and MIDI [20] are included to diversify. Performance is evaluated with MSE, LCC, SRCC, and KTAU. MusicEval comprises 384 prompts (80 manual, 20 from MusicCaps [1], 284 from system demo pages); of these, 100 prompts generate music with open-access models and 284 with demo-only systems. Prompt-length distributions are shown in Figure 5, revealing a broad range with a concentration on short to mid-length prompts, and prompts focus on pop and classical genres to leverage evaluators’ expertise. The MusicEval dataset contains 2,748 mono clips totaling 16.62 hours and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) in response to 384 prompts, with all audio resampled to 16 kHz mono via ffmpeg; each clip is rated by five experts, and the data are split 85/15 into train/test. The study presents MusicEval as the first expert-scored dataset for generative music evaluation and a CLAP-based baseline to predict both musical impression and textual alignment, using a pre-trained CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k and a 3-layer MLP head trained with L1 loss, with experiments run on a single NVIDIA 4090 GPU at batch size 64. Table I reports utterance-level and system-level metrics showing that utterance-level predictions align well with human judgments, while alignment predictions are harder; the CLAP-based model achieves MSE, LCC, SRCC, and KTAU values as detailed in the table. Reliability checks include inserting real AudioSet clips in batches (scores below 3 invalidate the rater) and placing identical sample pairs within a batch (large score differences invalidate the rater). Distributions of both musical impression and textual alignment scores approximate normality with mid-range peaks and system-dependent variation (Figures 1–3), underscoring that MusicEval captures cross-system diversity and supports validating CLAP-based automatic scoring for TTM evaluation, with future work aimed at broader genres and more advanced automatic evaluation architectures.