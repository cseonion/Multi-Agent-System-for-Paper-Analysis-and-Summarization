To ensure comprehensive coverage, we analyze 31 models comprising 14 TTM systems [1]-[3], [18]-[22] and 7 text-to-audio (TTA) systems [23]-[29], with some models differing in size or training data; their diversity is summarized in Fig. 4 as four dimensions—accessibility, commercialization, year, and model size. Twenty-five systems are publicly accessible to generate samples, while the remaining six provide only demo audio; commercially, six are commercial and 25 non-commercial; temporally, 14 were developed in 2023, 7 in 2024, and 10 in 2022, with models spanning a range of sizes to balance large-scale and smaller architectures. Most TTM systems generate music from latent representations or discrete tokens decoded into waveform via autoregressive or non-autoregressive methods, while symbolic generation exists but is limited in TTM by timbre modeling, with a few samples from symbolic systems based on ABC notation and MIDI included for diversity [20]. Performance is evaluated with mean square error (MSE), linear correlation coefficient (LCC), Spearman rank correlation (SRCC), and Kendall tau rank correlation (KTAU). The MusicEval dataset comprises 384 prompts (80 manual, 20 from MusicCaps, 284 from system demo pages); 100 prompts generate music with open-access models and 284 with demo-only systems. Prompt lengths (Fig. 5) cover a broad range, concentrated on short to mid-length prompts, with two genres emphasized—pop and classical—to leverage evaluators’ expertise. MusicEval contains 2,748 mono audio clips totaling 16.62 hours and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) in response to 384 prompts; all audio is resampled to 16 kHz mono via ffmpeg; each clip is rated by five experts and the data are split 85/15 into train/test. MusicEval is introduced as the first expert-scored dataset for generative-music evaluation, accompanied by a CLAP-based baseline to predict both musical impression and textual alignment, using a pre-trained CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, plus a 3-layer MLP head trained with L1 loss, with experiments run on a single NVIDIA 4090 GPU at batch size 64. Table I reports utterance-level and system-level metrics, with utterance-level musical impression MSE 0.647, LCC 0.606, SRCC 0.633, KTAU 0.461 and textual alignment MSE 0.616, LCC 0.438, SRCC 0.443, KTAU 0.314; system-level musical impression MSE 0.446, LCC 0.839, SRCC 0.926, KTAU 0.767 and textual alignment MSE 0.354, LCC 0.757, SRCC 0.784, KTAU 0.617, all under an 85/15 split. The official pretrained CLAP checkpoint is used, and fine-tuning employs a batch size of 64 with SGD at learning rate 0.0005. Results show that utterance-level predictions align well with human judgments, while alignment predictions are harder, supporting the feasibility of CLAP-based automatic scoring for TTM evaluation; future work seeks broader genres and more advanced automatic evaluation architectures. The section also notes reliability checks—inserting real AudioSet clips into batches to invalidate scores below 3 and placing identical sample pairs to detect inconsistent raters—and reports that score distributions for musical impression and textual alignment approximate normality with mid-range peaks, while cross-system variation confirms MusicEval’s broad coverage and the value of CLAP-based automatic scoring. The broader context highlights the lack of a universal evaluation paradigm for TTM and motivates automatic, perception-aligned assessment through two dimensions—overall musical impression and alignment with the text prompt—embodied in MusicEval and CLAP-based scoring.