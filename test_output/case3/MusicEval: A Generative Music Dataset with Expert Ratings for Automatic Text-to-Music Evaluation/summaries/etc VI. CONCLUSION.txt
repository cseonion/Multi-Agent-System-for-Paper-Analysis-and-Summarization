This concluding section evaluates 31 models drawn from 14 TTM systems [1]-[3], [18]-[22] and 7 TTA systems [23]-[29], where some have multiple variants, yielding 31 models in total. The systems vary across several dimensions, summarized by a four-dimension pie chart in Figure 4: accessibility, commercialization, year, and model size. Specifically, 25 systems are publicly accessible while 6 are not and are assessed via their demo audio; 6 systems are commercial and 25 non-commercial; development spans 2022 (10), 2023 (14), and 2024 (7); model sizes are diversified. Most TTM systems generate music by decoding latent representations or discrete tokens into waveforms via autoregressive or non-autoregressive methods, with a few symbolic systems (ABC notation and MIDI) included to broaden coverage due to timbre limitations. Objective evaluation uses mean square error (MSE), linear correlation coefficient (LCC), Spearman rank correlation (SRCC), and Kendall tau (KTAU) between predictions and ground truths. The MusicEval dataset comprises 384 prompts (80 manual, 20 MusicCaps, 284 from demos) produced by 21 systems (31 models); 100 prompts are used with open-access models, while 284 come from demo-only systems. Prompt lengths show broad distribution with high variance, focusing on pop and classical genres. MusicEval totals 16.62 hours of mono audio, 2,748 clips, and 13,740 ratings from 14 music experts; all samples are resampled to 16 kHz mono with ffmpeg and scored by five conservatory experts; details are in Sections III–IV. A CLAP-based baseline predicts both musical impression and textual alignment, using an official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, a 3-layer MLP head, and L1 loss, trained on a single NVIDIA 4090 with batch size 64 and SGD LR 0.0005. Table I reports utterance-level metrics (Musical impression: U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461; Textual alignment: U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314) and system-level metrics (Musical impression: S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767; Textual alignment: S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617) under an 85/15 train/test split, with the CLAP checkpoint pretrained on music, AudioSet, and LAION-Audio-630k. The results indicate strong utterance-level alignment but weaker system-level alignment; two probing methods—inserting real AudioSet clips and interleaving identical samples—yield approximately normal distributions with mid-range peaks, supporting cross-system breadth and CLAP scoring validity. The authors advocate a universal evaluation paradigm that couples automatic assessment with human-perception alignment across two dimensions—overall musical impression and alignment with the text prompt—via MusicEval and CLAP-based scoring, augmented by online MOS data (13,740 ratings from 14 raters) and batch-replay reliability checks, and outline future work toward broader genres and more advanced evaluation architectures. The paper presents three contributions: defining automatic evaluation for TTM, introducing MusicEval with 31 models across 384 prompts and expert MOS, and developing a CLAP-based dual-prediction model, demonstrating feasibility and establishing a strong baseline. MOS tests were conducted online by 14 professionals who, after training, listened to prompt-based samples and rated them on two 5-point Likert scales; each sample was judged by five evaluators and averaged, with reliability reinforced by inserting real AudioSet clips and placing identical samples within batches to flag inconsistent raters; results are illustrated by score distributions indicating broadly normal patterns and system-level differences. The CLAP-based model uses a pre-trained CLAP upstream feature extractor and a 3-layer MLP to predict musical impression and textual alignment, trained with L1 loss; training used batch size 64 on a single NVIDIA 4090. Finally, the MusicEval dataset is randomly split into an 85% train and 15% test set.