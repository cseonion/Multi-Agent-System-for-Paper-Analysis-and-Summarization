This section documents the quality of ratings used to evaluate generative music, focusing on 31 models drawn from 14 TTM systems [1]-[3], [18]-[22] and 7 TTA systems [23]-[29], some with multiple variants, spanning four characteristic dimensions shown in Fig. 4: accessibility, commercialization, year, and model size. Twenty-five systems are publicly accessible and used to generate samples from designed prompts, while six provide only demo audio. Commercially, six are commercial and 25 non-commercial, offering a balanced mix of industry and academia. Temporal coverage includes 14 systems developed in 2023, 7 in 2024, and 10 in 2022, with models ranging from large-scale to smaller architectures. Most TTM systems generate music by turning latent representations or tokens into waveforms via autoregressive or non-autoregressive decoding; symbolic generation exists but is limited for timbre modeling, though a few samples from ABC/MIDI-based symbolic systems are included to diversify. The evaluation uses objective metrics MSE, LCC, SRCC, and KTAU to compare predictions against ground truth.

MusicEval, the generative music evaluation dataset introduced here, comprises 384 prompts (80 manual, 20 from MusicCaps, 284 from system demos); 100 prompts are used with open-access models, and 284 with demo-only systems. Prompts address emotion, structure, rhythm, theme, and instrumentation, with lengths broadly distributed and high variance, focusing on pop and classical genres to leverage expert reliability. The dataset contains 16.62 hours of mono audio, 2,748 clips, and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) over 384 prompts; audio is resampled to 16 kHz mono via ffmpeg, with each clip scored by five conservatory experts. Details of data collection and processing are given in Sections III–IV. MusicEval is presented as the first expert-scored automatic evaluation dataset for generative music, alongside a CLAP-based baseline to predict both musical impression and textual alignment. The baseline uses the official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, with a 3-layer MLP head trained by L1 loss on a single NVIDIA 4090 GPU with batch size 64 and SGD learning rate 0.0005.

Table I reports utterance-level metrics (U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461) for musical impression and (U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314) for textual alignment, and system-level metrics (Musical impression: S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767; Textual alignment: S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617) under the 85/15 split. The results show strong alignment of utterance-level predictions with human judgments, while alignment predictions are more challenging, supporting the CLAP-based scoring approach and motivating broader genre and architecture coverage in future work. To enhance reliability, two probing methods insert real AudioSet clips into batches to invalidate scores below 3 and interleave identical samples within batches to detect inconsistent raters. The distributions of musical-impression and textual-alignment scores are approximately normal with mid-range peaks, and cross-system variation confirms MusicEval’s breadth and the value of CLAP-based scoring. The section advocates a universal evaluation paradigm for TTM, combining automatic assessment with human-perception alignment across two dimensions—overall musical impression and alignment with the text prompt—captured by MusicEval and CLAP-based scoring, with CLAP as the upstream feature extractor, a 3-layer MLP head, and L1 loss. MOS data were collected online from 14 raters totaling 13,740 ratings, with samples scored on two 5-point Likert scales by five evaluators each, released in batches with replayable samples, and reliability checks include real-music clip insertion and repeat-sample checks; Figures 1–3 illustrate score distributions and per-system variations, indicating intra-system consistency but notable inter-system differences.