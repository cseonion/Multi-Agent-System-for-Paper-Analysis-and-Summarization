This section catalogs the evaluation setup for mean opinion scores by describing 31 models in total: 14 TTM systems [1]-[3], [18]-[22] and 7 TTA systems [23]-[29], some with multiple variants, thereby yielding 31 models. These systems vary along four dimensions shown in Fig. 4: accessibility, commercialization, year, and model size. Twenty-five systems are publicly accessible for sample generation, while six provide only demo audio; commercially, six are commercial and 25 non-commercial; temporally, 14 were developed in 2023, 7 in 2024, and 10 in 2022; models span large-scale and smaller architectures. Most TTM systems generate music by latent representations or discrete tokens decoded into waveform via autoregressive or non-autoregressive methods; symbolic generation exists but is limited in TTM due to timbre modeling, though a few samples from symbolic systems based on ABC notation and MIDI are included to diversify. Objective metrics used are MSE, LCC, SRCC, and KTAU. MusicEval comprises 384 prompts (80 manual, 20 from MusicCaps, 284 from system demo pages); among these, 100 prompts generate music with open-access models and 284 with demo-only systems. Prompt lengths show a broad short-to-mid-length range with high variance, and prompts focus on pop and classical genres to leverage evaluators’ expertise. The dataset spans 16.62 hours, with 2,748 mono audio clips and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) across 384 prompts; all audio are resampled to 16 kHz mono via ffmpeg, and each clip is rated by five experts with an 85/15 train/test split. MusicEval is presented as the first expert-scored generative-music evaluation dataset, accompanied by a CLAP-based baseline predicting both musical impression and textual alignment, using the official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, plus a 3-layer MLP head trained with L1 loss, with experiments on a single NVIDIA 4090 GPU at batch size 64. Table I reports utterance-level and system-level metrics: musical impression U–MSE 0.647, U–LCC 0.606, U–SRCC 0.633, U–KTAU 0.461; textual alignment U–MSE 0.616, U–LCC 0.438, U–SRCC 0.443, U–KTAU 0.314; system-level musical impression S–MSE 0.446, S–LCC 0.839, S–SRCC 0.926, S–KTAU 0.767; textual alignment S–MSE 0.354, S–LCC 0.757, S–SRCC 0.784, S–KTAU 0.617, all under the 85/15 split. The CLAP checkpoint is the official pretrained one; finetuning uses SGD with learning rate 0.0005 and batch size 64 on an NVIDIA 4090. Results show utterance-level predictions align well with human judgments, while alignment predictions are harder, supporting CLAP-based automatic scoring; future work will broaden genres and architectures. To enhance reliability, two probing methods are used: inserting real AudioSet clips into batches to invalidate scores below 3 and interleaving identical samples within a batch to detect inconsistent raters. Score distributions for musical impression and textual alignment are approximately normal with mid-range peaks; cross-system variation confirms MusicEval’s breadth and the value of CLAP-based scoring. The section frames the lack of a universal evaluation paradigm for TTM and motivates automatic, perception-aligned assessment via the two dimensions—overall musical impression and alignment with the text prompt—captured by MusicEval and CLAP-based scoring. It also details the methodological setup: CLAP as the upstream audio feature extractor, a 3-layer MLP as the downstream head, and L1 loss; MOS data were collected online from 14 raters (2 professionals and 12 conservatory students) totaling 13,740 ratings, with samples scored on two 5-point Likert dimensions by five evaluators each; data were released in batches with replayable samples. Reliability checks include real-music clip insertion and repeat-sample checks, and Figures 1–3 illustrate score distributions—both dimensions show roughly normal shapes with mid-range peaks—and Figures 2 and 3 reveal per-system score distributions, indicating intra-system consistency but notable inter-system variation.