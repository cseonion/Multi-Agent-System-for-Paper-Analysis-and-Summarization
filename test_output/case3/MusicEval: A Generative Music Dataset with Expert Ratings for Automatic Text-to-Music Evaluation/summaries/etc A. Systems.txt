To achieve broad coverage, the study evaluates 14 TTM systems [1]- [3], [18]- [22] and 7 TTA systems [23]- [29], totaling 31 models that vary across four dimensions shown in Fig. 4 (accessibility, commercialization, year, and model size), with 25 systems publicly accessible for sample generation and 6 providing only demo audio; six systems are commercial and 25 non-commercial, spanning development years 2022, 2023 and 2024 (14 in 2023, 7 in 2024, 10 in 2022) and a range of model sizes. Most TTM systems generate music from latent representations or discrete tokens decoded into waveform via autoregressive or non-autoregressive methods, while symbolic generation exists but is limited in TTM due to timbre modeling; a few samples from symbolic systems based on ABC notation and MIDI are included for diversification. Performance is evaluated with mean square error (MSE), linear correlation coefficient (LCC), Spearman rank correlation (SRCC), and Kendall tau rank correlation (KTAU). MusicEval comprises 384 prompts (80 manual, 20 from MusicCaps, 284 from system demo pages); 100 prompts generate music with open-access models and 284 with demo-only systems. Prompt-length distributions (Fig. 5) show a broad range, concentrated on short to mid-length prompts, and prompts focus on pop and classical genres to leverage evaluators’ expertise. The MusicEval dataset contains 2,748 mono audio clips totaling 16.62 hours and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) in response to 384 prompts, with all audio resampled to 16 kHz mono via ffmpeg; each clip is rated by five experts, and the data are split 85/15 into train/test. MusicEval is presented as the first expert-scored dataset for generative music evaluation, accompanied by a CLAP-based baseline to predict both musical impression and textual alignment, using a pre-trained CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k and a 3-layer MLP head trained with L1 loss, with experiments run on a single NVIDIA 4090 GPU at batch size 64. Table I reports utterance-level and system-level metrics, showing that utterance-level predictions align well with human judgments while alignment predictions are harder; the CLAP-based model achieves the listed MSE, LCC, SRCC, and KTAU values. Reliability checks include inserting real AudioSet clips in batches to invalidate raters scoring real clips below 3 and placing identical sample pairs within a batch to invalidate raters whose scores differ excessively. The score distributions for musical impression and textual alignment approximate normality with mid-range peaks and system-dependent variation, illustrating MusicEval’s cross-system coverage and supporting CLAP-based automatic scoring for TTM evaluation, with future work aimed at broader genres and more advanced automatic evaluation architectures. The CLAP-based baseline achieves utterance-level MSE 0.647, LCC 0.606, SRCC 0.633, KTAU 0.461 for musical impression, and MSE 0.616, LCC 0.438, SRCC 0.443, KTAU 0.314 for textual alignment, while system-level results show MSE 0.446, LCC 0.839, SRCC 0.926, KTAU 0.767 for musical impression and MSE 0.354, LCC 0.757, SRCC 0.784, KTAU 0.617 for textual alignment.