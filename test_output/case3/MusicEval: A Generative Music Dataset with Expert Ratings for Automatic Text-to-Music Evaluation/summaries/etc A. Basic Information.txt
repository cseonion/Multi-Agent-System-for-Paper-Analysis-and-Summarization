This section outlines the composition and evaluation framework behind MusicEval. The study covers 14 TTM systems [1]-[3], [18]-[22] and 7 TTA systems [23]-[29], with some systems offering multiple models of different sizes or training data, yielding 31 models in total from 21 systems, analyzed across four dimensions—accessibility, commercialization, year, and model size (Figure 4). Of these, 25 systems are publicly accessible (used to generate samples), while 6 are not and provide demo data; 6 are commercial [30]-[35], and 25 are non-commercial, with 14 systems developed in 2023, 7 in 2024, and 10 in 2022. Most systems generate music from latent representations or discrete tokens decoded into waveforms (autoregressive or non-autoregressive); symbolic generation is included but limited in TTM due to timbre modeling, though a few samples based on ABC notation and MIDI [20] are used to diversify. Performance is evaluated with MSE, LCC, SRCC, and KTAU. MusicEval comprises 384 prompts (80 manual, 20 from MusicCaps [1], and 284 from system demo pages); of these, 100 prompts generate music with open-access models and 284 with demo-only systems. All audio is resampled to 16 kHz mono via ffmpeg. The dataset contains 2,748 mono clips totaling 16.62 hours and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) responding to 384 prompts, with five experts rating each clip; the data are split 85/15 into train/test. A baseline CLAP-based model (3-layer MLP head) is trained on the official pretrained CLAP checkpoint foot_1 (trained on music, AudioSet, LAION-Audio-630k) with batch size 64 on a single NVIDIA 4090 GPU and SGD LR 0.0005. Table I reports utterance-level metrics (Musical impression: MSE 0.647, LCC 0.606, SRCC 0.633, KTAU 0.461; Textual alignment: MSE 0.616, LCC 0.438, SRCC 0.443, KTAU 0.314) and system-level metrics (Musical impression: MSE 0.446, LCC 0.839, SRCC 0.926, KTAU 0.767; Textual alignment: MSE 0.354, LCC 0.757, SRCC 0.784, KTAU 0.617). The results suggest utterance-level predictions align well with human judgments, while alignment predictions are harder. Reliability checks include inserting real AudioSet clips (scores below 3 invalidate the rater) and placing identical sample pairs within a batch (large score differences invalidate the rater). Figures 1–3 depict score distributions, which approximate normality with mid-range peaks, and show system-to-system variability. The section concludes with three contributions: proposing automatic evaluation for TTM, releasing MusicEval, and presenting a CLAP-based baseline for predicting musical impression and textual alignment.