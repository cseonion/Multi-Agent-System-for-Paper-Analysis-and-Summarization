Section D reports a comprehensive evaluation across 31 models drawn from 14 TTM and 7 TTA systems. Among these, 25 systems are publicly accessible for music sample generation, while 6 are not publicly available and are evaluated via their demo audio; 6 systems are commercial and 25 non-commercial, with development distributed as 14 in 2023, 7 in 2024, and 10 in 2022, and a mix of model sizes to ensure diversity. Most systems generate music by decoding latent representations or discrete tokens into waveforms through autoregressive or non-autoregressive methods, with only a few symbolic systems (ABC notation and MIDI) included to broaden coverage due to timbre limitations. Evaluation uses objective metrics MSE, LCC, SRCC, and KTAU between predictions and ground-truths. The MusicEval dataset comprises 384 prompts (80 manual, 20 MusicCaps, 284 from demos) produced by 21 systems (31 models); 100 prompts are used with open-access models, while 284 come from demo-only systems. Prompts address emotion, structure, rhythm, theme, and instrumentation; prompt lengths show broad distribution with high variance and a focus on pop and classical genres. MusicEval totals 16.62 hours of mono audio, 2,748 clips, and 13,740 ratings from 14 music experts, with all samples resampled to 16 kHz mono via ffmpeg and scored by five conservatory experts; details are in Sections III–IV. A CLAP-based baseline predicts both musical impression and textual alignment, using an official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, a 3-layer MLP head, and L1 loss, trained on a single NVIDIA 4090 with batch size 64 and SGD LR 0.0005. Table I reports utterance-level metrics (Musical impression: U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461; Textual alignment: U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314) and system-level metrics (Musical impression: S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767; Textual alignment: S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617) under an 85/15 train/test split, with the CLAP checkpoint pretrained on music, AudioSet, and LAION-Audio-630k. The results indicate strong utterance-level alignment but weaker system-level alignment; two probing methods—inserting real AudioSet clips and interleaving identical samples—yield approximately normal distributions with mid-range peaks, supporting cross-system breadth and CLAP scoring validity. The authors advocate a universal evaluation paradigm that couples automatic assessment with human-perception alignment across two dimensions—overall musical impression and alignment with the text prompt—via MusicEval and CLAP-based scoring, augmented by online MOS data (13,740 ratings from 14 raters) and batch-replay reliability checks, and outline future work toward broader genres and more advanced evaluation architectures. The paper presents three contributions: defining automatic evaluation for TTM, introducing MusicEval with 31 models across 384 prompts and expert MOS, and developing a CLAP-based dual-prediction model, demonstrating feasibility and establishing a strong baseline. MOS testing was conducted online by 14 professionals who, after a training session, listened to prompt-based samples and rated them on two 5-point Likert scales; each sample was judged by five evaluators and averaged, with reliability reinforced by inserting real AudioSet clips and placing identical samples within batches to flag inconsistent raters, and results are illustrated by score distributions showing broadly normal patterns and system-level differences.