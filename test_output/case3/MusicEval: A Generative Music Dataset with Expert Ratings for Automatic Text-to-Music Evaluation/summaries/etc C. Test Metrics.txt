To ensure broad coverage of generative systems, Section C includes 14 TTM systems [1]-[3], [18]-[22] and 7 text-to-audio (TTA) systems [23]-[29], totaling 31 models. Figure 4 presents a four-dimension breakdown across accessibility, commercialization, year, and model size; 25 systems are publicly accessible for sample generation, while the remaining 6 not publicly available are represented via their demo audio. Commercialization comprises 6 commercial and 25 non-commercial systems, with temporal distribution of 14 systems from 2023, 7 from 2024, and 10 from 2022, alongside varying model sizes to capture diversity. Most TTM systems generate music by decoding latent representations or discrete tokens into waveforms via autoregressive or non-autoregressive methods; symbolic generation (ABC notation and MIDI) also contributes, though timbre modeling is limited, with a few symbolic samples included from system [20]. Performance is assessed with objective metrics MSE, LCC, SRCC, and KTAU between predictions and ground-truths.

The MusicEval dataset comprises 384 prompts—80 manual, 20 MusicCaps, and 284 from system demos—producing samples by 21 systems (31 models); 100 prompts are used with open-access models, and 284 with demo-only systems. Manual prompts address emotion, structure, rhythm, theme, and instrumentation; prompt-length distributions are broad with high variance, focusing on pop and classical genres for reliability (Figure 5). The dataset totals 16.62 hours of mono audio, 2,748 clips, and 13,740 ratings from 14 music experts, with all samples resampled to 16 kHz mono via ffmpeg and scored by five conservatory experts; details are in Sections III–IV. A CLAP-based baseline predicts both musical impression and textual alignment, using an official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, a 3-layer MLP head, and L1 loss, trained on a single NVIDIA 4090 GPU with batch size 64 and SGD LR 0.0005. Table I reports utterance-level metrics (U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461 for musical impression; U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314 for textual alignment) and system-level metrics (S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767 for musical impression; S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617 for textual alignment) under an 85/15 train/test split. The CLAP baseline uses an official pretrained checkpoint trained on music, AudioSet, and LAION-Audio-630k, with results indicating strong utterance-level alignment but more difficulty predicting alignment at the system level. Two probing methods insert real AudioSet clips and interleave identical samples to detect inconsistent raters, yielding approximately normal score distributions with mid-range peaks, validating cross-system breadth and CLAP scoring. The section advocates a universal evaluation paradigm that couples automatic assessment with human-perception alignment across two dimensions—overall musical impression and alignment with the text prompt—captured by MusicEval and CLAP-based scoring, with reliability checks including online MOS data from 14 raters (13,740 ratings) and batch-replay capabilities, and highlights future work toward broader genres and more advanced evaluation architectures.