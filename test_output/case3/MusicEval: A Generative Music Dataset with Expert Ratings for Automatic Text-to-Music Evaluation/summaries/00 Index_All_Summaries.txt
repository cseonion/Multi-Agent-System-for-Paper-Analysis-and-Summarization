논문 제목: MusicEval: A Generative Music Dataset with Expert Ratings for Automatic Text-to-Music Evaluation
전체 섹션별 요약 인덱스
============================================================

섹션 etc: I. INTRODUCTION
----------------------------------------
Please provide the raw text of the first section (and, if this is not the first section, the summary of the previous sections). For each subsequent step, share the raw text of that section along with the prior section summaries. I will produce a single-paragraph, concise summary per section that preserves all numerical details. If you have any length targets or emphasis preferences (e.g., highlight methods, results, or key numbers), please specify.

섹션 etc: A. Basic Information
----------------------------------------
This section outlines the composition and evaluation framework behind MusicEval. The study covers 14 TTM systems [1]-[3], [18]-[22] and 7 TTA systems [23]-[29], with some systems offering multiple models of different sizes or training data, yielding 31 models in total from 21 systems, analyzed across four dimensions—accessibility, commercialization, year, and model size (Figure 4). Of these, 25 systems are publicly accessible (used to generate samples), while 6 are not and provide demo data; 6 are commercial [30]-[35], and 25 are non-commercial, with 14 systems developed in 2023, 7 in 2024, and 10 in 2022. Most systems generate music from latent representations or discrete tokens decoded into waveforms (autoregressive or non-autoregressive); symbolic generation is included but limited in TTM due to timbre modeling, though a few samples based on ABC notation and MIDI [20] are used to diversify. Performance is evaluated with MSE, LCC, SRCC, and KTAU. MusicEval comprises 384 prompts (80 manual, 20 from MusicCaps [1], and 284 from system demo pages); of these, 100 prompts generate music with open-access models and 284 with demo-only systems. All audio is resampled to 16 kHz mono via ffmpeg. The dataset contains 2,748 mono clips totaling 16.62 hours and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) responding to 384 prompts, with five experts rating each clip; the data are split 85/15 into train/test. A baseline CLAP-based model (3-layer MLP head) is trained on the official pretrained CLAP checkpoint foot_1 (trained on music, AudioSet, LAION-Audio-630k) with batch size 64 on a single NVIDIA 4090 GPU and SGD LR 0.0005. Table I reports utterance-level metrics (Musical impression: MSE 0.647, LCC 0.606, SRCC 0.633, KTAU 0.461; Textual alignment: MSE 0.616, LCC 0.438, SRCC 0.443, KTAU 0.314) and system-level metrics (Musical impression: MSE 0.446, LCC 0.839, SRCC 0.926, KTAU 0.767; Textual alignment: MSE 0.354, LCC 0.757, SRCC 0.784, KTAU 0.617). The results suggest utterance-level predictions align well with human judgments, while alignment predictions are harder. Reliability checks include inserting real AudioSet clips (scores below 3 invalidate the rater) and placing identical sample pairs within a batch (large score differences invalidate the rater). Figures 1–3 depict score distributions, which approximate normality with mid-range peaks, and show system-to-system variability. The section concludes with three contributions: proposing automatic evaluation for TTM, releasing MusicEval, and presenting a CLAP-based baseline for predicting musical impression and textual alignment.

섹션 etc: B. Data Distribution
----------------------------------------
To ensure broad coverage, the study includes 14 TTM systems and 7 text-to-audio (TTA) systems, totaling 31 models from 21 systems and spanning four dimensions—accessibility, commercialization, year, and model size (Figure 4). Twenty-five systems are publicly accessible for sample generation, while the remaining six provide demo audio only; six are commercial and 25 non-commercial, with temporal development distributed as 14 systems in 2023, 7 in 2024, and 10 in 2022, and models vary in size to capture a wide range of capabilities. Most TTM systems generate music from latent representations or discrete tokens decoded into waveform via autoregressive or non-autoregressive methods; symbolic generation exists but is limited in TTM due to timbre modeling, though a few samples based on ABC notation and MIDI [20] are included to diversify. Performance is evaluated with MSE, LCC, SRCC, and KTAU. MusicEval comprises 384 prompts (80 manual, 20 from MusicCaps [1], 284 from system demo pages); of these, 100 prompts generate music with open-access models and 284 with demo-only systems. Prompt-length distributions are shown in Figure 5, revealing a broad range with a concentration on short to mid-length prompts, and prompts focus on pop and classical genres to leverage evaluators’ expertise. The MusicEval dataset contains 2,748 mono clips totaling 16.62 hours and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) in response to 384 prompts, with all audio resampled to 16 kHz mono via ffmpeg; each clip is rated by five experts, and the data are split 85/15 into train/test. The study presents MusicEval as the first expert-scored dataset for generative music evaluation and a CLAP-based baseline to predict both musical impression and textual alignment, using a pre-trained CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k and a 3-layer MLP head trained with L1 loss, with experiments run on a single NVIDIA 4090 GPU at batch size 64. Table I reports utterance-level and system-level metrics showing that utterance-level predictions align well with human judgments, while alignment predictions are harder; the CLAP-based model achieves MSE, LCC, SRCC, and KTAU values as detailed in the table. Reliability checks include inserting real AudioSet clips in batches (scores below 3 invalidate the rater) and placing identical sample pairs within a batch (large score differences invalidate the rater). Distributions of both musical impression and textual alignment scores approximate normality with mid-range peaks and system-dependent variation (Figures 1–3), underscoring that MusicEval captures cross-system diversity and supports validating CLAP-based automatic scoring for TTM evaluation, with future work aimed at broader genres and more advanced automatic evaluation architectures.

섹션 etc: A. Systems
----------------------------------------
To achieve broad coverage, the study evaluates 14 TTM systems [1]- [3], [18]- [22] and 7 TTA systems [23]- [29], totaling 31 models that vary across four dimensions shown in Fig. 4 (accessibility, commercialization, year, and model size), with 25 systems publicly accessible for sample generation and 6 providing only demo audio; six systems are commercial and 25 non-commercial, spanning development years 2022, 2023 and 2024 (14 in 2023, 7 in 2024, 10 in 2022) and a range of model sizes. Most TTM systems generate music from latent representations or discrete tokens decoded into waveform via autoregressive or non-autoregressive methods, while symbolic generation exists but is limited in TTM due to timbre modeling; a few samples from symbolic systems based on ABC notation and MIDI are included for diversification. Performance is evaluated with mean square error (MSE), linear correlation coefficient (LCC), Spearman rank correlation (SRCC), and Kendall tau rank correlation (KTAU). MusicEval comprises 384 prompts (80 manual, 20 from MusicCaps, 284 from system demo pages); 100 prompts generate music with open-access models and 284 with demo-only systems. Prompt-length distributions (Fig. 5) show a broad range, concentrated on short to mid-length prompts, and prompts focus on pop and classical genres to leverage evaluators’ expertise. The MusicEval dataset contains 2,748 mono audio clips totaling 16.62 hours and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) in response to 384 prompts, with all audio resampled to 16 kHz mono via ffmpeg; each clip is rated by five experts, and the data are split 85/15 into train/test. MusicEval is presented as the first expert-scored dataset for generative music evaluation, accompanied by a CLAP-based baseline to predict both musical impression and textual alignment, using a pre-trained CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k and a 3-layer MLP head trained with L1 loss, with experiments run on a single NVIDIA 4090 GPU at batch size 64. Table I reports utterance-level and system-level metrics, showing that utterance-level predictions align well with human judgments while alignment predictions are harder; the CLAP-based model achieves the listed MSE, LCC, SRCC, and KTAU values. Reliability checks include inserting real AudioSet clips in batches to invalidate raters scoring real clips below 3 and placing identical sample pairs within a batch to invalidate raters whose scores differ excessively. The score distributions for musical impression and textual alignment approximate normality with mid-range peaks and system-dependent variation, illustrating MusicEval’s cross-system coverage and supporting CLAP-based automatic scoring for TTM evaluation, with future work aimed at broader genres and more advanced automatic evaluation architectures. The CLAP-based baseline achieves utterance-level MSE 0.647, LCC 0.606, SRCC 0.633, KTAU 0.461 for musical impression, and MSE 0.616, LCC 0.438, SRCC 0.443, KTAU 0.314 for textual alignment, while system-level results show MSE 0.446, LCC 0.839, SRCC 0.926, KTAU 0.767 for musical impression and MSE 0.354, LCC 0.757, SRCC 0.784, KTAU 0.617 for textual alignment.

섹션 etc: B. Text Prompts
----------------------------------------
To ensure comprehensive coverage, we analyze 31 models comprising 14 TTM systems [1]-[3], [18]-[22] and 7 text-to-audio (TTA) systems [23]-[29], with some models differing in size or training data; their diversity is summarized in Fig. 4 as four dimensions—accessibility, commercialization, year, and model size. Twenty-five systems are publicly accessible to generate samples, while the remaining six provide only demo audio; commercially, six are commercial and 25 non-commercial; temporally, 14 were developed in 2023, 7 in 2024, and 10 in 2022, with models spanning a range of sizes to balance large-scale and smaller architectures. Most TTM systems generate music from latent representations or discrete tokens decoded into waveform via autoregressive or non-autoregressive methods, while symbolic generation exists but is limited in TTM by timbre modeling, with a few samples from symbolic systems based on ABC notation and MIDI included for diversity [20]. Performance is evaluated with mean square error (MSE), linear correlation coefficient (LCC), Spearman rank correlation (SRCC), and Kendall tau rank correlation (KTAU). The MusicEval dataset comprises 384 prompts (80 manual, 20 from MusicCaps, 284 from system demo pages); 100 prompts generate music with open-access models and 284 with demo-only systems. Prompt lengths (Fig. 5) cover a broad range, concentrated on short to mid-length prompts, with two genres emphasized—pop and classical—to leverage evaluators’ expertise. MusicEval contains 2,748 mono audio clips totaling 16.62 hours and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) in response to 384 prompts; all audio is resampled to 16 kHz mono via ffmpeg; each clip is rated by five experts and the data are split 85/15 into train/test. MusicEval is introduced as the first expert-scored dataset for generative-music evaluation, accompanied by a CLAP-based baseline to predict both musical impression and textual alignment, using a pre-trained CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, plus a 3-layer MLP head trained with L1 loss, with experiments run on a single NVIDIA 4090 GPU at batch size 64. Table I reports utterance-level and system-level metrics, with utterance-level musical impression MSE 0.647, LCC 0.606, SRCC 0.633, KTAU 0.461 and textual alignment MSE 0.616, LCC 0.438, SRCC 0.443, KTAU 0.314; system-level musical impression MSE 0.446, LCC 0.839, SRCC 0.926, KTAU 0.767 and textual alignment MSE 0.354, LCC 0.757, SRCC 0.784, KTAU 0.617, all under an 85/15 split. The official pretrained CLAP checkpoint is used, and fine-tuning employs a batch size of 64 with SGD at learning rate 0.0005. Results show that utterance-level predictions align well with human judgments, while alignment predictions are harder, supporting the feasibility of CLAP-based automatic scoring for TTM evaluation; future work seeks broader genres and more advanced automatic evaluation architectures. The section also notes reliability checks—inserting real AudioSet clips into batches to invalidate scores below 3 and placing identical sample pairs to detect inconsistent raters—and reports that score distributions for musical impression and textual alignment approximate normality with mid-range peaks, while cross-system variation confirms MusicEval’s broad coverage and the value of CLAP-based automatic scoring. The broader context highlights the lack of a universal evaluation paradigm for TTM and motivates automatic, perception-aligned assessment through two dimensions—overall musical impression and alignment with the text prompt—embodied in MusicEval and CLAP-based scoring.

섹션 etc: IV. MEAN OPINION SCORES COLLECTION
----------------------------------------
This section catalogs the evaluation setup for mean opinion scores by describing 31 models in total: 14 TTM systems [1]-[3], [18]-[22] and 7 TTA systems [23]-[29], some with multiple variants, thereby yielding 31 models. These systems vary along four dimensions shown in Fig. 4: accessibility, commercialization, year, and model size. Twenty-five systems are publicly accessible for sample generation, while six provide only demo audio; commercially, six are commercial and 25 non-commercial; temporally, 14 were developed in 2023, 7 in 2024, and 10 in 2022; models span large-scale and smaller architectures. Most TTM systems generate music by latent representations or discrete tokens decoded into waveform via autoregressive or non-autoregressive methods; symbolic generation exists but is limited in TTM due to timbre modeling, though a few samples from symbolic systems based on ABC notation and MIDI are included to diversify. Objective metrics used are MSE, LCC, SRCC, and KTAU. MusicEval comprises 384 prompts (80 manual, 20 from MusicCaps, 284 from system demo pages); among these, 100 prompts generate music with open-access models and 284 with demo-only systems. Prompt lengths show a broad short-to-mid-length range with high variance, and prompts focus on pop and classical genres to leverage evaluators’ expertise. The dataset spans 16.62 hours, with 2,748 mono audio clips and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) across 384 prompts; all audio are resampled to 16 kHz mono via ffmpeg, and each clip is rated by five experts with an 85/15 train/test split. MusicEval is presented as the first expert-scored generative-music evaluation dataset, accompanied by a CLAP-based baseline predicting both musical impression and textual alignment, using the official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, plus a 3-layer MLP head trained with L1 loss, with experiments on a single NVIDIA 4090 GPU at batch size 64. Table I reports utterance-level and system-level metrics: musical impression U–MSE 0.647, U–LCC 0.606, U–SRCC 0.633, U–KTAU 0.461; textual alignment U–MSE 0.616, U–LCC 0.438, U–SRCC 0.443, U–KTAU 0.314; system-level musical impression S–MSE 0.446, S–LCC 0.839, S–SRCC 0.926, S–KTAU 0.767; textual alignment S–MSE 0.354, S–LCC 0.757, S–SRCC 0.784, S–KTAU 0.617, all under the 85/15 split. The CLAP checkpoint is the official pretrained one; finetuning uses SGD with learning rate 0.0005 and batch size 64 on an NVIDIA 4090. Results show utterance-level predictions align well with human judgments, while alignment predictions are harder, supporting CLAP-based automatic scoring; future work will broaden genres and architectures. To enhance reliability, two probing methods are used: inserting real AudioSet clips into batches to invalidate scores below 3 and interleaving identical samples within a batch to detect inconsistent raters. Score distributions for musical impression and textual alignment are approximately normal with mid-range peaks; cross-system variation confirms MusicEval’s breadth and the value of CLAP-based scoring. The section frames the lack of a universal evaluation paradigm for TTM and motivates automatic, perception-aligned assessment via the two dimensions—overall musical impression and alignment with the text prompt—captured by MusicEval and CLAP-based scoring. It also details the methodological setup: CLAP as the upstream audio feature extractor, a 3-layer MLP as the downstream head, and L1 loss; MOS data were collected online from 14 raters (2 professionals and 12 conservatory students) totaling 13,740 ratings, with samples scored on two 5-point Likert dimensions by five evaluators each; data were released in batches with replayable samples. Reliability checks include real-music clip insertion and repeat-sample checks, and Figures 1–3 illustrate score distributions—both dimensions show roughly normal shapes with mid-range peaks—and Figures 2 and 3 reveal per-system score distributions, indicating intra-system consistency but notable inter-system variation.

섹션 etc: A. Evaluation Dimension
----------------------------------------
To ensure comprehensive coverage of generative systems, we assemble 14 TTM systems [1]-[3], [18]-[22] and 7 text-to-audio (TTA) systems [23]-[29], some with multiple variants, yielding 31 models. These systems vary across four dimensions—accessibility, commercialization, year, and model size—as shown in Fig. 4. Twenty-five systems are publicly accessible for sample generation, while the remaining six provide only demo audio. Commercially, six are commercial and 25 non-commercial; temporally, 14 were developed in 2023, 7 in 2024, and 10 in 2022, with models spanning large-scale to smaller architectures. Most TTM systems generate music from latent representations or discrete tokens decoded into waveforms via autoregressive or non-autoregressive methods; symbolic generation exists but is limited by timbre modeling, though a few samples from ABC notation and MIDI-based systems are included to diversify. We evaluate with objective metrics MSE, LCC, SRCC, and KTAU. MusicEval comprises 384 prompts (80 manual, 20 from MusicCaps, 284 from system demos); among these, 100 prompts generate music with open-access models and 284 with demo-only systems. Prompts are crafted to cover emotion, structure, rhythm, theme, and instrumentation, with some prompts from existing datasets to reflect training exposure; prompt lengths span a broad range with high variance, concentrated on short to mid-length prompts, and the dataset focuses on pop and classical genres to leverage expert reliability. MusicEval totals 16.62 hours of mono audio across 2,748 clips and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) over 384 prompts; audio are resampled to 16 kHz mono via ffmpeg, and each clip is rated by five conservatory experts. The dataset and scoring pipeline are described in Sections III–IV. MusicEval is the first expert-scored automatic evaluation dataset for generative music, and a CLAP-based baseline is proposed to predict both musical impression and textual alignment. The baseline uses the official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, with a 3-layer MLP head trained by L1 loss, on a single NVIDIA 4090 GPU with batch size 64 and SGD learning rate 0.0005. Table I reports utterance-level metrics (U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461) and textual alignment (U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314); and system-level metrics (S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767; S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617), all under the 85/15 split. Results indicate utterance-level predictions align well with human judgments, while alignment predictions are harder, supporting CLAP-based scoring and motivating broader genre and architecture coverage in future work. To bolster reliability, two probing methods insert real AudioSet clips into batches to invalidate scores below 3 and interleave identical samples within a batch to detect inconsistent raters. Score distributions for musical impression and textual alignment are approximately normal with mid-range peaks; cross-system variation confirms MusicEval’s breadth and the value of CLAP-based scoring. The section frames the lack of a universal evaluation paradigm for TTM and advocates automatic, perception-aligned assessment across two dimensions—overall musical impression and alignment with the text prompt—captured by MusicEval and CLAP-based scoring, with CLAP as the upstream audio feature extractor, a 3-layer MLP head, and L1 loss; MOS data were collected online from 14 raters totaling 13,740 ratings, with samples scored on two 5-point Likert scales by five evaluators each, released in batches with replayable samples. Reliability checks include real-music clip insertion and repeat-sample checks; Figures 1–3 illustrate score distributions and per-system variations, indicating intra-system consistency but notable inter-system differences.

섹션 etc: B. Listening Test Design
----------------------------------------
We select 14 TTM systems [1]-[3], [18]-[22] and 7 text-to-audio (TTA) systems [23]-[29], some with multiple models differing in size or training data, yielding a total of 31 models that vary across four dimensions, as illustrated in Fig. 4. Twenty-five systems are publicly accessible and used to generate samples from designed prompts, while the remaining six provide only demo audio. In terms of commercialization, six systems are commercial and 25 are non-commercial. Temporally, 14 systems were developed in 2023, 7 in 2024, and 10 in 2022, with models spanning large-scale to smaller architectures, and a balanced mix ensures coverage of diverse characteristics. Most TTM systems generate music by producing latent representations or discrete tokens decoded into waveforms via autoregressive or non-autoregressive methods; symbolic generation exists but remains limited for TTM due to timbre modeling limitations, though a few samples from symbolic systems using ABC notation and MIDI are included to diversify. Evaluation relies on objective indicators MSE, LCC, SRCC, and KTAU. The MusicEval dataset comprises 384 prompts (80 manual, 20 from MusicCaps, 284 from system demos); among these, 100 prompts are used with open-access models and 284 with demo-only systems, with prompts crafted to address emotion, structure, rhythm, theme, and instrumentation; prompt lengths show a broad distribution with high variance, concentrated on short to mid-length prompts, and focus on pop and classical genres to leverage expert reliability. MusicEval totals 16.62 hours of mono audio across 2,748 clips and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) over 384 prompts; audio is resampled to 16 kHz mono via ffmpeg, with each clip scored by five conservatory experts. The dataset and scoring pipeline are described in Sections III–IV. MusicEval is the first expert-scored automatic evaluation dataset for generative music, and a CLAP-based baseline is proposed to predict both musical impression and textual alignment. The baseline uses the official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, with a 3-layer MLP head trained by L1 loss, on a single NVIDIA 4090 GPU with batch size 64 and SGD learning rate 0.0005. Table I reports utterance-level metrics (U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461) and textual alignment (U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314); and system-level metrics (Musical impression: S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767; Textual alignment: S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617), all under the 85/15 split. Results show utterance-level predictions align well with human judgments, while alignment predictions are harder, supporting CLAP-based scoring and motivating broader genre and architecture coverage in future work. To bolster reliability, two probing methods insert real AudioSet clips into batches to invalidate scores below 3 and interleave identical samples within a batch to detect inconsistent raters. Score distributions for musical impression and textual alignment are approximately normal with mid-range peaks, and cross-system variation confirms MusicEval’s breadth and the value of CLAP-based scoring. The section argues for a universal evaluation paradigm for TTM and advocates automatic, perception-aligned assessment across two dimensions—overall musical impression and alignment with the text prompt—captured by MusicEval and CLAP-based scoring, with CLAP as the upstream audio feature extractor, a 3-layer MLP head, and L1 loss; MOS data were collected online from 14 raters totaling 13,740 ratings, with samples scored on two 5-point Likert scales by five evaluators each, released in batches with replayable samples. Reliability checks include real-music clip insertion and repeat-sample checks; Figures 1–3 illustrate score distributions and per-system variations, indicating intra-system consistency but notable inter-system differences.

섹션 etc: C. Quality of Ratings
----------------------------------------
This section documents the quality of ratings used to evaluate generative music, focusing on 31 models drawn from 14 TTM systems [1]-[3], [18]-[22] and 7 TTA systems [23]-[29], some with multiple variants, spanning four characteristic dimensions shown in Fig. 4: accessibility, commercialization, year, and model size. Twenty-five systems are publicly accessible and used to generate samples from designed prompts, while six provide only demo audio. Commercially, six are commercial and 25 non-commercial, offering a balanced mix of industry and academia. Temporal coverage includes 14 systems developed in 2023, 7 in 2024, and 10 in 2022, with models ranging from large-scale to smaller architectures. Most TTM systems generate music by turning latent representations or tokens into waveforms via autoregressive or non-autoregressive decoding; symbolic generation exists but is limited for timbre modeling, though a few samples from ABC/MIDI-based symbolic systems are included to diversify. The evaluation uses objective metrics MSE, LCC, SRCC, and KTAU to compare predictions against ground truth.

MusicEval, the generative music evaluation dataset introduced here, comprises 384 prompts (80 manual, 20 from MusicCaps, 284 from system demos); 100 prompts are used with open-access models, and 284 with demo-only systems. Prompts address emotion, structure, rhythm, theme, and instrumentation, with lengths broadly distributed and high variance, focusing on pop and classical genres to leverage expert reliability. The dataset contains 16.62 hours of mono audio, 2,748 clips, and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) over 384 prompts; audio is resampled to 16 kHz mono via ffmpeg, with each clip scored by five conservatory experts. Details of data collection and processing are given in Sections III–IV. MusicEval is presented as the first expert-scored automatic evaluation dataset for generative music, alongside a CLAP-based baseline to predict both musical impression and textual alignment. The baseline uses the official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, with a 3-layer MLP head trained by L1 loss on a single NVIDIA 4090 GPU with batch size 64 and SGD learning rate 0.0005.

Table I reports utterance-level metrics (U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461) for musical impression and (U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314) for textual alignment, and system-level metrics (Musical impression: S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767; Textual alignment: S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617) under the 85/15 split. The results show strong alignment of utterance-level predictions with human judgments, while alignment predictions are more challenging, supporting the CLAP-based scoring approach and motivating broader genre and architecture coverage in future work. To enhance reliability, two probing methods insert real AudioSet clips into batches to invalidate scores below 3 and interleave identical samples within batches to detect inconsistent raters. The distributions of musical-impression and textual-alignment scores are approximately normal with mid-range peaks, and cross-system variation confirms MusicEval’s breadth and the value of CLAP-based scoring. The section advocates a universal evaluation paradigm for TTM, combining automatic assessment with human-perception alignment across two dimensions—overall musical impression and alignment with the text prompt—captured by MusicEval and CLAP-based scoring, with CLAP as the upstream feature extractor, a 3-layer MLP head, and L1 loss. MOS data were collected online from 14 raters totaling 13,740 ratings, with samples scored on two 5-point Likert scales by five evaluators each, released in batches with replayable samples, and reliability checks include real-music clip insertion and repeat-sample checks; Figures 1–3 illustrate score distributions and per-system variations, indicating intra-system consistency but notable inter-system differences.

섹션 etc: A. Model Architecture
----------------------------------------
This section describes the composition and evaluation framework for MusicEval, introducing 31 models drawn from 14 TTM and 7 TTA systems to cover broad architectural diversity; 25 systems are publicly accessible (with 6 providing only demo audio) and 6 are commercial while 25 are non-commercial, spanning 2022, 2023, and 2024 (14, 7, and 10 systems respectively) and varying in size. Most TTM systems generate music by decoding latent representations or discrete tokens into waveforms via autoregressive or non-autoregressive methods, with symbolic generation contributing limited timbre modeling opportunities; a few symbolic samples (ABC/MIDI) are included to enrich diversity. Evaluation uses objective metrics MSE, LCC, SRCC, and KTAU to compare predictions against ground truth. MusicEval comprises 384 prompts (80 manual, 20 from MusicCaps, 284 from system demos); 100 prompts are used with open-access models and 284 with demo-only systems. Prompts address emotion, structure, rhythm, theme, and instrumentation, with lengths showing broad short-to-mid ranges and high variance; focus genres are pop and classical to leverage expert reliability. The dataset spans 16.62 hours of mono audio, 2,748 clips, and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) over 384 prompts, with audio resampled to 16 kHz mono via ffmpeg and each clip scored by five conservatory experts; details of data collection and processing are provided in Sections III–IV. MusicEval is presented as the first expert-scored automatic evaluation dataset for generative music, alongside a CLAP-based baseline to predict both musical impression and textual alignment, using the official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, with a 3-layer MLP head trained by L1 loss on a single NVIDIA 4090 GPU with batch size 64 and SGD learning rate 0.0005. In Table I, utterance-level metrics for musical impression are U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461, and for textual alignment U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314, while system-level metrics show musical impression S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767 and textual alignment S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617 under an 85/15 train/test split. The results indicate strong alignment of utterance-level predictions with human judgments, but greater difficulty predicting alignment, justifying the CLAP-based approach and broader genre coverage for future work. To bolster reliability, two probing methods insert real AudioSet clips into batches to invalidate scores below 3 and interleave identical samples within batches to detect inconsistent raters; score distributions for musical impression and textual alignment are approximately normal with mid-range peaks, and cross-system variation confirms MusicEval’s breadth and the value of CLAP-based scoring. The section advocates a universal evaluation paradigm for TTM that couples automatic assessment with human-perception alignment across two dimensions—overall musical impression and alignment with the text prompt—captured by MusicEval and CLAP-based scoring with CLAP as the upstream feature extractor, a 3-layer MLP head, and L1 loss. MOS data were collected online from 14 raters totaling 13,740 ratings, with samples scored on two 5-point Likert scales by five evaluators each, released in batches with replayable samples; reliability checks include real-music clip insertion and repeat-sample checks, and Figures 1–3 illustrate score distributions and per-system variations, indicating intra-system consistency but notable inter-system differences. The paper details a two-dimension evaluation framework, the first expert-scored generative-music dataset, and a CLAP-based automatic scoring baseline, with future work targeting broader genres and more advanced automatic evaluation architectures.

섹션 etc: B. Implementation Details
----------------------------------------
Section B. Implementation Details describes how MusicEval covers broad generative-system diversity and how its evaluation framework is operationalized. It includes 14 TTM systems [1]-[3], [18]-[22] and 7 TTA systems [23]-[29], totaling 31 models, with Figure 4 illustrating a four-dimension breakdown across accessibility, commercialization, year, and model size. Of these, 25 systems are publicly accessible for sample generation, while 6 not publicly available are represented via their demo audio; commercialization comprises 6 commercial systems [30]-[35] and 25 non-commercial, and temporal distribution includes 10 systems from 2022, 14 from 2023, and 7 from 2024, with varying model sizes to capture diversity. Most TTM systems generate music by decoding latent representations or discrete tokens into waveforms via autoregressive or non-autoregressive methods, though symbolic generation—ABC notation and MIDI—also contributes diversity, despite limited timbre modeling and reliance on external renderers; a few symbolic samples from system [20] are included. Performance is evaluated with objective metrics MSE, LCC, SRCC, and KTAU between predictions and ground-truths. MusicEval comprises 384 prompts (80 manual, 20 MusicCaps, 284 from system demos); 100 prompts are used with open-access models, and 284 with demo-only systems. Manual prompts address emotion, structure, rhythm, theme, and instrumentation, while distribution analyses show a broad range of prompt lengths with high variance, focusing on pop and classical genres for reliability. The dataset totals 16.62 hours of mono audio, 2,748 clips, and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) across 384 prompts, with audio resampled to 16 kHz mono via ffmpeg and each clip scored by five conservatory experts; details of data collection and processing are given in Sections III–IV. MusicEval is presented as the first expert-scored generative-music evaluation dataset, alongside a CLAP-based baseline that predicts both musical impression and textual alignment, using an official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, a 3-layer MLP head, and an L1 loss, trained on a single NVIDIA 4090 GPU with batch size 64 and SGD learning rate 0.0005. Table I reports utterance-level metrics for musical impression (U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461) and textual alignment (U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314), and system-level metrics for musical impression (S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767) and textual alignment (S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617) under an 85/15 train/test split; results indicate strong utterance-level alignment with human judgments but greater difficulty predicting alignment, justifying the CLAP-based approach and broader genre coverage for future work. Two probing methods insert real AudioSet clips into batches and interleave identical samples to detect inconsistent raters, with score distributions for musical impression and textual alignment approximately normal and mid-range peaks, confirming cross-system breadth and the value of CLAP scoring. The section advocates a universal evaluation paradigm that couples automatic assessment with human-perception alignment across two dimensions—overall musical impression and alignment with the text prompt—captured by MusicEval and CLAP-based scoring, using CLAP as the upstream feature extractor, a 3-layer MLP head, and L1 loss. MOS data were collected online from 14 raters totaling 13,740 ratings, with samples scored on two 5-point Likert scales by five evaluators each, released in batches with replayable samples; reliability checks include real-music clip insertion and repeat-sample checks, and Figures 1–3 illustrate score distributions and per-system variations, indicating intra-system consistency but notable inter-system differences. The paper outlines a two-dimension evaluation framework, the first expert-scored dataset for generative music, and a CLAP-based automatic scoring baseline, with future work pursuing broader genres and more advanced evaluation architectures, and details a CLAP-based baseline where CLAP serves as the upstream audio feature extractor and a 3-layer MLP head is trained with L1 loss on a batch size of 64 using a single NVIDIA 4090 GPU with SGD learning rate 0.0005. The MusicEval dataset is randomly split into train and test sets at 85% and 15%, respectively.

섹션 etc: C. Test Metrics
----------------------------------------
To ensure broad coverage of generative systems, Section C includes 14 TTM systems [1]-[3], [18]-[22] and 7 text-to-audio (TTA) systems [23]-[29], totaling 31 models. Figure 4 presents a four-dimension breakdown across accessibility, commercialization, year, and model size; 25 systems are publicly accessible for sample generation, while the remaining 6 not publicly available are represented via their demo audio. Commercialization comprises 6 commercial and 25 non-commercial systems, with temporal distribution of 14 systems from 2023, 7 from 2024, and 10 from 2022, alongside varying model sizes to capture diversity. Most TTM systems generate music by decoding latent representations or discrete tokens into waveforms via autoregressive or non-autoregressive methods; symbolic generation (ABC notation and MIDI) also contributes, though timbre modeling is limited, with a few symbolic samples included from system [20]. Performance is assessed with objective metrics MSE, LCC, SRCC, and KTAU between predictions and ground-truths.

The MusicEval dataset comprises 384 prompts—80 manual, 20 MusicCaps, and 284 from system demos—producing samples by 21 systems (31 models); 100 prompts are used with open-access models, and 284 with demo-only systems. Manual prompts address emotion, structure, rhythm, theme, and instrumentation; prompt-length distributions are broad with high variance, focusing on pop and classical genres for reliability (Figure 5). The dataset totals 16.62 hours of mono audio, 2,748 clips, and 13,740 ratings from 14 music experts, with all samples resampled to 16 kHz mono via ffmpeg and scored by five conservatory experts; details are in Sections III–IV. A CLAP-based baseline predicts both musical impression and textual alignment, using an official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, a 3-layer MLP head, and L1 loss, trained on a single NVIDIA 4090 GPU with batch size 64 and SGD LR 0.0005. Table I reports utterance-level metrics (U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461 for musical impression; U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314 for textual alignment) and system-level metrics (S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767 for musical impression; S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617 for textual alignment) under an 85/15 train/test split. The CLAP baseline uses an official pretrained checkpoint trained on music, AudioSet, and LAION-Audio-630k, with results indicating strong utterance-level alignment but more difficulty predicting alignment at the system level. Two probing methods insert real AudioSet clips and interleave identical samples to detect inconsistent raters, yielding approximately normal score distributions with mid-range peaks, validating cross-system breadth and CLAP scoring. The section advocates a universal evaluation paradigm that couples automatic assessment with human-perception alignment across two dimensions—overall musical impression and alignment with the text prompt—captured by MusicEval and CLAP-based scoring, with reliability checks including online MOS data from 14 raters (13,740 ratings) and batch-replay capabilities, and highlights future work toward broader genres and more advanced evaluation architectures.

섹션 etc: D. Experimental Results
----------------------------------------
Section D reports a comprehensive evaluation across 31 models drawn from 14 TTM and 7 TTA systems. Among these, 25 systems are publicly accessible for music sample generation, while 6 are not publicly available and are evaluated via their demo audio; 6 systems are commercial and 25 non-commercial, with development distributed as 14 in 2023, 7 in 2024, and 10 in 2022, and a mix of model sizes to ensure diversity. Most systems generate music by decoding latent representations or discrete tokens into waveforms through autoregressive or non-autoregressive methods, with only a few symbolic systems (ABC notation and MIDI) included to broaden coverage due to timbre limitations. Evaluation uses objective metrics MSE, LCC, SRCC, and KTAU between predictions and ground-truths. The MusicEval dataset comprises 384 prompts (80 manual, 20 MusicCaps, 284 from demos) produced by 21 systems (31 models); 100 prompts are used with open-access models, while 284 come from demo-only systems. Prompts address emotion, structure, rhythm, theme, and instrumentation; prompt lengths show broad distribution with high variance and a focus on pop and classical genres. MusicEval totals 16.62 hours of mono audio, 2,748 clips, and 13,740 ratings from 14 music experts, with all samples resampled to 16 kHz mono via ffmpeg and scored by five conservatory experts; details are in Sections III–IV. A CLAP-based baseline predicts both musical impression and textual alignment, using an official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, a 3-layer MLP head, and L1 loss, trained on a single NVIDIA 4090 with batch size 64 and SGD LR 0.0005. Table I reports utterance-level metrics (Musical impression: U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461; Textual alignment: U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314) and system-level metrics (Musical impression: S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767; Textual alignment: S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617) under an 85/15 train/test split, with the CLAP checkpoint pretrained on music, AudioSet, and LAION-Audio-630k. The results indicate strong utterance-level alignment but weaker system-level alignment; two probing methods—inserting real AudioSet clips and interleaving identical samples—yield approximately normal distributions with mid-range peaks, supporting cross-system breadth and CLAP scoring validity. The authors advocate a universal evaluation paradigm that couples automatic assessment with human-perception alignment across two dimensions—overall musical impression and alignment with the text prompt—via MusicEval and CLAP-based scoring, augmented by online MOS data (13,740 ratings from 14 raters) and batch-replay reliability checks, and outline future work toward broader genres and more advanced evaluation architectures. The paper presents three contributions: defining automatic evaluation for TTM, introducing MusicEval with 31 models across 384 prompts and expert MOS, and developing a CLAP-based dual-prediction model, demonstrating feasibility and establishing a strong baseline. MOS testing was conducted online by 14 professionals who, after a training session, listened to prompt-based samples and rated them on two 5-point Likert scales; each sample was judged by five evaluators and averaged, with reliability reinforced by inserting real AudioSet clips and placing identical samples within batches to flag inconsistent raters, and results are illustrated by score distributions showing broadly normal patterns and system-level differences.

섹션 etc: VI. CONCLUSION
----------------------------------------
This concluding section evaluates 31 models drawn from 14 TTM systems [1]-[3], [18]-[22] and 7 TTA systems [23]-[29], where some have multiple variants, yielding 31 models in total. The systems vary across several dimensions, summarized by a four-dimension pie chart in Figure 4: accessibility, commercialization, year, and model size. Specifically, 25 systems are publicly accessible while 6 are not and are assessed via their demo audio; 6 systems are commercial and 25 non-commercial; development spans 2022 (10), 2023 (14), and 2024 (7); model sizes are diversified. Most TTM systems generate music by decoding latent representations or discrete tokens into waveforms via autoregressive or non-autoregressive methods, with a few symbolic systems (ABC notation and MIDI) included to broaden coverage due to timbre limitations. Objective evaluation uses mean square error (MSE), linear correlation coefficient (LCC), Spearman rank correlation (SRCC), and Kendall tau (KTAU) between predictions and ground truths. The MusicEval dataset comprises 384 prompts (80 manual, 20 MusicCaps, 284 from demos) produced by 21 systems (31 models); 100 prompts are used with open-access models, while 284 come from demo-only systems. Prompt lengths show broad distribution with high variance, focusing on pop and classical genres. MusicEval totals 16.62 hours of mono audio, 2,748 clips, and 13,740 ratings from 14 music experts; all samples are resampled to 16 kHz mono with ffmpeg and scored by five conservatory experts; details are in Sections III–IV. A CLAP-based baseline predicts both musical impression and textual alignment, using an official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, a 3-layer MLP head, and L1 loss, trained on a single NVIDIA 4090 with batch size 64 and SGD LR 0.0005. Table I reports utterance-level metrics (Musical impression: U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461; Textual alignment: U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314) and system-level metrics (Musical impression: S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767; Textual alignment: S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617) under an 85/15 train/test split, with the CLAP checkpoint pretrained on music, AudioSet, and LAION-Audio-630k. The results indicate strong utterance-level alignment but weaker system-level alignment; two probing methods—inserting real AudioSet clips and interleaving identical samples—yield approximately normal distributions with mid-range peaks, supporting cross-system breadth and CLAP scoring validity. The authors advocate a universal evaluation paradigm that couples automatic assessment with human-perception alignment across two dimensions—overall musical impression and alignment with the text prompt—via MusicEval and CLAP-based scoring, augmented by online MOS data (13,740 ratings from 14 raters) and batch-replay reliability checks, and outline future work toward broader genres and more advanced evaluation architectures. The paper presents three contributions: defining automatic evaluation for TTM, introducing MusicEval with 31 models across 384 prompts and expert MOS, and developing a CLAP-based dual-prediction model, demonstrating feasibility and establishing a strong baseline. MOS tests were conducted online by 14 professionals who, after training, listened to prompt-based samples and rated them on two 5-point Likert scales; each sample was judged by five evaluators and averaged, with reliability reinforced by inserting real AudioSet clips and placing identical samples within batches to flag inconsistent raters; results are illustrated by score distributions indicating broadly normal patterns and system-level differences. The CLAP-based model uses a pre-trained CLAP upstream feature extractor and a 3-layer MLP to predict musical impression and textual alignment, trained with L1 loss; training used batch size 64 on a single NVIDIA 4090. Finally, the MusicEval dataset is randomly split into an 85% train and 15% test set.

============================================================
총 섹션 수: 14
생성 시간: 2025-08-19 08:40:11
