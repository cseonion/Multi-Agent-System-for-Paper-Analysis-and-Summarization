**Final Summary of "MusicEval: A Generative Music Dataset with Expert Ratings for Automatic Text-to-Music Evaluation"**

**1. Research Objective and Background**  
The paper addresses the critical need for a standardized, perception-aligned evaluation framework for text-to-music (TTM) generative systems, a rapidly advancing field lacking universal benchmarks and automatic assessment tools. The authors introduce *MusicEval*, the first large-scale, expert-scored dataset designed to enable automatic and human-aligned evaluation of generative music models. The dataset and framework aim to assess two key dimensions: (1) overall musical impression and (2) alignment between generated music and the input text prompt, thus providing a comprehensive basis for both system development and comparative research.

**2. Key Methodology**  
MusicEval encompasses 31 models from 21 systems (14 TTM and 7 text-to-audio [TTA]), spanning a diverse range of accessibility (25 public, 6 demo-only), commercialization (6 commercial, 25 non-commercial), development years (2022–2024), and model sizes. The dataset is constructed from 384 prompts (80 manual, 20 from MusicCaps, 284 from system demos), focusing on pop and classical genres to leverage expert evaluator reliability. These prompts elicit 2,748 mono audio clips (16.62 hours), each rated by five of 14 music experts (2 professionals, 12 conservatory students), yielding 13,740 ratings on two 5-point Likert scales (musical impression and textual alignment). All audio is resampled to 16 kHz mono for consistency. The evaluation pipeline includes a CLAP-based baseline: the CLAP (Contrastive Language-Audio Pretraining) model serves as the upstream audio feature extractor, with a 3-layer MLP head trained using L1 loss to predict both evaluation dimensions. Training is performed on a single NVIDIA 4090 GPU (batch size 64, SGD learning rate 0.0005), and the dataset is split 85/15 for train/test. Objective metrics—mean square error (MSE), linear correlation coefficient (LCC), Spearman rank correlation (SRCC), and Kendall tau (KTAU)—quantify alignment between model predictions and expert ratings. Reliability is ensured by inserting real AudioSet clips (to flag inattentive raters) and interleaving identical samples (to detect inconsistent raters).

**3. Results and Conclusion**  
The CLAP-based baseline demonstrates strong alignment with human judgments at the utterance level, particularly for musical impression (U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461), and moderate performance for textual alignment (U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314). At the system level, performance improves (musical impression: S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767; textual alignment: S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617), indicating the model’s ability to capture broader system trends. Score distributions for both dimensions are approximately normal with mid-range peaks, and notable inter-system variation is observed, reflecting the dataset’s breadth. The results validate the feasibility of automatic, perception-aligned evaluation for TTM and establish a strong baseline for future research.

**4. Implications and Significance**  
MusicEval represents a foundational advance for the TTM research community, providing the first expert-scored, large-scale dataset and a robust, reproducible evaluation pipeline. By enabling automatic, perception-aligned assessment across diverse generative systems, MusicEval facilitates fair benchmarking, accelerates model development, and supports the creation of more musically and semantically coherent generative models. The dual-dimension evaluation (musical impression and textual alignment) captures both subjective quality and prompt fidelity, addressing key gaps in prior work. The CLAP-based baseline offers a practical starting point for future automatic evaluators, and the dataset’s diversity ensures broad applicability.

**5. Limitations**  
While MusicEval covers a wide range of systems, genres, and prompt types, its focus on pop and classical genres may limit generalizability to other musical styles. The reliance on expert raters, though ensuring high-quality annotations, may introduce subjective bias and limit scalability. The CLAP-based baseline, while effective, shows weaker performance on textual alignment, indicating room for improvement in modeling prompt-music correspondence. Finally, the dataset’s current scope is limited to the systems and prompts available at the time of collection; future work should expand genre coverage, include more diverse systems, and explore advanced evaluation architectures.

**In summary**, MusicEval establishes a new standard for TTM evaluation by combining expert-annotated data, a rigorous evaluation framework, and a strong automatic baseline, paving the way for more reliable, scalable, and perception-aligned assessment of generative music systems.