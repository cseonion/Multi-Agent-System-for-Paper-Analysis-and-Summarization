Section B. Implementation Details describes how MusicEval covers broad generative-system diversity and how its evaluation framework is operationalized. It includes 14 TTM systems [1]-[3], [18]-[22] and 7 TTA systems [23]-[29], totaling 31 models, with Figure 4 illustrating a four-dimension breakdown across accessibility, commercialization, year, and model size. Of these, 25 systems are publicly accessible for sample generation, while 6 not publicly available are represented via their demo audio; commercialization comprises 6 commercial systems [30]-[35] and 25 non-commercial, and temporal distribution includes 10 systems from 2022, 14 from 2023, and 7 from 2024, with varying model sizes to capture diversity. Most TTM systems generate music by decoding latent representations or discrete tokens into waveforms via autoregressive or non-autoregressive methods, though symbolic generation—ABC notation and MIDI—also contributes diversity, despite limited timbre modeling and reliance on external renderers; a few symbolic samples from system [20] are included. Performance is evaluated with objective metrics MSE, LCC, SRCC, and KTAU between predictions and ground-truths. MusicEval comprises 384 prompts (80 manual, 20 MusicCaps, 284 from system demos); 100 prompts are used with open-access models, and 284 with demo-only systems. Manual prompts address emotion, structure, rhythm, theme, and instrumentation, while distribution analyses show a broad range of prompt lengths with high variance, focusing on pop and classical genres for reliability. The dataset totals 16.62 hours of mono audio, 2,748 clips, and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) across 384 prompts, with audio resampled to 16 kHz mono via ffmpeg and each clip scored by five conservatory experts; details of data collection and processing are given in Sections III–IV. MusicEval is presented as the first expert-scored generative-music evaluation dataset, alongside a CLAP-based baseline that predicts both musical impression and textual alignment, using an official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, a 3-layer MLP head, and an L1 loss, trained on a single NVIDIA 4090 GPU with batch size 64 and SGD learning rate 0.0005. Table I reports utterance-level metrics for musical impression (U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461) and textual alignment (U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314), and system-level metrics for musical impression (S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767) and textual alignment (S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617) under an 85/15 train/test split; results indicate strong utterance-level alignment with human judgments but greater difficulty predicting alignment, justifying the CLAP-based approach and broader genre coverage for future work. Two probing methods insert real AudioSet clips into batches and interleave identical samples to detect inconsistent raters, with score distributions for musical impression and textual alignment approximately normal and mid-range peaks, confirming cross-system breadth and the value of CLAP scoring. The section advocates a universal evaluation paradigm that couples automatic assessment with human-perception alignment across two dimensions—overall musical impression and alignment with the text prompt—captured by MusicEval and CLAP-based scoring, using CLAP as the upstream feature extractor, a 3-layer MLP head, and L1 loss. MOS data were collected online from 14 raters totaling 13,740 ratings, with samples scored on two 5-point Likert scales by five evaluators each, released in batches with replayable samples; reliability checks include real-music clip insertion and repeat-sample checks, and Figures 1–3 illustrate score distributions and per-system variations, indicating intra-system consistency but notable inter-system differences. The paper outlines a two-dimension evaluation framework, the first expert-scored dataset for generative music, and a CLAP-based automatic scoring baseline, with future work pursuing broader genres and more advanced evaluation architectures, and details a CLAP-based baseline where CLAP serves as the upstream audio feature extractor and a 3-layer MLP head is trained with L1 loss on a batch size of 64 using a single NVIDIA 4090 GPU with SGD learning rate 0.0005. The MusicEval dataset is randomly split into train and test sets at 85% and 15%, respectively.