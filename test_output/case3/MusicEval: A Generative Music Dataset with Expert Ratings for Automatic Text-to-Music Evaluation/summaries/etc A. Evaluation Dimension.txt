To ensure comprehensive coverage of generative systems, we assemble 14 TTM systems [1]-[3], [18]-[22] and 7 text-to-audio (TTA) systems [23]-[29], some with multiple variants, yielding 31 models. These systems vary across four dimensions—accessibility, commercialization, year, and model size—as shown in Fig. 4. Twenty-five systems are publicly accessible for sample generation, while the remaining six provide only demo audio. Commercially, six are commercial and 25 non-commercial; temporally, 14 were developed in 2023, 7 in 2024, and 10 in 2022, with models spanning large-scale to smaller architectures. Most TTM systems generate music from latent representations or discrete tokens decoded into waveforms via autoregressive or non-autoregressive methods; symbolic generation exists but is limited by timbre modeling, though a few samples from ABC notation and MIDI-based systems are included to diversify. We evaluate with objective metrics MSE, LCC, SRCC, and KTAU. MusicEval comprises 384 prompts (80 manual, 20 from MusicCaps, 284 from system demos); among these, 100 prompts generate music with open-access models and 284 with demo-only systems. Prompts are crafted to cover emotion, structure, rhythm, theme, and instrumentation, with some prompts from existing datasets to reflect training exposure; prompt lengths span a broad range with high variance, concentrated on short to mid-length prompts, and the dataset focuses on pop and classical genres to leverage expert reliability. MusicEval totals 16.62 hours of mono audio across 2,748 clips and 13,740 ratings from 14 music experts, produced by 21 systems (31 models) over 384 prompts; audio are resampled to 16 kHz mono via ffmpeg, and each clip is rated by five conservatory experts. The dataset and scoring pipeline are described in Sections III–IV. MusicEval is the first expert-scored automatic evaluation dataset for generative music, and a CLAP-based baseline is proposed to predict both musical impression and textual alignment. The baseline uses the official CLAP checkpoint trained on music, AudioSet, and LAION-Audio-630k, with a 3-layer MLP head trained by L1 loss, on a single NVIDIA 4090 GPU with batch size 64 and SGD learning rate 0.0005. Table I reports utterance-level metrics (U-MSE 0.647, U-LCC 0.606, U-SRCC 0.633, U-KTAU 0.461) and textual alignment (U-MSE 0.616, U-LCC 0.438, U-SRCC 0.443, U-KTAU 0.314); and system-level metrics (S-MSE 0.446, S-LCC 0.839, S-SRCC 0.926, S-KTAU 0.767; S-MSE 0.354, S-LCC 0.757, S-SRCC 0.784, S-KTAU 0.617), all under the 85/15 split. Results indicate utterance-level predictions align well with human judgments, while alignment predictions are harder, supporting CLAP-based scoring and motivating broader genre and architecture coverage in future work. To bolster reliability, two probing methods insert real AudioSet clips into batches to invalidate scores below 3 and interleave identical samples within a batch to detect inconsistent raters. Score distributions for musical impression and textual alignment are approximately normal with mid-range peaks; cross-system variation confirms MusicEval’s breadth and the value of CLAP-based scoring. The section frames the lack of a universal evaluation paradigm for TTM and advocates automatic, perception-aligned assessment across two dimensions—overall musical impression and alignment with the text prompt—captured by MusicEval and CLAP-based scoring, with CLAP as the upstream audio feature extractor, a 3-layer MLP head, and L1 loss; MOS data were collected online from 14 raters totaling 13,740 ratings, with samples scored on two 5-point Likert scales by five evaluators each, released in batches with replayable samples. Reliability checks include real-music clip insertion and repeat-sample checks; Figures 1–3 illustrate score distributions and per-system variations, indicating intra-system consistency but notable inter-system differences.