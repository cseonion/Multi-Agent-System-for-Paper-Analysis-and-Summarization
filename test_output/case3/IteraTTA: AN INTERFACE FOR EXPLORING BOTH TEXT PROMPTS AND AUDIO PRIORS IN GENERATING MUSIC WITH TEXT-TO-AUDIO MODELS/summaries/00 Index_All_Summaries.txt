논문 제목: IteraTTA: AN INTERFACE FOR EXPLORING BOTH TEXT PROMPTS AND AUDIO PRIORS IN GENERATING MUSIC WITH TEXT-TO-AUDIO MODELS
전체 섹션별 요약 인덱스
============================================================

섹션 1.: INTRODUCTION
----------------------------------------
Please provide the raw text of the first section of the paper and, if available, the summary of any previous sections. I will generate a concise, paragraph-form summary for this section, preserving all numerical details and key points, with no bullet points and no distortion of numbers. You can paste the section text and the prior-section summary together; I will proceed sequentially.

섹션 2.1: Music Generation Techniques
----------------------------------------
Music generation remains a central topic in the MIR community, with generative machine learning increasingly used to create both symbolic MIDI outputs and raw audio for greater expressiveness; notable examples like Jukebox and RAVE employ variational autoencoders and autoregressive models trained on large music datasets to generate diverse audio. Controllability is emphasized as essential to support users’ creative processes, with approaches such as Music FaderNets that adjust rhythm and note density, Music SketchNet that specifies pitch contours and rhythm patterns, and methods by Wang et al. and Dai et al. that constrain chord progressions. Yet many users lack musical knowledge, so some perceptual control methods based on emotion (valence-arousal) provide limited, four-class taxonomy that may restrict user agency. Text-to-audio models offer an effective, novice-friendly alternative by learning the relationship between audio and text descriptions through latent representations encoded by RoBERTa, enabling flexible control via text prompts of variable length and content and allowing constraints by audio priors to keep outputs aligned with reference audio. In diffusion-based AudioLDM, seeds traditionally based on Gaussian noise can be replaced by a noise-infused audio prior to preserve the characteristics of the input. The parallel to text-to-image models—where iterative prompt exploration and prior constraints boost creativity—inspires applying similar ideas to text-to-audio to enable iterative exploration and customization. Nonetheless, text-to-audio generation poses specific challenges discussed earlier, motivating the development of an interface dedicated to text-to-audio models to address these difficulties.

섹션 2.2: Interfaces for Music Generation
----------------------------------------
There is a series of research on building interfaces to let users interact with music generation techniques effectively [47][48][49][50][51][52][53]. MySong [47] involves a music accompaniment generation model with controls over happiness or jazziness. Louie et al. [49] provide an interactive interface for novices to use a symbolic music generation technique with controls of happiness or randomness, and the interface allows constraining results with music priors, which experiments confirm are effective for iterative refinement. Zhou et al. [52,53] use a user-in-the-loop Bayesian optimization approach to enable novice users to iteratively explore melodies produced by a generative model. These interfaces underscore the importance of controls and iterative exploration in facilitating novice creativity with music generation techniques. Consequently, deploying recent text-to-audio models to novices would be well suited, offering more flexible control than a handful of parameters such as happiness and enabling the use of audio priors. Our paper contributes by examining design considerations for interfaces for text-to-audio music generation processes, aiming to broaden the applications of recent MIR techniques.

섹션 3.: DESIGN REQUIREMENTS
----------------------------------------
Section 3 articulates design requirements for enabling novice users to creatively express themselves with text-to-audio music generation, guided by human–computer interaction principles. The authors used a think-aloud protocol with three volunteers who reported no formal musical training beyond compulsory education, giving them access to one of the latest text-to-audio models in Google Colab via its official implementation. The users could input any text prompts and listen to three generated music outputs; the participants, Japanese speakers, were advised to use DeepL Translator to translate prompts into English to better align with the model’s English-trained labels. Over roughly 30 minutes of free exploration—with screen sharing and verbalized thoughts—the study identified challenges and contributing factors, which were then validated and enriched through semistructured interviews. The responses were analyzed with open coding, yielding design requirements that align with the broader creativity-support literature.

섹션 3.1: Computational guidance for constructing initial prompts
----------------------------------------
Section 3.1 reports that volunteers struggled to craft effective initial prompts to activate the model. For example, the prompt “a song sounds like star wars” produced a battle-cry with a space-like sound, highlighting that the model’s music labels are largely musical descriptions—genres, instruments, and moods—and that prompts must align with those labels (e.g., “An orchestra plays a happy melody while the strings and wind instruments are being played”). The difficulty stems from a vocabulary gap between experts and novices and novices’ loosely specified goals, who typically refine objectives through iterative exploration. However, text-to-audio models depend on precise, clearly defined goals to initiate exploration, making initial prompt construction hard for novices. Consequently, providing computational guidance to help users build initial prompts could facilitate novice creativity, as exemplified by the successful prompting of “solemn music starting with a trumpet fanfare.”

섹션 3.2: Dual-sided iterative exploration of text prompts and audio priors
----------------------------------------
We observed that volunteers struggled to efficiently explore generated results in text-to-audio tasks. A volunteer with experience in text-to-image models noted that, unlike T2I, comparing results at a glance in text-to-audio was difficult, making it hard to find a text prompt that faithfully reflects their intention. Consequently, iterating prompts alone does not necessarily help novices understand the space of possible results, even though refining loosely specified goals is still necessary [60,61]. As a result, users cannot readily determine which direction or which prompt to try next to approach their goals. Another volunteer illustrated the problem by describing how adding “with a flute” to a prompt altered the melody so drastically that the result became frustratingly different. This suggests a need to constrain generation using not only text prompts but also audio priors. In sum, to support novice creativity in text-to-audio music generation, there must be an ability to efficiently explore variations in both text prompts and audio priors, enabling iterative goal refinement by understanding the space of possible results, which requires an interface specifically designed for text-to-audio models to support dual-sided exploration of prompts and priors.

섹션 4.: IteraTTA
----------------------------------------
IteraTTA is introduced as a dedicated interface for the text-to-audio music generation process, designed to meet the earlier design requirements. Implemented as a web-based system, it enables novice users to immediately benefit from the latest text-to-audio models in their creative workflows.

섹션 4.1: Design
----------------------------------------
The design section describes how IteraTTA begins with a user-entered theme phrase and uses a large language model to generate four variational lists of comma-separated phrases describing how a music clip of that theme might sound. These four lists serve as diverse first prompts to start music generation in parallel, enabling novices to translate loosely specified goals into musical descriptions and to envisage prompt variations. IteraTTA then creates three music audios for each of the four prompts (twelve in total), arranged in a two-dimensional layout (Figure 1) so users can compare how outputs differ across prompts and within the same prompt, aiding alignment with goals and indicating exploratory directions. If a suitable candidate prompt is found, users can customize it to generate new audios, or, if a suitable audio is found, use it as an audio prior to produce additional outputs; in short, users can explore the subspace near their goals by constraining prompts or priors while gradually refining their objectives. The section also introduces exploration-supporting features (Figure 2): when an audio prior is specified, users can compare generated results with it, and there is an instant text-prompt editing tool that can amplify or suppress a chosen instrument by adding phrases like “with strong [instrument]” or “with no [instrument]” to the prompt, illustrating how results can be steered through prompt adjustments.

섹션 4.2: Implementation
----------------------------------------
We implemented IteraTTA as a web-based system aimed at novice users. On the backend, we used Python with FastAPI and GPT-3.5 to construct the initial prompts, and Audi-oLDM to generate the music audios. The audio length was fixed at 10 seconds to enable the GPU server (an NVIDIA RTX 2080 Ti) to generate 12 audios (3 audios × 4 prompts) concurrently, with an average generation time of about 15 seconds. Additionally, the DeepL API translates non-English prompts into English, which yielded better results in Section 3. The frontend is built with Vue.js, allowing users to download generated music audios or share them on Twitter.

섹션 5.: ANALYSIS
----------------------------------------
To assess IteraTTA’s effectiveness for diverse users, we deployed a publicly accessible Japanese web service. Within two days of release, 8,831 users generated 246,423 music audios. In this section, we report insights from usage logs and from responses to a voluntary feedback form linked on the Web service for users to share their opinions.

섹션 5.1: Diversity of theme phrases
----------------------------------------
The section begins by noting that users’ theme phrases for initiating text-to-audio generation were highly diverse, with examples ranging from music-specific terms like “nice city pop” and “cute future bass” to more explicit phrases such as “80’s hip hop that break dancers would dance to,” and abstract ones like “Arabian caves” and “silent dream of a priestess,” a variety illustrated in a word cloud (Figure 3). To assess IteraTTA’s role, the authors compared the users’ theme phrases with text prompts generated by a large language model from those phrases and with the text labels in the training dataset. They randomly sampled 1,000 cases for each of the three categories—theme phrases, derived text prompts, and text labels—computed their representation vectors using the same pretrained RoBERTa model as the text-to-audio system, and visualized the distributions with t-SNE (Figure 4). The results suggest that IteraTTA helped the LLM produce prompts that bridge the gap between diverse user phrases and training labels. Notably, the LLM could derive musical descriptions even from abstract phrases, such as “otherworldly harmonies, delicate strings, minimalistic percussion, dreamlike vocals” for “silent dream of a priestess.” This demonstrates the effectiveness of guiding initial prompt construction to support novice users’ creative processes, as discussed in Section 3.1. The English version is available at iteratta.duckdns.org, and for text labels they extracted labels containing “music” from AudioCaps. Overall, the section shows that IteraTTA can align user-generated themes with dataset labels through LLM-derived prompts, including from abstract inputs.

섹션 5.2: Journey of iterative exploration
----------------------------------------
We analyzed how users interacted with IteraTTA-generated results by inspecting the service’s interaction logs and referring to Figure 5. The data show varied usage: some users tried the exploration feature only once, while others used it iteratively, alternating between entering text prompts and providing audio priors. In particular, one user repeated the refinement process 32 times, specifying text prompts 14 times and audio priors 18 times before sharing their final result on Twitter. These findings suggest that the dual-sided iterative exploration design helps users effectively leverage the text-to-audio model.

섹션 5.3: Unleashing the creativity of novice users
----------------------------------------
Section 5.3 analyzes 33 feedback responses, with the majority expressing positive experiences with the text-to-audio music creation process; examples include “It was a very interesting trial. I can interact with it throughout the day” and views of it as a source of sampling material and idea generator, with no negative feelings toward composing from text. The comments also suggest that IteraTTA’s novice-friendly features can benefit experienced users, echoing design requirements from Section 3 for open-ended exploration starting from loosely-specified theme phrases, such as “It was fun to encounter songs that fit the theme I provided but I had never heard before,” and “I really enjoyed … ChatGPT’s ability to associate and verbalize even seemingly unconnected ideas, which allowed me to provide crazy theme phrases.” Some users reported successful prompts after exploration, including adding a phrase of “simple progression” or limiting the number of tracks to yield stabilized music audios, for example: “Ideal harmonious song: balanced instrumentation, band sound, simple chord progressions, rhythmic drum patterns, catchy pop melody, up to 12 tracks,” and “Adding ‘clear sound quality’ produces less noisy audios.” It is notable that users gained knowledge about text-to-audio model behavior through iterative exploration, even without explicit description, and the authors note that prompt modifiers (quality boosters) known for text-to-image models would be among the first examples for text-to-audio models. They attribute this to user creativity in text-to-audio music generation and suggest it would be hard to derive without IteraTTA.

섹션 6.: CONCLUSION
----------------------------------------
IteraTTA is an interface designed to help novice users engage in text-to-audio music generation. Its design rests on two guiding principles: providing computational guidance to construct initial prompts and enabling dual-sided iterative exploration of text prompts and audio priors. The former assists users in translating loosely specified goals into starting prompts even when they lack rich artistic vocabularies, while the latter helps users grasp the space of possible results and progressively refine their aims. The authors deployed IteraTTA as a publicly available web service and analyzed user behaviors to demonstrate how these design choices support creativity. They emphasize that these principles generalize beyond the specific text-to-audio model to other models anticipated in the near future, arguing the work offers a foundation for enabling novice users to benefit from state-of-the-art models in the MIR community.

섹션 7.: ACKNOWLEDGEMENT
----------------------------------------
This work was supported in part by JSPS KAKENHI Grant Number JP21J20353, JST ACT-X Grant Number JPM-JAX200R, and JST CREST Grant Number JPMJCR20D4, Japan.

============================================================
총 섹션 수: 15
생성 시간: 2025-08-19 08:36:57
