There is a series of research on building interfaces to let users interact with music generation techniques effectively [47][48][49][50][51][52][53]. MySong [47] involves a music accompaniment generation model with controls over happiness or jazziness. Louie et al. [49] provide an interactive interface for novices to use a symbolic music generation technique with controls of happiness or randomness, and the interface allows constraining results with music priors, which experiments confirm are effective for iterative refinement. Zhou et al. [52,53] use a user-in-the-loop Bayesian optimization approach to enable novice users to iteratively explore melodies produced by a generative model. These interfaces underscore the importance of controls and iterative exploration in facilitating novice creativity with music generation techniques. Consequently, deploying recent text-to-audio models to novices would be well suited, offering more flexible control than a handful of parameters such as happiness and enabling the use of audio priors. Our paper contributes by examining design considerations for interfaces for text-to-audio music generation processes, aiming to broaden the applications of recent MIR techniques.