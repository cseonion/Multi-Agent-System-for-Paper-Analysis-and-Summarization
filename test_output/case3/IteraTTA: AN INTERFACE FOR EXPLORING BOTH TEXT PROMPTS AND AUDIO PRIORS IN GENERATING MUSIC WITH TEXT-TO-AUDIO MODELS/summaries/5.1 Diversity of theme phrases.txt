The section begins by noting that users’ theme phrases for initiating text-to-audio generation were highly diverse, with examples ranging from music-specific terms like “nice city pop” and “cute future bass” to more explicit phrases such as “80’s hip hop that break dancers would dance to,” and abstract ones like “Arabian caves” and “silent dream of a priestess,” a variety illustrated in a word cloud (Figure 3). To assess IteraTTA’s role, the authors compared the users’ theme phrases with text prompts generated by a large language model from those phrases and with the text labels in the training dataset. They randomly sampled 1,000 cases for each of the three categories—theme phrases, derived text prompts, and text labels—computed their representation vectors using the same pretrained RoBERTa model as the text-to-audio system, and visualized the distributions with t-SNE (Figure 4). The results suggest that IteraTTA helped the LLM produce prompts that bridge the gap between diverse user phrases and training labels. Notably, the LLM could derive musical descriptions even from abstract phrases, such as “otherworldly harmonies, delicate strings, minimalistic percussion, dreamlike vocals” for “silent dream of a priestess.” This demonstrates the effectiveness of guiding initial prompt construction to support novice users’ creative processes, as discussed in Section 3.1. The English version is available at iteratta.duckdns.org, and for text labels they extracted labels containing “music” from AudioCaps. Overall, the section shows that IteraTTA can align user-generated themes with dataset labels through LLM-derived prompts, including from abstract inputs.