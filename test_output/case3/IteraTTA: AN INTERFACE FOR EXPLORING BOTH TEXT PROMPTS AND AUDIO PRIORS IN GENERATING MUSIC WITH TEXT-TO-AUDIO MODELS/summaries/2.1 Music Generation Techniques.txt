Music generation remains a central topic in the MIR community, with generative machine learning increasingly used to create both symbolic MIDI outputs and raw audio for greater expressiveness; notable examples like Jukebox and RAVE employ variational autoencoders and autoregressive models trained on large music datasets to generate diverse audio. Controllability is emphasized as essential to support users’ creative processes, with approaches such as Music FaderNets that adjust rhythm and note density, Music SketchNet that specifies pitch contours and rhythm patterns, and methods by Wang et al. and Dai et al. that constrain chord progressions. Yet many users lack musical knowledge, so some perceptual control methods based on emotion (valence-arousal) provide limited, four-class taxonomy that may restrict user agency. Text-to-audio models offer an effective, novice-friendly alternative by learning the relationship between audio and text descriptions through latent representations encoded by RoBERTa, enabling flexible control via text prompts of variable length and content and allowing constraints by audio priors to keep outputs aligned with reference audio. In diffusion-based AudioLDM, seeds traditionally based on Gaussian noise can be replaced by a noise-infused audio prior to preserve the characteristics of the input. The parallel to text-to-image models—where iterative prompt exploration and prior constraints boost creativity—inspires applying similar ideas to text-to-audio to enable iterative exploration and customization. Nonetheless, text-to-audio generation poses specific challenges discussed earlier, motivating the development of an interface dedicated to text-to-audio models to address these difficulties.