The design section describes how IteraTTA begins with a user-entered theme phrase and uses a large language model to generate four variational lists of comma-separated phrases describing how a music clip of that theme might sound. These four lists serve as diverse first prompts to start music generation in parallel, enabling novices to translate loosely specified goals into musical descriptions and to envisage prompt variations. IteraTTA then creates three music audios for each of the four prompts (twelve in total), arranged in a two-dimensional layout (Figure 1) so users can compare how outputs differ across prompts and within the same prompt, aiding alignment with goals and indicating exploratory directions. If a suitable candidate prompt is found, users can customize it to generate new audios, or, if a suitable audio is found, use it as an audio prior to produce additional outputs; in short, users can explore the subspace near their goals by constraining prompts or priors while gradually refining their objectives. The section also introduces exploration-supporting features (Figure 2): when an audio prior is specified, users can compare generated results with it, and there is an instant text-prompt editing tool that can amplify or suppress a chosen instrument by adding phrases like “with strong [instrument]” or “with no [instrument]” to the prompt, illustrating how results can be steered through prompt adjustments.