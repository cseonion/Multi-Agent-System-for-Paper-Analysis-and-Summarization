We observed that volunteers struggled to efficiently explore generated results in text-to-audio tasks. A volunteer with experience in text-to-image models noted that, unlike T2I, comparing results at a glance in text-to-audio was difficult, making it hard to find a text prompt that faithfully reflects their intention. Consequently, iterating prompts alone does not necessarily help novices understand the space of possible results, even though refining loosely specified goals is still necessary [60,61]. As a result, users cannot readily determine which direction or which prompt to try next to approach their goals. Another volunteer illustrated the problem by describing how adding “with a flute” to a prompt altered the melody so drastically that the result became frustratingly different. This suggests a need to constrain generation using not only text prompts but also audio priors. In sum, to support novice creativity in text-to-audio music generation, there must be an ability to efficiently explore variations in both text prompts and audio priors, enabling iterative goal refinement by understanding the space of possible results, which requires an interface specifically designed for text-to-audio models to support dual-sided exploration of prompts and priors.