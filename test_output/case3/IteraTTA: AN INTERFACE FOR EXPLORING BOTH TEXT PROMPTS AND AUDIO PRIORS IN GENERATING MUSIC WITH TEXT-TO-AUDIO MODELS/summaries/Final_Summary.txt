**Final Summary of "IteraTTA: An Interface for Exploring Both Text Prompts and Audio Priors in Generating Music with Text-to-Audio Models"**

**1. Research Objective and Background**

The paper addresses the challenge of enabling novice users to creatively generate music using state-of-the-art text-to-audio models. While generative machine learning has made significant strides in music creation—producing both symbolic and raw audio outputs—controllability and accessibility remain key hurdles, especially for users lacking formal musical training. Existing methods often require users to specify musical parameters or rely on limited perceptual controls, which can restrict creative agency. Text-to-audio models, which map natural language descriptions to audio outputs, offer a promising, user-friendly alternative. However, these models present unique challenges: novices struggle to craft effective prompts and to iteratively refine their creative goals due to a gap between their intentions and the model’s expectations. The research aims to bridge this gap by designing an interface that supports both the construction of initial prompts and iterative exploration through text and audio priors, thereby democratizing access to advanced music generation tools.

**2. Key Methodology**

The authors adopted a human–computer interaction (HCI) approach, beginning with a think-aloud study involving three volunteers with no formal musical training. Participants interacted with a state-of-the-art text-to-audio model, verbalizing their thought processes as they attempted to generate music from text prompts. The study revealed two main challenges: difficulty in constructing effective initial prompts and inefficiency in exploring the space of possible outputs. These findings informed the design requirements for a new interface. The resulting system, IteraTTA, is a web-based platform that leverages a large language model (LLM) to generate multiple, musically descriptive prompts from a user-supplied theme phrase. It presents users with a grid of generated audio samples, allowing for side-by-side comparison and iterative refinement. Users can edit prompts, specify audio priors, and explore variations, with backend support from GPT-3.5 for prompt generation, AudioLDM for audio synthesis, and DeepL for translation. The system was deployed as a public web service, and user interactions were logged for analysis.

**3. Result and Conclusion**

IteraTTA was released as a publicly accessible web service, attracting 8,831 users who generated 246,423 music audios within two days. Analysis of user logs and feedback demonstrated that the interface successfully enabled both novice and experienced users to explore and refine their creative ideas. The LLM-generated prompts effectively bridged the gap between diverse, often abstract user themes and the model’s training labels, facilitating meaningful music generation even from non-musical or imaginative inputs. Users engaged in iterative exploration, alternating between text prompt modifications and audio prior constraints, with some users performing extensive refinement cycles. Feedback was overwhelmingly positive, highlighting the system’s value as a creative tool and idea generator. Users reported gaining insights into prompt engineering and model behavior through hands-on exploration, even without explicit instruction.

**4. Implications and Significance**

The study demonstrates that computational guidance in prompt construction and dual-sided iterative exploration (via both text and audio priors) are critical for supporting novice creativity in text-to-audio music generation. IteraTTA’s design principles—grounded in HCI and creativity support literature—generalize beyond the specific models used, offering a blueprint for future interfaces in the music information retrieval (MIR) community. By lowering the barrier to entry, the system democratizes access to advanced generative models, enabling a broader range of users to participate in music creation. The findings also suggest that user-driven exploration can yield new prompt engineering strategies and creative uses for text-to-audio models, potentially informing future model development and interface design.

**5. Limitations**

While IteraTTA proved effective in supporting novice users, several limitations are noted. The initial user study involved only three participants, which may limit the generalizability of the design requirements. The system’s reliance on English-language prompts and translation services may introduce biases or inaccuracies for non-English speakers. Audio generation was limited to 10-second clips due to computational constraints, which may not capture the full expressive potential of longer musical pieces. Finally, while user feedback was positive, the voluntary nature of responses and the short deployment window may not fully capture long-term user engagement or challenges.

**Conclusion**

IteraTTA represents a significant step toward accessible, creative music generation for novices using text-to-audio models. By combining computational prompt guidance with iterative, dual-sided exploration, the interface empowers users to translate abstract ideas into musical outputs and refine their creations through hands-on experimentation. The system’s public deployment and positive user reception underscore its potential as a foundation for future creativity-support tools in the MIR field.