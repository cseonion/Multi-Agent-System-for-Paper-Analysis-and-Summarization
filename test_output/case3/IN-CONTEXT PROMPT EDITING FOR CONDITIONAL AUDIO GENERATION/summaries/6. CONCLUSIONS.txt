This conclusion notes that distributional shift arises when text-to-audio models are conditioned on under-specified prompts, and it proposes editing prompts with demonstrative exemplars by using training captions as demonstrations for the LLMs to craft better edits. The authors report consistent audio-quality improvements as the edited prompts become more aligned with the training-caption distribution. The approach is simple, requires no model retraining, and can be easily integrated into any text-based audio pipeline.