The section formalizes two prompt distributions, Pt for training prompts and Pu for user prompts, and outlines a feature extraction pipeline to compare them. For a user prompt x_u drawn from Du, a corresponding prompt x_t is retrieved from Pt (or Pt ∪ P_LM), and conditioned on x_t, the latent representation z is sampled as z ∼ f(x_t) from a trained text encoder f, where f can be any pretrained model such as RoBERTa, T5, or CLAP, making the approach encoder-agnostic. To address distributional shift, the authors propose measuring KL reduction between the encoded feature distributions, rather than the text-level divergence, by comparing P(X) and Q(X) where each input x yields normalized scores in [0,1], and the KL value is averaged across feature channels. The prompt divergence r_div is computed from p_train (the original prompt feature distribution), q_user (the converted prompt feature distribution), and p′_new (the empirical distribution of the sampled x_t), with Z denoting the set of extracted text features from f. The core aim is to relate the KL reduction induced by prompt editing to audio quality, quantified by FAD scores, by assessing how much the chosen dataset reduces KL divergence to the target distribution compared with random sampling; the authors note a strong correlation between KL reduction and FAD reduction. Retrieval relies on samples from the training prompt distribution, while the user specifies inputs from Pu, and the section emphasizes the empirical link between prompt-domain KL shifts and downstream audio quality.