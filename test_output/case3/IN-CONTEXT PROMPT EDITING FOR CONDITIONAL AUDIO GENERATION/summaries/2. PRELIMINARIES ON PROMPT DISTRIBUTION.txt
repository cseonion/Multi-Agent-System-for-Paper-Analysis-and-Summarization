This section formalizes modeling two text prompt distributions by projecting them into a feature space for metric computation. The training prompt distribution is Pt and the user prompt distribution is Pu; the process consists of (1) from a user prompt x_u sampled from Du, retrieving a training prompt x_t drawn from Pt or the joint Pt ∪ P_LM (to be elaborated in Section 3), and (2) conditioning on x_t to sample a latent representation z ∼ f(x_t) using a trained text encoder f(•). The encoder is generic and can be RoBERTa, T5, CLAP, or other pretrained models, making the approach broadly applicable. They address distributional shift in prompts by measuring KL reduction, which reflects the relative divergence when user prompts are fed into the encoder versus the training prompts. Specifically, KL(P || Q) is defined between encoded feature distributions P(X) and Q(X), with each input x consisting of normalized scores in [0, 1], the KL value averaged across feature channels, and the prompt-divergence score r_div derived from p_train (the original prompt feature distribution), q_user (the converted prompt feature distribution), and p′_new (the empirical feature distribution of the sampled prompt x_t). Z denotes the set of extracted text features (or latent codes z) from f(•). The authors then relate the KL reduction produced by a given prompt-editing approach to audio quality via FAD scores, defined as how much the selected dataset reduces KL divergence to the target distribution under random sampling. Retrieval uses data samples from the training prompt distribution for x_t while the user supplies x_u, and they report a strong correlation between KL reduction and FAD reduction.