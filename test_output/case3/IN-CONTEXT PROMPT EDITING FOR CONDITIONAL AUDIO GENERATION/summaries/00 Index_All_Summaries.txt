논문 제목: IN-CONTEXT PROMPT EDITING FOR CONDITIONAL AUDIO GENERATION
전체 섹션별 요약 인덱스
============================================================

섹션 1.: INTRODUCTION
----------------------------------------
I didn’t receive any text yet. Please paste the raw text of Section 1 (and, if you have it, the summary of the previous sections) so I can produce a concise, paragraph-form summary that preserves all numerical details. I will summarize section by section, no bullet points, maintaining all key numbers and quantitative characteristics. If you have multiple sections, you can send Section 1 first and I’ll respond with its summary, then you can provide Section 2 with the prior section’s summary for continuity.

섹션 2.: PRELIMINARIES ON PROMPT DISTRIBUTION
----------------------------------------
This section formalizes modeling two text prompt distributions by projecting them into a feature space for metric computation. The training prompt distribution is Pt and the user prompt distribution is Pu; the process consists of (1) from a user prompt x_u sampled from Du, retrieving a training prompt x_t drawn from Pt or the joint Pt ∪ P_LM (to be elaborated in Section 3), and (2) conditioning on x_t to sample a latent representation z ∼ f(x_t) using a trained text encoder f(•). The encoder is generic and can be RoBERTa, T5, CLAP, or other pretrained models, making the approach broadly applicable. They address distributional shift in prompts by measuring KL reduction, which reflects the relative divergence when user prompts are fed into the encoder versus the training prompts. Specifically, KL(P || Q) is defined between encoded feature distributions P(X) and Q(X), with each input x consisting of normalized scores in [0, 1], the KL value averaged across feature channels, and the prompt-divergence score r_div derived from p_train (the original prompt feature distribution), q_user (the converted prompt feature distribution), and p′_new (the empirical feature distribution of the sampled prompt x_t). Z denotes the set of extracted text features (or latent codes z) from f(•). The authors then relate the KL reduction produced by a given prompt-editing approach to audio quality via FAD scores, defined as how much the selected dataset reduces KL divergence to the target distribution under random sampling. Retrieval uses data samples from the training prompt distribution for x_t while the user supplies x_u, and they report a strong correlation between KL reduction and FAD reduction.

섹션 3.: IN-CONTEXT PROMPT EDITING
----------------------------------------
Inspired by in-context learning, this section formulates prompt editing as transforming a user query x_u into an in-context prompt by appending a task instruction I and an in-context demonstration set C = {c1, ..., ck}, where each c_i is a caption retrieved from the training prompt set D_t. Let Y = {y1, ..., ym} be the set of refined audio samples; the edited prompt is x = [I, C, x_u]. The likelihood that an audio y_j is representative is approximated by a scoring function f parameterized by θ applied to the full input sequence, and rather than conditioning on an unseen user prompt, the audio signal is drawn from a surrogate distribution: ŷ = arg max_{y_j ∈ Y} P(y_j | x; θ). To mitigate distributional shift between training and real distributions, the framework edits user prompts with demonstrative exemplars drawn from the training prompt distribution, consisting of two main steps: first, de-duplication to improve retrieval efficiency since D_t can be prohibitively large.

섹션 2.: Retrieval of demonstrative exemplars for language model inference.
----------------------------------------
The section formalizes two prompt distributions, Pt for training prompts and Pu for user prompts, and outlines a feature extraction pipeline to compare them. For a user prompt x_u drawn from Du, a corresponding prompt x_t is retrieved from Pt (or Pt ∪ P_LM), and conditioned on x_t, the latent representation z is sampled as z ∼ f(x_t) from a trained text encoder f, where f can be any pretrained model such as RoBERTa, T5, or CLAP, making the approach encoder-agnostic. To address distributional shift, the authors propose measuring KL reduction between the encoded feature distributions, rather than the text-level divergence, by comparing P(X) and Q(X) where each input x yields normalized scores in [0,1], and the KL value is averaged across feature channels. The prompt divergence r_div is computed from p_train (the original prompt feature distribution), q_user (the converted prompt feature distribution), and p′_new (the empirical distribution of the sampled x_t), with Z denoting the set of extracted text features from f. The core aim is to relate the KL reduction induced by prompt editing to audio quality, quantified by FAD scores, by assessing how much the chosen dataset reduces KL divergence to the target distribution compared with random sampling; the authors note a strong correlation between KL reduction and FAD reduction. Retrieval relies on samples from the training prompt distribution, while the user specifies inputs from Pu, and the section emphasizes the empirical link between prompt-domain KL shifts and downstream audio quality.

섹션 3.1.: De-duplication
----------------------------------------
Retrieving prompts from large-scale datasets can be computationally intensive, especially when multiple exemplars are sought. To address this, the section adopts de-duplication to remove duplicate or near-duplicate items from the sample pool. It uses MinHash to identify demonstrative exemplars within the training data, representing each document xi (and xj) by sets of its n-grams di (and dj). The similarity between documents is measured with the Jaccard Index to quantify overlap between these n-gram sets, and highly matched documents with a Jaccard similarity greater than 0.8 are discarded.

섹션 3.2.: Retrieval of demonstrative exemplars
----------------------------------------
The section describes a retrieval pipeline that uses K-means clustering implemented with the Faiss library for scalable similarity search. Training prompts are projected into SBERT embeddings, and AudioCAPS and BBC Sounds prompts are used as exemplars for clustering. For each user prompt x_u, the system performs a similarity search against the indexed clusters to retrieve the top-M nearest training prompts x1, x2, ..., xM based on embedding-space distances. Using these neighboring exemplars, the in-context demonstration set C is built, with each x_i mapped to a corresponding c_i. The neighbors are ordered in nondecreasing distance so that d(c1) ≤ d(ci) for i > 1, creating a natural hierarchy of sentences within the cluster by contextual relevance to the user query. Consequently, the top-M candidate prompts are selected as illustrative examples, and the top candidate is structured as shown in Figure 2.

섹션 4.: EXPERIMENTAL SETTINGS
----------------------------------------
The experimental settings use AudioLDM to generate realistic speech and piano music samples, with LLaMa-70B as the prompt-editing model (a decoder-only language model). The study collects 1525 free-form user prompts (Open-prompts) as real-world test prompts and evaluates on AudioCAPS to assess potential performance degradation when using more elaborate, expert-annotated prompts. No training is performed except for instruction-tuning of the large language models. Audio samples are evaluated with CLAP for automatic text–audio alignment and with FAD to measure distance to clean audio. Human evaluation comprises subjective (SUB) and objective (OBJ) assessments of audio quality by five participants, with both SUB and OBJ rated on a 5-point scale; SUB focuses on audio quality, while OBJ assesses relevance to the edited prompts, with scores averaged across participants.

섹션 5.: RESULTS AND ANALYSIS
----------------------------------------
Section 5 shows that retrieval-based exemplars outperform the original user-prompt baseline (User) in both automatic metrics and human evaluation (Table 1), with consistent improvements and a reduced r div when guided by demonstrations. To verify that gains are not solely from LLM prompt editing, exemplar (K=100, closest) is contrasted with LLM, yielding a text–audio alignment gain of +0.011 (CLAP) and a further FAD improvement of +2.125. The study also tests the hypothesis that the most similar exemplars are best by comparing exemplar (K=100, farthest) and exemplar (K=100, random), with closest exemplars described as more distinct and displaying the highest token-type ratio (Table 2). Overall, exemplar-based editing achieves higher agreement than other editing techniques (Table 2). The greatest audio improvement arises from LLM editing with in-context learning, which even outperforms pure LLM editing by up to +0.23 in subjective human evaluation.

섹션 5.1.: Correlation of prompt divergence with audio quality
----------------------------------------
This section examines how exemplars influence audio quality through prompt divergence, using Table 1 and Figure 3. It finds that the average KL reduction scales linearly with quality, as measured by the normalized FAD reduction N(∆FAD), indicating that retrieved prompts serve effectively as exemplars for LLM prompt editing. Additionally, the comparison of in-domain versus out-domain prompts shows that in-domain prompts as demonstrative exemplars yield higher audio quality, with domain relevance enhancing editing performance. Table 1 reports a +0.37 improvement in FAD and corresponding increases in both human evaluation metrics.

섹션 5.2.: On the inference efficiency
----------------------------------------
Although in-context retrieval improves audio quality, inference-time efficiency remains a concern. On an Intel Xeon CPU with FAISS, the average search time for K = 100 candidates is 2. samples. Several factors influence speed: the number of clusters, where more clusters can improve performance but the gains are modest since LLM editing is also crucial; the number of retrieved candidates, since the similarity to the user query must be recomputed for each candidate, causing linear scaling of the pool size with speed; and the embedding dimension, fixed at 384 due to S-BERT.

섹션 6.: CONCLUSIONS
----------------------------------------
This conclusion notes that distributional shift arises when text-to-audio models are conditioned on under-specified prompts, and it proposes editing prompts with demonstrative exemplars by using training captions as demonstrations for the LLMs to craft better edits. The authors report consistent audio-quality improvements as the edited prompts become more aligned with the training-caption distribution. The approach is simple, requires no model retraining, and can be easily integrated into any text-based audio pipeline.

============================================================
총 섹션 수: 11
생성 시간: 2025-08-19 08:36:43
