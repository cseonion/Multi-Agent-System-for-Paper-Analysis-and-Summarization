Inspired by in-context learning, this section formulates prompt editing as transforming a user query x_u into an in-context prompt by appending a task instruction I and an in-context demonstration set C = {c1, ..., ck}, where each c_i is a caption retrieved from the training prompt set D_t. Let Y = {y1, ..., ym} be the set of refined audio samples; the edited prompt is x = [I, C, x_u]. The likelihood that an audio y_j is representative is approximated by a scoring function f parameterized by θ applied to the full input sequence, and rather than conditioning on an unseen user prompt, the audio signal is drawn from a surrogate distribution: ŷ = arg max_{y_j ∈ Y} P(y_j | x; θ). To mitigate distributional shift between training and real distributions, the framework edits user prompts with demonstrative exemplars drawn from the training prompt distribution, consisting of two main steps: first, de-duplication to improve retrieval efficiency since D_t can be prohibitively large.