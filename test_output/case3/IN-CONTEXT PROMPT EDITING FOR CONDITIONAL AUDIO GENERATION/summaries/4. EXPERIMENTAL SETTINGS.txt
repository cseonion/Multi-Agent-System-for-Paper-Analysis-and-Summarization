The experimental settings use AudioLDM to generate realistic speech and piano music samples, with LLaMa-70B as the prompt-editing model (a decoder-only language model). The study collects 1525 free-form user prompts (Open-prompts) as real-world test prompts and evaluates on AudioCAPS to assess potential performance degradation when using more elaborate, expert-annotated prompts. No training is performed except for instruction-tuning of the large language models. Audio samples are evaluated with CLAP for automatic textâ€“audio alignment and with FAD to measure distance to clean audio. Human evaluation comprises subjective (SUB) and objective (OBJ) assessments of audio quality by five participants, with both SUB and OBJ rated on a 5-point scale; SUB focuses on audio quality, while OBJ assesses relevance to the edited prompts, with scores averaged across participants.