RoBERTa consolidates the improvements explored earlier into a single configuration named RoBERTa for Robustly optimized BERT approach. It combines dynamic masking (Section 4.1), FULL-SENTENCES training without NSP loss (Section 4.2), large mini-batches (Section 4.3), and a larger byte-level BPE (Section 4.4), and it also investigates two factors often overlooked: the amount and diversity of pretraining data and the number of training passes through the data. To contextualize the gains, the section contrasts with XLNet, which trains on roughly ten times more data and eight times larger batches for fewer optimization steps, yielding roughly four times as many pretraining sequences as BERT. The authors first train RoBERTa in the BERT LARGE style (L = 24, H = 1024, A = 16, 355M parameters) for 100K steps on a comparable BOOK-CORPUS plus Wikipedia dataset, using 1024 V100 GPUs for about one day, and report results in Table 4, noting that RoBERTa substantially improves over the original BERT LARGE when data is controlled. Next, they combine this data with three additional datasets from Section 3.2 and pretrain RoBERTa over the same 100K-step schedule on a total of 160GB of text, observing further improvements across downstream tasks and underscoring the importance of data size and diversity. Finally, they extend pretraining to 300K and then 500K steps, obtaining significant gains and finding that the 300K and 500K-step models outperform XLNet LARGE on most tasks, with no overfitting observed even at the longest training, suggesting potential benefits from even more training. The remaining analysis evaluates the best RoBERTa model on GLUE, SQuaD, and RACE, while noting that the experiments conflate increases in data size and diversity and reserving a more careful analysis for future work. They conclude with RoBERTa being trained for 500K steps over all five datasets introduced in Section 3.2.