In the GLUE results, RoBERTa is evaluated under two finetuning settings. In the first setting, the single-task, dev setup, RoBERTa is finetuned separately for each GLUE task using only the task’s training data, with a limited hyperparameter sweep over batch sizes {16, 32} and learning rates {1e-5, 2e-5, 3e-5}, a linear warmup for the first 6% of steps followed by a linear decay to 0, and finetuning for 10 epochs with early stopping based on the task’s dev metric; all other hyperparameters mirror pretraining, and results are reported as medians over five random initializations without ensembling. In the second setting, ensembles, test, RoBERTa is submitted to the GLUE leaderboard based on single-task finetuning; for RTE, STS, and MRPC they fine-tune from the MNLI single-task model rather than the baseline RoBERTa, and they explore a wider hyperparameter space with ensembles of 5 to 7 models per task. Task-specific modifications include QNLI, which adopts a pairwise ranking approach for the test submission while providing dev results for a pure classification baseline for comparability; and WNLI, reformatted via Super-GLUE, trained with a margin ranking loss using spaCy-extracted noun phrases so that positive referents score higher, though this uses only positive training examples. Table 5 shows that in the single-task setting RoBERTa achieves state-of-the-art results on all nine GLUE development sets, outperforming both BERT LARGE and XLNet LARGE despite using the same masked language modeling objective and architecture, underscoring the influence of data size and training time. In the ensemble setting, RoBERTa attains state-of-the-art on four of nine tasks and the highest average score to date, notably without multi-task finetuning, suggesting future gains from more sophisticated multitask fine-tuning.