Summary of Section 5.1: GLUE Results

- Finetuning settings
  - Single-task, dev setting: RoBERTa is finetuned separately for each GLUE task using only the task’s training data. Hyperparameters are lightly swept over batch sizes {16, 32} and learning rates {1e-5, 2e-5, 3e-5}, with a linear warmup for the first 6% of steps followed by linear decay to 0. Training runs for 10 epochs with early stopping based on the task’s dev metric. All other pretraining-era hyperparameters are kept as in pretraining. Results reported are the median dev results over five random initializations, with no model ensembling.
  - Ensembles, test setting: RoBERTa is compared to other approaches on the test set via the GLUE leaderboard. Unlike many top submissions, this uses single-task finetuning rather than multitask finetuning. For RTE, STS, and MRPC, improvements come from starting from the MNLI single-task model rather than the baseline RoBERTa. A broader hyperparameter search is used (details in the Appendix), and ensembles between 5 and 7 models per task are formed.

- Task-specific modifications
  - QNLI: Recent GLUE leaderboard approaches use a pairwise ranking formulation with mined candidate answers; this test submission adopts the ranking approach, but dev-set results for direct comparison with BERT are reported using a pure classification setup.
  - WNLI: The authors found the provided NLI-format data challenging, so they use reformatted WNLI data from Super-GLUE, which indicates the span of the query pronoun and referent. Finetuning uses a margin ranking loss. For a given input, spaCy is used to extract candidate noun phrases; the model is trained to score positive referents higher than generated negatives. A consequence is that only positive training examples are used, effectively discarding over half of the provided training data. The text cuts off, but the approach is described up to this point.

- Results
  - Single-task, dev results: RoBERTa achieves state-of-the-art results on all 9 GLUE development sets. Despite using the same masked language modeling objective and architecture as BERT LARGE, RoBERTa consistently outperforms BERT LARGE and XLNet LARGE, highlighting the impact of training dynamics and data rather than architecture alone.
  - Ensembles, test results: RoBERTa achieves state-of-the-art results on 4 out of 9 tasks and the highest average score to date on the GLUE test leaderboard. This is notable because the submission does not rely on multitask finetuning, unlike many other top entries; future work may further improve results by incorporating more sophisticated multitask fine-tuning.

- Takeaways
  - The GLUE results illustrate that adjustments to pretraining data, training length, and broader data diversity can yield substantial gains, sometimes surpassing improvements from architectural changes alone. The single-task dev setting shows strong dev-set performance, while the ensembles test demonstrates competitive top-end results on the official leaderboard without multitask finetuning.