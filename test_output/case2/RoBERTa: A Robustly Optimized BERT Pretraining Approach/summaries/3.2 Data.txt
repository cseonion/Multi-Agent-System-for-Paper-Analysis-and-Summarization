Summary of Section 3.2. Data

- Purpose and approach
  - Large-scale text data is crucial for BERT-style pretraining, with prior work showing that bigger datasets can improve end-task performance.
  - The study aims to gather as much data as possible to match the data quality/quantity needed for fair comparisons, noting that not all additional datasets can be publicly released.
  - The authors focus on English-language corpora and compile a diverse set of sources totaling over 160 GB of uncompressed text.

- Data sources and sizes
  - BOOKCORPUS + English Wikipedia: 16 GB (the original data used to train BERT).
  - CC-NEWS: 63 million English news articles from the English portion of CommonCrawl News, crawled between Sept 2016 and Feb 2019; 76 GB after filtering.
  - OPENWEBTEXT: 38 GB; an open-source recreation of the WebText corpus, consisting of web content from URLs shared on Reddit with at least three upvotes.
  - STORIES: 31 GB; introduced by Trinh and Le, a subset of CommonCrawl filtered to resemble story-like text (Winograd-schema style).

- Note on scope
  - The section states five English-language corpora, but the enumerated list includes four items. This appears to be a discrepancy between the stated count and the listed corpora.

- Overall takeaway
  - The section enumerates large, diverse English datasets drawn from web and news sources to enable robust pretraining, prioritizing data quantity and public availability where possible.