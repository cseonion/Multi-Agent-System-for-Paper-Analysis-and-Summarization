Section 3.2 emphasizes that BERT-style pretraining requires large-scale text and that more data can improve downstream performance, noting that many large datasets are not publicly released. To maximize experimentation, the authors focus on gathering as much data as possible to match the overall quality and quantity needed for fair comparisons. They consider five English-language corpora totaling over 160 GB of uncompressed text, namely BOOKCORPUS plus English Wikipedia (16 GB; the original data used to train BERT), CC-NEWS from the English portion of CommonCrawl News (63 million English articles, 76 GB after filtering, collected between September 2016 and February 2019), OPENWEBTEXT (38 GB), an open-source recreation of WebText consisting of web content from Reddit links with at least three upvotes, and STORIES (31 GB), a subset of CommonCrawl filtered to resemble story-like Winograd schemas; together these amount to approximately 161 GB.