Optimization is performed with Adam (Kingma and Ba, 2015) using β1 = 0.9, β2 = 0.999, ε = 1e-6 and an L2 weight decay of 0.01. The learning rate warms up over the first 10,000 steps to a peak of 1e-4 and then decays linearly. Training uses dropout of 0.1 on all layers and attention weights, and a GELU activation function. Pretraining runs for S = 1,000,000 updates, with minibatches of B = 256 sequences of maximum length T = 512 tokens.