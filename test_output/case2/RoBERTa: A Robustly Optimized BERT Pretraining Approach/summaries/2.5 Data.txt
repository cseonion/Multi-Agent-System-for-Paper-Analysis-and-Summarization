BERT is trained on a combination of BookCorpus (Zhu et al., 2015) and English Wikipedia, totaling 16GB of uncompressed text.