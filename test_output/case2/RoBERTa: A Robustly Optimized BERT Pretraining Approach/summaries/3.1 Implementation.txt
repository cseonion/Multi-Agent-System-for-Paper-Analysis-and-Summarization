The authors reimplement BERT in FAIRSEQ and largely adhere to the original BERT optimization hyperparameters from Section 2, except that the peak learning rate and the number of warmup steps are tuned separately for each setting. They find that the Adam epsilon term is highly sensitive, and tuning it can yield better performance or stability, and that setting beta2 to 0.98 improves stability when training with large batches. Pretraining uses sequences of up to T = 512 tokens (i.e., 512-token maximum). Unlike Devlin et al. (2019), they do not inject short sequences randomly and do not train with a reduced sequence length for the first 90% of updates; training is conducted only with full-length sequences. Training runs use mixed-precision arithmetic on DGX-1 machines, each equipped with 8 Ã— 32 GB Nvidia V100 GPUs connected by Infiniband (Micikevicius et al., 2018).