Section 4.1 investigates static versus dynamic masking. The original BERT used a single static mask created during preprocessing; to avoid reusing the same mask across epochs, the authors duplicated the training data 10 times so each sequence is masked in 10 different ways over 40 training epochs, meaning each sequence is seen with the same mask four times. They compare this with dynamic masking, where the masking pattern is regenerated each time a sequence is fed to the model, a strategy that becomes important for longer pretraining. Table 1 reports SQuAD 2.0 F1, MNLI-m and SST-2 accuracy, with reference results from Devlin et al. (2019) of 76.3, 84.3 and 92.8, respectively. The reimplementation with static masking achieves 78.3 (SQuAD), 84.3 (MNLI-m), and 92.5 (SST-2), while dynamic masking achieves 78.7, 84.0, and 92.9, respectively; results are medians over five seeds. The authors find that static masking reproduces the original BERT results closely, and dynamic masking is comparable or slightly better. Given the efficiency benefits of dynamic masking, they adopt it for the remaining experiments.