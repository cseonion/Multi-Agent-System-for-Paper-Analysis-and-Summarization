- Overview: Section 3.3 describes how the pretrained models are evaluated on downstream tasks, using three benchmarks: GLUE, SQuAD, and RACE.

- GLUE
  - GLUE is a suite of 9 datasets for natural language understanding; 6 of the tasks are framed as either single-sentence or sentence-pair classification.
  - The GLUE setup provides training and development splits plus a submission server/leaderboard for private test data.
  - Replication study (Section 4): results are reported on development sets after finetuning on single-task training data only (no multi-task training or ensembling). The finetuning procedure follows the original BERT paper.
  - Section 5: test set results on the public leaderboard are reported, with several task-specific modifications described in Section 5.1.

- SQuAD
  - SQuAD consists of a context paragraph and a question; the task is to extract the answer span from the context. Two versions used: SQuAD V1.1 and V2.0.
  - V1.1: context always contains an answer; use the standard span-prediction method (as in BERT).
  - V2.0: some questions are unanswerable. An additional binary classifier is added to predict answerability; this classifier is trained jointly with the span-prediction objective (sum of classification and span losses). Evaluation predicts spans only for questions deemed answerable.

- RACE
  - ReAding Comprehension from Examinations (RACE) is a large-scale multiple-choice reading comprehension dataset from English exams in China.
  - Contains over 28,000 passages and about 100,000 questions; for each question, four options are provided.
  - Passages are relatively long, and a substantial portion of questions require reasoning.