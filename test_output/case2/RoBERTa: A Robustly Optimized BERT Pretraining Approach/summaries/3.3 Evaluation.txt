This evaluation section assesses pretrained models on three downstream benchmarks: GLUE, SQuAD, and RACE. GLUE aggregates nine datasets for natural language understanding, with six tasks framed as either single-sentence or sentence-pair classifications; GLUE provides train and development splits plus a submission server and leaderboard for private held-out test data. For the replication study in Section 4, results are reported on development sets after finetuning the pretrained models on the corresponding single-task training data, without multi-task training or ensembling, following the finetuning procedure of the original BERT paper (Devlin et al., 2019). In Section 5, test-set results obtained from the public leaderboard are also reported, with several task-specific modifications described in Section 5.1. SQuAD comprises a paragraph of context and a question; there are two versions: V1.1 and V2.0, with V2.0 introducing unanswerable questions. For SQuAD V1.1, the same span-prediction method as BERT is used; for V2.0, an additional binary classifier predicts answerability and is trained jointly by summing the classification and span losses, with evaluation only on spans for questions classified as answerable. RACE is a large-scale reading-comprehension dataset from English examinations in China, containing more than 28,000 passages and nearly 100,000 questions; each question has four options, and passages have significantly longer context with substantial reasoning requirements.