During pretraining, BERT uses two objectives: masked language modeling (MLM) and next sentence prediction (NSP). For MLM, a random subset of input tokens is selected and replaced with the [MASK] token, with the objective being a cross-entropy loss over the predicted tokens. Specifically, 15% of input tokens are chosen for possible replacement; of these, 80% are replaced with [MASK], 10% are left unchanged, and 10% are replaced by a randomly selected vocabulary token. In the original implementation, masking and replacement are decided once at the beginning and saved for the training duration, though in practice data are duplicated so the mask is not fixed for every training sentence (see Section 4.1). The NSP objective is designed to improve performance on downstream tasks requiring reasoning about relationships between pairs of sentences, such as natural language inference.