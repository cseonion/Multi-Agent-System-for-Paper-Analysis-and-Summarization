- BERT pretraining uses two objectives: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).

- MLM details:
  - 15% of input tokens are selected for possible replacement.
  - Of these selected tokens: 80% are replaced with the special token [MASK], 10% are left unchanged, and 10% are replaced by a randomly chosen token from the vocabulary.
  - The MLM objective is a cross-entropy loss over predicting the masked tokens.

- Masking scheme notes:
  - In the original implementation, masking and replacement are performed once at the beginning and kept fixed for the duration of training.
  - In practice, data duplication is used to vary the mask across training passes (the mask is not always the same for every training sentence; see Section 4.1).

- NSP details:
  - NSP is designed to improve performance on downstream tasks that require reasoning about relationships between sentence pairs (e.g., Natural Language Inference).