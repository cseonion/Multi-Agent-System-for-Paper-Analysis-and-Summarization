Byte-Pair Encoding (BPE) is used to manage large vocabularies by forming subword units that bridge character- and word-level representations. BPE vocabularies typically span 10Kâ€“100K, but Radford et al. (2019) show that using bytes as the base subword units yields a 50K-subword vocabulary that can encode any input without unknown tokens, adding about 15M parameters to BERT-Base and 20M to BERT-Large. The original BERT used a 30K character-level BPE after heuristic tokenization; here a universal byte-level BPE is adopted without extra preprocessing, with only minor differences in end-task performance in early experiments, though the universal encoding is favored and a detailed comparison is deferred to future work. Large-batch training can improve efficiency even without extensive parallel hardware via gradient accumulation, a capability supported natively in FAIRSEQ (Ott et al., 2019).