In RACE, where a passage, a question, and four candidate answers are provided, the model must select the correct option. The RoBERTa adaptation concatenates each candidate answer with the corresponding question and passage, encodes each of the four resulting sequences, and uses the [CLS] representations passed through a single fully-connected layer to predict the correct answer. They truncate any questionâ€“answer pair longer than 128 tokens, and, if needed, shorten the passage so that the total length does not exceed 512 tokens. Results on the RACE test sets (Table 7) show that RoBERTa achieves state-of-the-art performance on both middle-school and high-school settings.