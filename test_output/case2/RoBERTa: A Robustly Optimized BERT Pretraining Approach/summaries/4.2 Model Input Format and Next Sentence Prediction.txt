Section 4.2 examines how the pretraining input format and the Next Sentence Prediction (NSP) objective affect performance. The original BERT uses a SEGMENT-PAIR input with NSP, where the two segments can come from the same document contiguously (p = 0.5) or from different documents. Four training formats are compared: SEGMENT-PAIR+NSP; SENTENCE-PAIR+NSP, which uses pairs of natural sentences (shorter than 512 tokens; the batch is enlarged to keep the total token count similar to SEGMENT-PAIR+NSP); FULL-SENTENCES, where inputs are packed with full sentences from one or more documents (total length â‰¤ 512, can cross document boundaries) and NSP is removed; and DOC-SENTENCES, similar to FULL-SENTENCES but restricted to a single document (may not cross boundaries), with dynamically increased batch sizes when sequences near the end of a document are shorter to match the total tokens of FULL-SENTENCES, and NSP removed. Table 2 shows that SEGMENT-PAIR and SENTENCE-PAIR differ in performance, with the latter harming downstream tasks, likely due to reduced long-range dependency learning. Training without NSP and using DOC-SENTENCES outperforms the original BERT BASE and, in some cases, matches or slightly improves downstream tasks, suggesting the original results may have effectively removed NSP while retaining the SEGMENT-PAIR input. Among single-document formats, DOC-SENTENCES is marginally better than FULL-SENTENCES; however, DOC-SENTENCES yields variable batch sizes, so for easier cross-study comparison the authors adopt FULL-SENTENCES in subsequent experiments.