Pretraining methods have pursued a range of objectives, including language modeling, machine translation, and masked language modeling, with many works following the standard recipe of finetuning models for each task and pretraining with a masked LM objective. Yet progress has also come from multi-task fine-tuning, incorporating entity embeddings, span prediction, and autoregressive pretraining variants, with performance typically improving as models grow larger and are trained on more data. The authors’ aim is to replicate, simplify, and better tune BERT’s training to understand the relative performance of these methods. They systematically evaluate design decisions in pretraining BERT and find that substantial gains come from longer training with bigger batches over more data, removing the next sentence prediction objective, training on longer sequences, and dynamically varying the masking pattern. Their RoBERTa pretraining procedure achieves state-of-the-art results on GLUE, RACE, and SQuAD without multi-task fine-tuning for GLUE or additional data for SQuAD, illustrating the importance of these design choices and suggesting that BERT’s pretraining objective remains competitive with newer alternatives.