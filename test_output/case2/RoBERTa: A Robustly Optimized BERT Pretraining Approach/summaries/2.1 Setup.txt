BERT's Setup section states that input is a single sequence formed by concatenating two token segments, x1…xN and y1…yM, which typically contain multiple sentences. These segments are separated by special tokens, and their combined length must satisfy M + N < T, where T is the maximum sequence length used during training. The model is first pretrained on a large unlabeled text corpus and subsequently finetuned on labeled data for the end task.