Summary of Section 5.2: SQuAD Results

- Finetuning setup:
  - SQuAD v1.1: Finetuning follows the same procedure as Devlin et al. (2019).
  - SQuAD v2.0: Adds an answerability classifier trained jointly with the span predictor by summing the classification and span losses. The model uses the same learning rate for all layers.

- Results:
  - SQuAD v1.1 development set: RoBERTa matches the state-of-the-art achieved by XLNet.
  - SQuAD v2.0 development set: RoBERTa achieves a new state-of-the-art, improving over XLNet by 0.4 EM and 0.6 F1.

- Public leaderboard submission:
  - The RoBERTa submission does not use any additional data (no external training data). Most top systems rely on BERT or XLNet with extra data.
  - A single RoBERTa model outperforms all but one of the single-model submissions and is the top-performing model among those not using data augmentation.

- Takeaways:
  - Training dynamics and data diversity (rather than architectural changes) can drive substantial gains.
  - RoBERTa achieves strong SQuAD performance with a simpler, data-efficient finetuning approach.