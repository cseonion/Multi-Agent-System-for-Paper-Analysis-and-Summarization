We adopt a simpler approach for SQuAD than in prior work, using the same learning rate for all layers as a shared setting. For SQuAD v1.1 we follow the finetuning procedure of Devlin et al. (2019); for SQuAD v2.0 we additionally classify whether a question is answerable and train this classifier jointly with the span predictor by summing their loss terms. Results are reported in Table 6: on SQuAD v1.1 development, RoBERTa matches the state-of-the-art set by XLNet, and on SQuAD v2.0 development, RoBERTa sets a new state-of-the-art, improving XLNet by 0.4 EM and 0.6 F1. We also submitted RoBERTa to the public SQuAD 2.0 leaderboard, comparing to other systems; most top systems build on either BERT or XLNet, both of which rely on additional external training data, whereas our submission uses no extra data. Our single RoBERTa model outperforms all but one of the single-model submissions and is the top scoring system among those that do not rely on data augmentation.