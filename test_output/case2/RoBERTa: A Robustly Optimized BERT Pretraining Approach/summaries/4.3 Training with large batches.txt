Training with very large mini-batches, a strategy well established in neural machine translation, is shown to accelerate optimization and improve end-task performance for BERT when the learning rate is scaled appropriately. Devlin et al. originally trained BERT BASE for 1M steps with a batch size of 256 sequences, which is computationally equivalent (via gradient accumulation) to 125K steps with a 2K batch or 31K steps with an 8K batch. In Table 3, base models trained on BOOKCORPUS and Wikipedia with batch sizes of 256, 2K, and 8K sequences—corresponding to 1M, 125K, and 31K steps and learning rates of 1e-4, 7e-4, and 1e-3, respectively—achieve perplexities of 3.99, 3.68, and 3.77 and development accuracies of 84.7/92.7, 85.2/92.9, and 84.6/92.8 on MNLI-m and SST-2, with the same number of passes and overall computational cost. The results indicate that larger batches improve both masked language modeling perplexity and end-task accuracy, and larger batches ease parallelization via distributed data parallel training, with experiments extending to 8K-sequence batches; You et al. push even larger up to 32K. The authors note that exploring the ultimate limits of large-batch training is left for future work.