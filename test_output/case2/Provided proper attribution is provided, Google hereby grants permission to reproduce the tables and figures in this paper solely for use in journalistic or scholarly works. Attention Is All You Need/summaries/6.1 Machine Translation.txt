On the WMT 2014 English-to-German task, the Transformer big model outperforms all previously reported models, including ensembles, by more than 2.0 BLEU and achieves a new state-of-the-art BLEU of 28.4; its configuration is listed in the bottom line of Table 3, and training took 3.5 days on 8 P100 GPUs. Even the base model surpasses all earlier published models and ensembles at a fraction of their training cost. For the WMT 2014 English-to-French task, the big model attains a BLEU of 41.0, beating all previously published single models while requiring less than one quarter of the training cost of the prior state-of-the-art; the English-to-French Transformer big used a dropout rate of P_drop = 0.1 rather than 0.3. For the base models, a single model was obtained by averaging the last 5 checkpoints written at 10-minute intervals, while for the big models the last 20 checkpoints were averaged. Beam search with a beam size of 4 and length penalty Î± = 0.6 was used, with hyperparameters chosen after development-set experiments; the maximum output length during inference was set to input length plus 50, terminating early when possible. Table 2 summarizes results and compares translation quality and training costs to other architectures from the literature. We estimate the total FLOPs to train a model by multiplying training time, the number of GPUs, and an estimate of the sustained single-precision capacity of each GPU (footnote 1).