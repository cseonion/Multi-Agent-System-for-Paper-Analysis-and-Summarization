To assess whether the Transformer generalizes to another task, the authors applied it to English constituency parsing, noting the task’s strong structural constraints and that outputs are significantly longer than inputs. They trained a 4-layer Transformer with d_model = 1024 on the WSJ portion of the Penn Treebank (about 40K training sentences) and also in a semi-supervised setting using roughly 17M sentences from high-confidence and BerkleyParser corpora. The WSJ-only setup used a 16K token vocabulary, while the semi-supervised setting used 32K. A small number of experiments were performed to select dropout on attention and residual connections, learning rates, and beam size on the Section 22 development set, with all other parameters unchanged from the English-to-German base model. During inference, the maximum output length was increased to input length plus 300, with a beam size of 21 and α = 0.3 for both WSJ-only and semi-supervised settings. Table 4 shows the Transformer performs surprisingly well despite limited task-specific tuning, beating all previously reported models except the Recurrent Neural Network Grammar. Unlike RNN seq2seq models, the Transformer also outperforms the Berkeley Parser even when trained only on the 40K WSJ sentences.