The Transformer uses multi-head attention in three modes. In encoder-decoder attention layers, queries come from the previous decoder layer while memory keys and values come from the encoder output, allowing every decoder position to attend over all input positions, consistent with standard encoder-decoder attention in seq2seq models. The encoder itself contains self-attention layers, where queries, keys, and values all originate from the encoder's previous layer, so each encoder position can attend to all positions in that layer. Similarly, the decoder employs self-attention layers that allow each decoder position to attend to all prior positions in the decoder up to that point; to preserve the auto-regressive property, illegal connections are masked by setting their softmax inputs to -âˆž in scaled dot-product attention (see Figure 2).