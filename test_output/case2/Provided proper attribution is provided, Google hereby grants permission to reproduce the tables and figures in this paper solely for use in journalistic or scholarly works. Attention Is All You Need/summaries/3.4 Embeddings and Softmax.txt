3.4 Embeddings and Softmax uses learned embeddings to map input and output tokens to vectors of dimension d_model, followed by the usual learned linear projection and softmax to produce next-token probabilities. The model ties the input embedding, output embedding, and pre-softmax linear transformation by sharing a single weight matrix across the two embeddings and the final projection. In the embedding layers, these weights are scaled by âˆšd_model.