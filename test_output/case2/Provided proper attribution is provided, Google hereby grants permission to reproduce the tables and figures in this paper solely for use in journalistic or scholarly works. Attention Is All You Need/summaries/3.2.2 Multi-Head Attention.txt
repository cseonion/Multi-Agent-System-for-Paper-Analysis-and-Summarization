To extend single-head attention, the model linearly projects the queries, keys, and values h times with different learned projection matrices into d_k, d_k, and d_v dimensions, respectively. On each projected version, the attention function is computed in parallel, yielding d_v-dimensional output values per head. These outputs are concatenated and once again projected, producing the final values as depicted in Figure 2. This multi-head approach enables attending to information from different representation subspaces at different positions, since a single head would average across them. The authors use h = 8 parallel heads, each with its own projection matrices; and due to the reduced dimension per head, the total computational cost remains similar to that of a single head operating at full dimensionality.