During training, we apply three regularization methods. First, residual dropout is applied to the output of each sub-layer before it is added to the sub-layer input and normalized. Second, dropout is applied to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, the dropout rate is P_drop = 0.1. Third, label smoothing with Ïµ_ls = 0.1 is used; this hurts perplexity but improves accuracy and BLEU score.