We call this approach Scaled Dot-Product Attention. The inputs are queries and keys of dimension d_k and values of dimension d_v. We compute the dot products of the query with all keys, divide each by sqrt(d_k), and apply a softmax to obtain the weights on the values. Practically, we process a set of queries packed into Q and the keys and values into K and V, computing the outputs as softmax(QK^T / sqrt(d_k)) V (as illustrated in Figure 2). The two most commonly used attention functions are additive attention and dot-product (multiplicative) attention; dot-product attention is identical to our algorithm except for the scaling factor of 1/sqrt(d_k). Additive attention uses a feed-forward network with a single hidden layer to compute the compatibility function. While both have similar theoretical complexity, dot-product attention is faster and more space-efficient in practice due to highly optimized matrix multiplication. For small d_k, they perform similarly, but additive attention outperforms unscaled dot-product attention for larger d_k. The authors suspect that large d_k causes dot products to grow in magnitude, pushing the softmax into regions with near-zero gradients, and to counteract this effect, the dot products are scaled by 1/sqrt(d_k).