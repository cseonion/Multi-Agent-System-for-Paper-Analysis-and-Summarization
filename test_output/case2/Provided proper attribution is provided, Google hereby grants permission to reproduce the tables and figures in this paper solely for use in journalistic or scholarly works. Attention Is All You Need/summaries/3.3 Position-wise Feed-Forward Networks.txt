Besides the attention sub-layers, every encoder and decoder layer includes a position-wise feed-forward network applied to each position independently and identically. It consists of two linear transformations with a ReLU between them; the same form is used across positions but with layer-specific parameters, equivalently two 1Ã—1 convolutions. The model dimension is d_model = 512 for input and output, with an inner-layer dimension d_ff = 2048.