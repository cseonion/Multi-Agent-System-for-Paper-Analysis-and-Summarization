To evaluate the importance of different Transformer components, the authors varied their base model and measured English-to-German translation performance, using beam search as described in the previous section but with no checkpoint averaging; results are shown in Table 3. In Table 3A they vary the number of attention heads and the attention key and value dimensions while keeping computation constant (as described in Section 3.2.2); single-head attention is 0.9 BLEU worse than the best setting, and quality also drops off with too many heads. In Table 3B, reducing the attention key size d_k hurts model quality, suggesting that compatibility is not easy and a more sophisticated compatibility function than dot product may be beneficial. Rows C and D show that bigger models perform better, and dropout is very helpful in avoiding over-fitting. Row E replaces sinusoidal positional encoding with learned positional embeddings, yielding nearly identical results to the base model.