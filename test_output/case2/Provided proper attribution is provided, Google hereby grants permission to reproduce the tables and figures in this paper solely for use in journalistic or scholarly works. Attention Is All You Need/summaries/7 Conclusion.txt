The Transformer is presented as the first sequence transduction model built entirely on attention, replacing recurrent layers with multi-headed self-attention. For translation, it trains significantly faster than architectures based on recurrent or convolutional layers, and it achieves new state-of-the-art results on the WMT 2014 English–German and English–French tasks, with the best English–German model outperforming all previously reported ensembles. The authors express excitement about attention-based models and plan to apply them to other tasks, extend the Transformer to problems with input and output modalities beyond text, explore local, restricted attention to efficiently handle large inputs and outputs such as images, audio, and video, and pursue generation that is less sequential. The code used to train and evaluate their models is available at https://github.com/tensorflow/tensor2tensor.