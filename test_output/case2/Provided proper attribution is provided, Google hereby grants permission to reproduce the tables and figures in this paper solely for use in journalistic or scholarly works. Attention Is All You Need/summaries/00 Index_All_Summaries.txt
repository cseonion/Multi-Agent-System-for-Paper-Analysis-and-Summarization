논문 제목: Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need
전체 섹션별 요약 인덱스
============================================================

섹션 1: Introduction
----------------------------------------
I’m ready to summarize, but I need the raw text of Section 1 and, if available, the summary of any previous sections (there are none for the first section). Please paste Section 1 in full. If the paper is divided into multiple sections, you can provide them in order after I return Section 1’s summary. I will produce a concise, single-paragraph summary that preserves all key numerical details and results, with no bullet points. If there are important numbers in figures or tables, you can include their values or references to them so I can preserve them accurately.

섹션 2: Background
----------------------------------------
Background frames the goal of reducing sequential computation by noting that prior architectures like Extended Neural GPU, ByteNet, and ConvS2S rely on convolutional foundations that compute in parallel but incur distance-dependent costs: ConvS2S scales linearly with distance, ByteNet logarithmically, making long-range dependencies harder to learn. The Transformer counters this with a constant number of operations, though it trades off some effective resolution due to averaging attention-weighted positions, an issue alleviated by Multi-Head Attention as described in section 3.2. Self-attention relates different positions within a single sequence to produce representations and has proven effective across tasks such as reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations [4,27,28,22]. End-to-end memory networks instead use a recurrent attention mechanism rather than sequence-aligned recurrence and have shown promise on simple-language QA and language modeling tasks [34]. To our knowledge, the Transformer is the first transduction model that relies entirely on self-attention to compute representations of input and output without sequence-aligned RNNs or convolution. The section then previews that the paper will describe the Transformer, motivate self-attention, and compare its advantages to models such as [17,18] and [9].

섹션 3: Model Architecture
----------------------------------------
Most competitive neural sequence transduction models use an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) one symbol at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, as shown in the left and right halves of Figure 1.

섹션 3.1: Encoder and Decoder Stacks
----------------------------------------
The encoder and decoder both use stacks of six identical layers (N = 6). Each encoder layer contains two sub-layers: a multi-head self-attention mechanism and a positionwise fully connected feed-forward network, with residual connections around each sub-layer followed by layer normalization, implemented as LayerNorm(x + Sublayer(x)). All sub-layers and the embedding layers produce outputs of dimension d_model = 512. The decoder mirrors this structure but adds a third sub-layer per layer that performs multi-head attention over the encoder stack (encoder-decoder attention). Like the encoder, the decoder uses residual connections and layer normalization around each sub-layer. Additionally, the decoder’s self-attention is masked to prevent attending to subsequent positions, and the output embeddings are offset by one position, ensuring that position i can depend only on known outputs at positions less than i.

섹션 3.2: Attention
----------------------------------------
Section 3.2 defines attention as a function that takes a query and a set of key–value pairs and yields an output vector. All inputs and the output are vectors. The output is produced as a weighted sum of the values, with weights derived from a compatibility score between the query and each key; this mechanism is implemented as scaled dot-product attention and extended to multi-head attention, which runs multiple attention mechanisms in parallel.

섹션 3.2.1: Scaled Dot-Product Attention
----------------------------------------
We call this approach Scaled Dot-Product Attention. The inputs are queries and keys of dimension d_k and values of dimension d_v. We compute the dot products of the query with all keys, divide each by sqrt(d_k), and apply a softmax to obtain the weights on the values. Practically, we process a set of queries packed into Q and the keys and values into K and V, computing the outputs as softmax(QK^T / sqrt(d_k)) V (as illustrated in Figure 2). The two most commonly used attention functions are additive attention and dot-product (multiplicative) attention; dot-product attention is identical to our algorithm except for the scaling factor of 1/sqrt(d_k). Additive attention uses a feed-forward network with a single hidden layer to compute the compatibility function. While both have similar theoretical complexity, dot-product attention is faster and more space-efficient in practice due to highly optimized matrix multiplication. For small d_k, they perform similarly, but additive attention outperforms unscaled dot-product attention for larger d_k. The authors suspect that large d_k causes dot products to grow in magnitude, pushing the softmax into regions with near-zero gradients, and to counteract this effect, the dot products are scaled by 1/sqrt(d_k).

섹션 3.2.2: Multi-Head Attention
----------------------------------------
To extend single-head attention, the model linearly projects the queries, keys, and values h times with different learned projection matrices into d_k, d_k, and d_v dimensions, respectively. On each projected version, the attention function is computed in parallel, yielding d_v-dimensional output values per head. These outputs are concatenated and once again projected, producing the final values as depicted in Figure 2. This multi-head approach enables attending to information from different representation subspaces at different positions, since a single head would average across them. The authors use h = 8 parallel heads, each with its own projection matrices; and due to the reduced dimension per head, the total computational cost remains similar to that of a single head operating at full dimensionality.

섹션 3.2.3: Applications of Attention in our Model
----------------------------------------
The Transformer uses multi-head attention in three modes. In encoder-decoder attention layers, queries come from the previous decoder layer while memory keys and values come from the encoder output, allowing every decoder position to attend over all input positions, consistent with standard encoder-decoder attention in seq2seq models. The encoder itself contains self-attention layers, where queries, keys, and values all originate from the encoder's previous layer, so each encoder position can attend to all positions in that layer. Similarly, the decoder employs self-attention layers that allow each decoder position to attend to all prior positions in the decoder up to that point; to preserve the auto-regressive property, illegal connections are masked by setting their softmax inputs to -∞ in scaled dot-product attention (see Figure 2).

섹션 3.3: Position-wise Feed-Forward Networks
----------------------------------------
Besides the attention sub-layers, every encoder and decoder layer includes a position-wise feed-forward network applied to each position independently and identically. It consists of two linear transformations with a ReLU between them; the same form is used across positions but with layer-specific parameters, equivalently two 1×1 convolutions. The model dimension is d_model = 512 for input and output, with an inner-layer dimension d_ff = 2048.

섹션 3.4: Embeddings and Softmax
----------------------------------------
3.4 Embeddings and Softmax uses learned embeddings to map input and output tokens to vectors of dimension d_model, followed by the usual learned linear projection and softmax to produce next-token probabilities. The model ties the input embedding, output embedding, and pre-softmax linear transformation by sharing a single weight matrix across the two embeddings and the final projection. In the embedding layers, these weights are scaled by √d_model.

섹션 3.5: Positional Encoding
----------------------------------------
Because the model has no recurrence or convolution, positional information is injected by adding positional encodings to the input embeddings at the bottoms of the encoder and decoder stacks. The encodings share the same dimension d_model as the embeddings so they can be summed, and they can be either learned or fixed [9]. This work uses sinusoidal encodings: each dimension i corresponds to a sinusoid over positions pos, with wavelengths forming a geometric progression from 2π to 10000·2π. This design was chosen because it should enable the model to attend by relative positions, since for any fixed offset k, P_E,pos+k can be expressed as a linear function of P_E,pos. They also tried learned positional embeddings [9] and observed nearly identical results (Table 3 row (E)); the sinusoidal version is preferred because it may allow extrapolation to sequence lengths longer than those seen during training.

섹션 4: Why Self-Attention
----------------------------------------
This section argues for self-attention by contrasting it with recurrent and convolutional layers for mapping a variable-length input sequence to a sequence of the same length. It focuses on three desiderata: per-layer computational complexity, the amount of parallelization (i.e., the minimum number of sequential operations), and the path length of signals required to capture long-range dependencies. A self-attention layer connects all positions with a constant number of sequential operations, whereas a recurrent layer needs O(n) sequential steps; thus self-attention is faster than recurrence when the sequence length n is smaller than the representation dimensionality d, which is typical for sentence representations in modern models. To handle very long sequences, attention could be restricted to a neighborhood of radius r, which raises the maximum path length to O(n/r) and is identified as a direction for future work. In contrast, a single convolutional layer with kernel width k < n does not connect all input–output positions, requiring O(n/k) stacked layers for contiguous kernels or O(log_k(n)) layers for dilated convolutions, thereby increasing the longest path length; convolutional layers are generally more expensive than recurrent layers by a factor of k. Separable convolutions reduce this cost substantially to O(k), and with k = n their complexity matches the combination of a self-attention layer and a pointwise feed-forward layer—precisely the architecture adopted in this work. As a side benefit, self-attention may yield more interpretable models; attention heads learn to perform different tasks and often align with syntactic and semantic structure, with illustrative examples discussed in the appendix.

섹션 5: Training
----------------------------------------
The section outlines the training regime used for the models.

섹션 5.1: Training Data and Batching
----------------------------------------
The section describes the training data and batching strategy. For English–German on WMT 2014, about 4.5 million sentence pairs were used, with sentences encoded by byte-pair encoding to a shared source–target vocabulary of approximately 37,000 tokens. For English–French, the larger WMT 2014 dataset of 36 million sentences was used, with tokens split into a 32,000 token word-piece vocabulary. Sentence pairs were batched by approximate sequence length, and each training batch contained about 25,000 source tokens and 25,000 target tokens.

섹션 5.2: Hardware and Schedule
----------------------------------------
Training ran on a single machine with 8 NVIDIA P100 GPUs. Base models, using the paper’s hyperparameters, required about 0.4 seconds per training step, for 100,000 steps (approximately 12 hours). Big models, as indicated on the bottom line of Table 3, took about 1.0 second per step and were trained for 300,000 steps, roughly 3.5 days.

섹션 5.3: Optimizer
----------------------------------------
We used the Adam optimizer with β1 = 0.9, β2 = 0.98, and ε = 1e-9. The learning rate was scheduled to rise linearly during the first 4000 steps (warmup_steps = 4000) and then decay proportionally to the inverse square root of the step number, i.e., a linear warmup followed by 1/√step decay.

섹션 5.4: Regularization
----------------------------------------
During training, we apply three regularization methods. First, residual dropout is applied to the output of each sub-layer before it is added to the sub-layer input and normalized. Second, dropout is applied to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, the dropout rate is P_drop = 0.1. Third, label smoothing with ϵ_ls = 0.1 is used; this hurts perplexity but improves accuracy and BLEU score.

섹션 6.1: Machine Translation
----------------------------------------
On the WMT 2014 English-to-German task, the Transformer big model outperforms all previously reported models, including ensembles, by more than 2.0 BLEU and achieves a new state-of-the-art BLEU of 28.4; its configuration is listed in the bottom line of Table 3, and training took 3.5 days on 8 P100 GPUs. Even the base model surpasses all earlier published models and ensembles at a fraction of their training cost. For the WMT 2014 English-to-French task, the big model attains a BLEU of 41.0, beating all previously published single models while requiring less than one quarter of the training cost of the prior state-of-the-art; the English-to-French Transformer big used a dropout rate of P_drop = 0.1 rather than 0.3. For the base models, a single model was obtained by averaging the last 5 checkpoints written at 10-minute intervals, while for the big models the last 20 checkpoints were averaged. Beam search with a beam size of 4 and length penalty α = 0.6 was used, with hyperparameters chosen after development-set experiments; the maximum output length during inference was set to input length plus 50, terminating early when possible. Table 2 summarizes results and compares translation quality and training costs to other architectures from the literature. We estimate the total FLOPs to train a model by multiplying training time, the number of GPUs, and an estimate of the sustained single-precision capacity of each GPU (footnote 1).

섹션 6.2: Model Variations
----------------------------------------
To evaluate the importance of different Transformer components, the authors varied their base model and measured English-to-German translation performance, using beam search as described in the previous section but with no checkpoint averaging; results are shown in Table 3. In Table 3A they vary the number of attention heads and the attention key and value dimensions while keeping computation constant (as described in Section 3.2.2); single-head attention is 0.9 BLEU worse than the best setting, and quality also drops off with too many heads. In Table 3B, reducing the attention key size d_k hurts model quality, suggesting that compatibility is not easy and a more sophisticated compatibility function than dot product may be beneficial. Rows C and D show that bigger models perform better, and dropout is very helpful in avoiding over-fitting. Row E replaces sinusoidal positional encoding with learned positional embeddings, yielding nearly identical results to the base model.

섹션 6.3: English Constituency Parsing
----------------------------------------
To assess whether the Transformer generalizes to another task, the authors applied it to English constituency parsing, noting the task’s strong structural constraints and that outputs are significantly longer than inputs. They trained a 4-layer Transformer with d_model = 1024 on the WSJ portion of the Penn Treebank (about 40K training sentences) and also in a semi-supervised setting using roughly 17M sentences from high-confidence and BerkleyParser corpora. The WSJ-only setup used a 16K token vocabulary, while the semi-supervised setting used 32K. A small number of experiments were performed to select dropout on attention and residual connections, learning rates, and beam size on the Section 22 development set, with all other parameters unchanged from the English-to-German base model. During inference, the maximum output length was increased to input length plus 300, with a beam size of 21 and α = 0.3 for both WSJ-only and semi-supervised settings. Table 4 shows the Transformer performs surprisingly well despite limited task-specific tuning, beating all previously reported models except the Recurrent Neural Network Grammar. Unlike RNN seq2seq models, the Transformer also outperforms the Berkeley Parser even when trained only on the 40K WSJ sentences.

섹션 7: Conclusion
----------------------------------------
The Transformer is presented as the first sequence transduction model built entirely on attention, replacing recurrent layers with multi-headed self-attention. For translation, it trains significantly faster than architectures based on recurrent or convolutional layers, and it achieves new state-of-the-art results on the WMT 2014 English–German and English–French tasks, with the best English–German model outperforming all previously reported ensembles. The authors express excitement about attention-based models and plan to apply them to other tasks, extend the Transformer to problems with input and output modalities beyond text, explore local, restricted attention to efficiently handle large inputs and outputs such as images, audio, and video, and pursue generation that is less sequential. The code used to train and evaluate their models is available at https://github.com/tensorflow/tensor2tensor.

섹션 etc: Input-Input Layer5
----------------------------------------
This section, titled Input-Input Layer5, analyzes the Layer-5 input-to-input attention using the sentence “The Law will never be perfect, but its application should be just this is what we are missing, in my opinion.” with EOS and padding tokens included. It presents visualizations showing full attentions for head 5 and isolated attentions from the word “its” for attention heads 5 and 6, and notes that the attentions are very sharp for this word, illustrating a highly focused, token-specific pattern in this layer.

============================================================
총 섹션 수: 22
생성 시간: 2025-08-19 08:30:51
