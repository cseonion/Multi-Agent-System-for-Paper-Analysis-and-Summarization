The section describes the training data and batching strategy. For English–German on WMT 2014, about 4.5 million sentence pairs were used, with sentences encoded by byte-pair encoding to a shared source–target vocabulary of approximately 37,000 tokens. For English–French, the larger WMT 2014 dataset of 36 million sentences was used, with tokens split into a 32,000 token word-piece vocabulary. Sentence pairs were batched by approximate sequence length, and each training batch contained about 25,000 source tokens and 25,000 target tokens.