Summary of Section 2: Background

- The section situates the Transformer among architectures aimed at reducing sequential computation, highlighting alternatives like Extended Neural GPU, ByteNet, and ConvS2S that rely on convolution to process input and output positions in parallel. In these models, the cost to relate signals from two positions grows with their distance: linearly for ConvS2S and logarithmically for ByteNet, making learning long-range dependencies harder.

- The Transformer addresses this by using self-attention, which achieves a constant amount of work to relate any two positions. However, there is a trade-off: attention-weighted averaging can reduce effective resolution, a drawback mitigated by Multi-Head Attention (covered in section 3.2).

- Self-attention (intra-attention) is defined as an attention mechanism that relates different positions within a single sequence to compute its representation. It has been successfully applied to tasks such as reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.

- End-to-end memory networks are noted as using a recurrent attention mechanism rather than sequence-aligned recurrence, performing well on simple-language question answering and language modeling.

- The authors claim that the Transformer is the first transduction model to rely entirely on self-attention to compute representations of both input and output without using sequence-aligned RNNs or convolutions. The section previews that the subsequent sections will describe the Transformer, motivate self-attention, and discuss its advantages over prior models like those cited.