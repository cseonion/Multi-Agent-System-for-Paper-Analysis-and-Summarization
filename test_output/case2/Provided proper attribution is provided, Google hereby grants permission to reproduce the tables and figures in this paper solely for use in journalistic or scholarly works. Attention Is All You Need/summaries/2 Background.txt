Background frames the goal of reducing sequential computation by noting that prior architectures like Extended Neural GPU, ByteNet, and ConvS2S rely on convolutional foundations that compute in parallel but incur distance-dependent costs: ConvS2S scales linearly with distance, ByteNet logarithmically, making long-range dependencies harder to learn. The Transformer counters this with a constant number of operations, though it trades off some effective resolution due to averaging attention-weighted positions, an issue alleviated by Multi-Head Attention as described in section 3.2. Self-attention relates different positions within a single sequence to produce representations and has proven effective across tasks such as reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations [4,27,28,22]. End-to-end memory networks instead use a recurrent attention mechanism rather than sequence-aligned recurrence and have shown promise on simple-language QA and language modeling tasks [34]. To our knowledge, the Transformer is the first transduction model that relies entirely on self-attention to compute representations of input and output without sequence-aligned RNNs or convolution. The section then previews that the paper will describe the Transformer, motivate self-attention, and compare its advantages to models such as [17,18] and [9].