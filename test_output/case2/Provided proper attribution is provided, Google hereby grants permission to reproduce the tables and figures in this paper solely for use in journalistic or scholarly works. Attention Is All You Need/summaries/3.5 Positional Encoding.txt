Because the model has no recurrence or convolution, positional information is injected by adding positional encodings to the input embeddings at the bottoms of the encoder and decoder stacks. The encodings share the same dimension d_model as the embeddings so they can be summed, and they can be either learned or fixed [9]. This work uses sinusoidal encodings: each dimension i corresponds to a sinusoid over positions pos, with wavelengths forming a geometric progression from 2π to 10000·2π. This design was chosen because it should enable the model to attend by relative positions, since for any fixed offset k, P_E,pos+k can be expressed as a linear function of P_E,pos. They also tried learned positional embeddings [9] and observed nearly identical results (Table 3 row (E)); the sinusoidal version is preferred because it may allow extrapolation to sequence lengths longer than those seen during training.