Summary of Section 4: Why Self-Attention

- Purpose and criteria: The section contrasts self-attention with recurrent and convolutional layers for mapping a variable-length input sequence to an output sequence. It focuses on three desiderata: (1) total computational complexity per layer, (2) amount of parallelization (minimum sequential operations), and (3) the path length between long-range dependencies in the network.

- Self-attention vs recurrence: A self-attention layer connects all positions with a constant (O(1)) number of sequential operations, whereas a recurrent layer requires O(n) sequential steps. This makes self-attention more parallelizable and, in many settings, faster when the sequence length n is smaller than the representation dimensionality d.

- Long sequences and local attention: For very long sequences, performance can be improved by restricting attention to a neighborhood of size r around each position, which increases the maximum path length to O(n/r). The authors note they plan to investigate this approach further in future work.

- Self-attention vs convolution: A single convolutional layer with kernel width k < n cannot connect all input-output position pairs, requiring either O(n/k) stacked layers (for contiguous kernels) or O(log_k(n)) layers (with dilations) to do so, which increases the longest path length. Convolutional layers are generally more expensive than recurrent layers by a factor of k. Separable convolutions reduce complexity significantly to O(k). Even with k = n, the complexity of a separable convolution becomes comparable to the combination of a self-attention layer and a point-wise feed-forward layer—precisely the approach used in the authors’ model.

- Interpretability: Self-attention offers potential interpretability advantages. The authors examine attention distributions and find that different heads learn to perform varied tasks, with some heads reflecting syntactic and semantic structure. These observations are discussed further in the appendix.

- Takeaway: Self-attention presents favorable trade-offs in complexity, parallelism, and the ability to model long-range dependencies, while also enabling interpretable mechanisms via attention heads. The section also notes avenues for extending the approach to longer sequences (e.g., local attention) as future work.