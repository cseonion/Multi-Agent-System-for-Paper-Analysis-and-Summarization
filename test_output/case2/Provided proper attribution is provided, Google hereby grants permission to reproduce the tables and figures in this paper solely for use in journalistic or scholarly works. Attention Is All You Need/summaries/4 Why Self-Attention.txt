This section argues for self-attention by contrasting it with recurrent and convolutional layers for mapping a variable-length input sequence to a sequence of the same length. It focuses on three desiderata: per-layer computational complexity, the amount of parallelization (i.e., the minimum number of sequential operations), and the path length of signals required to capture long-range dependencies. A self-attention layer connects all positions with a constant number of sequential operations, whereas a recurrent layer needs O(n) sequential steps; thus self-attention is faster than recurrence when the sequence length n is smaller than the representation dimensionality d, which is typical for sentence representations in modern models. To handle very long sequences, attention could be restricted to a neighborhood of radius r, which raises the maximum path length to O(n/r) and is identified as a direction for future work. In contrast, a single convolutional layer with kernel width k < n does not connect all input–output positions, requiring O(n/k) stacked layers for contiguous kernels or O(log_k(n)) layers for dilated convolutions, thereby increasing the longest path length; convolutional layers are generally more expensive than recurrent layers by a factor of k. Separable convolutions reduce this cost substantially to O(k), and with k = n their complexity matches the combination of a self-attention layer and a pointwise feed-forward layer—precisely the architecture adopted in this work. As a side benefit, self-attention may yield more interpretable models; attention heads learn to perform different tasks and often align with syntactic and semantic structure, with illustrative examples discussed in the appendix.