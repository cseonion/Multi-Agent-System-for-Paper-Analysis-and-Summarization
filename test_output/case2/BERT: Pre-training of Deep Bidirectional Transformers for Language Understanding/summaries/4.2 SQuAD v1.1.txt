Section 4.2 – SQuAD v1.1

- Task and data: SQuAD v1.1 is 100k crowdsourced question/answer pairs. Given a question and a Wikipedia passage, the model must predict the exact text span of the answer within the passage.

- Input representation and model approach (during fine-tuning): The question and passage are packed into a single sequence with the question represented by the A embedding and the passage by the B embedding. The model uses a start vector S and an end vector E (learned during fine-tuning) to compute start and end probabilities over token positions via dot products with token representations T_i, followed by softmax.

- Span scoring and training objective: The score of a candidate span (i, j) is S•T_i + E•T_j (with j ≥ i). The predicted answer is the span with the maximum score. Training maximizes the log-likelihood of the correct start and end positions.

- Fine-tuning setup and hyperparameters: Fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32. The input/output and loss formulation align with standard SQuAD span prediction tasks.

- Data augmentation and baselines: Table 2 compares against top leaderboard entries and published systems. To improve robustness, the authors use modest data augmentation by first fine-tuning on TriviaQA before fine-tuning on SQuAD.

- Key results on SQuAD v1.1: The best system (BERT-based) outperforms the top leaderboard system by +1.5 F1 when using ensembling and by +1.3 F1 as a single system. In fact, the single BERT model surpasses the top ensemble in F1.

- Transition to SQuAD v2.0: The section then describes SQuAD v2.0, which adds the possibility of no answer. The approach extends the BERT model to include a no-answer option using the [CLS] token as the no-answer span; the no-answer score is s_null = S•[CLS] + E•[CLS], and the best non-null span score ŝ_{i,j} = max_{i≤j} (S•T_i + E•T_j). A non-null prediction is made if ŝ_{i,j} > s_null + τ, with τ chosen on the dev set to maximize F1.

- SQuAD v2.0 training and results: For the v2.0 extension, the model is fine-tuned for 2 epochs with a learning rate of 5e-5 and batch size 48. The results show a +5.1 F1 improvement over previous best systems (excluding those using BERT as a component in Table 3). TriviaQA data was not used for this v2.0 model.