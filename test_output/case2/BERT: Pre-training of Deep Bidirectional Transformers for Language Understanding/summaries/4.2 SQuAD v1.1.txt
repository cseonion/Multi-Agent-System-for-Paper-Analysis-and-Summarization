The SQuAD v1.1 section describes a dataset of 100k crowdsourced question/answer pairs where the task is to predict the exact answer span in a passage given a question and the passage embedded as a single packed sequence, with the question using an A embedding and the passage a B embedding. During fine-tuning, a start vector S and an end vector E are introduced; the probability that position i is the start of the answer is computed as a dot product S·T_i followed by a softmax over all positions, and the end probability is computed similarly with E. The score of a candidate span i..j is S·T_i + E·T_j with j ≥ i, and the model predicts the maximum-scoring valid span. The training objective is the sum of the log-likelihoods of the correct start and end positions. They fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32, and employ modest data augmentation by first fine-tuning on TriviaQA (TriviaQA-Wiki paragraphs formed of the first 400 tokens that contain at least one possible answer) before SQuAD. Their best system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system, with the single BERT model surpassing the top ensemble. For SQuAD v2.0, which adds the possibility of no answer, they extend the model by treating questions without an answer as having an answer span at the [CLS] token; the no-answer score is s_null = S·C + E·C, while the best non-null span ŝ_i,j = max_i≤j S·T_i + E·T_j, and a non-null answer is predicted when ŝ_i,j > s_null + τ, with τ chosen on the dev set to maximize F1. This v2.0 model does not use TriviaQA data, is fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48, and, when compared to prior leaderboard entries and top published work (excluding systems that use BERT as a component), achieves a +5.1 F1 improvement over the previous best.