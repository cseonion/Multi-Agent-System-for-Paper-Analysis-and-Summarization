This ablation study evaluates masking strategies used with BERT’s masked language modeling (MLM) objective to reduce pre-training–finetuning mismatch because the [MASK] token never appears during fine-tuning. Dev results are reported for MNLI and NER; for NER, both fine-tuning and a feature-based approach (concatenating the last four BERT layers) are shown, with results in Table 8. The MLM masking schemes are MASK (replace with [MASK]), SAME (keep the token), and RND (random token), with BERT’s pre-training using an 80/10/10 split; for the feature-based NER, the last four layers were found optimal. The results indicate that fine-tuning is robust to masking choices, but MASK-only hurts the feature-based NER, and RND-only underperforms relative to the mixed strategy. In the OpenAI GPT comparison, GPT is a left-to-right Transformer LM trained on BooksCorpus, whereas BERT uses BooksCorpus plus Wikipedia; GPT introduces [SEP] and [CLS] tokens only at fine-tuning, while BERT learns them during pre-training; GPT is trained for 1 million steps with a batch size of about 32,000 words, versus BERT’s 1 million steps with a batch size of about 128,000 words; GPT’s fine-tuning learning rate is fixed at 5e-5, whereas BERT uses task-specific fine-tuning learning rates that are tuned on the development set. Ablation experiments cited show that most improvements derive from the two pre-training tasks and bidirectionality, rather than these other design differences. The Appendix is organized into three sections (A for implementation details, B for experimental details, C for ablations including Effect of Training Steps and Ablation for Different Masking Procedures). A.1 illustrates the pre-training tasks with examples of masked LM; for random masking, selecting the 4th token in “my dog is hairy” and replacing 1.5% of all tokens (10% of 15%) means MLM predicts only 15% of tokens per batch, implying more pre-training steps are needed. This 15% token-prediction rate motivates longer pre-training than standard language modeling. The GLUE fine-tuning discussion mirrors pre-training, with datasets MNLI, QQP, QNLI, SST-2, CoLA, STS-B, MRPC, and RTE (WINOGRAD NLI excluded due to dataset issues and GLUE’s majority-class baseline; for GLUE submissions they predict the majority class). Hyperparameters for fine-tuning largely mirror pre-training, with batch sizes of 16 or 32, Adam learning rates of 5e-5, 3e-5, or 2e-5, epochs of 2, 3, or 4, and dropout fixed at 0.1; larger datasets show less sensitivity, justifying an exhaustive search over these values to select the dev-best model. The illustration of fine-tuning on different tasks is given in Figure 4, and the GLUE tasks are as listed above, with the stated hyperparameter ranges commonly effective across tasks.