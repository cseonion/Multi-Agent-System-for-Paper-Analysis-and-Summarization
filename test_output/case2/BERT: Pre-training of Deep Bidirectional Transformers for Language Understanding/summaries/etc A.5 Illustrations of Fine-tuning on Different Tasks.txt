Here is a concise, section-by-section summary of the current content for A.5 Illustrations of Fine-tuning on Different Tasks and the related appendix material:

- Masking strategy ablation (pre-training vs. fine-tuning)
  - Purpose: assess how different MLM masking strategies during pre-training affect downstream fine-tuning, since fine-tuning never sees [MASK].
  - Strategies tested: MASK (replace with [MASK]), SAME (keep token), RND (random token).
  - BERT masking mix during pre-training: 80% MASK, 10% SAME, 10% RND.
  - Evaluation: MNLI and NER (NER includes both fine-tuning and feature-based settings to gauge mismatch amplification).
  - Key findings:
    - Fine-tuning is surprisingly robust to masking variations.
    - MASK-only is problematic for NER when using the feature-based approach.
    - RND-only performs worse than the mixed strategy.
  - Conclusion: a mixed masking strategy is preferable; robustness in fine-tuning is demonstrated, with some caveats for feature-based NER.

- OpenAI GPT comparison and key distinctions
  - Positioning: GPT (OpenAI) and BERT are both fine-tuning based; ELMo is feature-based.
  - Core differences highlighted between GPT and BERT:
    - Data: GPT trained on BooksCorpus (800M words); BERT trained on BooksCorpus (800M) plus Wikipedia (2.5B words).
    - Special tokens: GPT uses SEP and CLS at fine-tuning time; BERT learns SEP, CLS, and sentence embeddings during pre-training.
    - Training regime: GPT ~1M steps with a batch sized to 32,000 words; BERT ~1M steps with 128,000-word batches.
    - Fine-tuning learning rate: GPT uses a single fixed rate (5e-5); BERT uses task-specific fine-tuning learning rates.
  - Main takeaway: Most improvements stem from the two pre-training tasks (MLM and NSP) and bidirectionality, rather than architectural/training differences between GPT and BERT. Ablations in Section 5.1 support this.

- Appendix structure and purpose
  - Organization:
    - Appendix A: Additional implementation details.
    - Appendix B: Additional experiment details.
    - Appendix C: Additional ablation studies.
  - Focus of additional ablations:
    - A.1: Effect of Number of Training Steps.
    - A.1: Ablation for Different Masking Procedures.
  - Overall aim: provide concrete methodological details to support the main results.

- A.1 Illustration of the Pre-training Tasks (and related content)
  - Content: concrete examples of MLM and masking procedures.
  - Example used: “my dog is hairy,” masking the 4th token (hairy).
  - Rationale: masks/random replacements force the Transformer to maintain distributional contextual representations without knowing which tokens will be predicted; random replacements occur on 1.5% of all tokens (10% of the 15% masked tokens) to avoid harming language understanding.
  - Implications:
    - MLM predicts ~15% of tokens per batch, implying more pre-training steps may be needed for convergence.
    - The section reinforces why the 80/10/10 masking mix is examined via ablations.

- Fine-tuning on GLUE tasks and practical details (Appendix content)
  - Fine-tuning setup: largely retains pre-training hyperparameters with task-specific adjustments to batch size, learning rate, and epochs; dropout fixed at 0.1.
  - Hyperparameter ranges that work well across tasks:
    - Batch size: 16, 32
    - Adam learning rate: 5e-5, 3e-5, 2e-5
    - Number of epochs: 2, 3, 4
  - Observations: larger datasets (≥100k labeled examples) are less sensitive to hyperparameters than small datasets.
  - GLUE tasks covered: MNLI, QQP, QNLI, SST-2, CoLA, STS (Semantic Textual Similarity), MRPC, RTE; WNLI is excluded from GLUE submissions due to baseline concerns, with majority-class predictions used in those cases.
  - Training scale and hardware:
    - Pre-training data: ~3.3 billion words
    - Duration: ~4 days per run
    - Hardware: BERT-Base on 4 Cloud TPUs; BERT-Large on 16 Cloud TPUs
    - Setup: ~1,000,000 pre-training steps (~40 epochs); batch size 256; sequence length 512 (with a strategy balancing shorter and longer sequence steps)
  - Practical conclusion: MLM + NSP with bidirectionality yields strong, transferable representations across tasks; longer sequences boost learning at higher compute cost; hyperparameters are fairly robust, especially on larger datasets.

- Overall takeaway for this section
  - Masking ablations show fine-tuning robustness to masking choices, with noted caveats for feature-based NER when using MASK-only.
  - The GPT vs. BERT discussion reinforces that the majority of gains come from the pre-training objectives and bidirectionality rather than other architectural differences.
  - The Appendix content provides concrete methodological detail on MLM masking illustrations, GLUE fine-tuning setups, hyperparameters, dataset scope, and hardware scale, underscoring the practical feasibility and generalizability of the BERT approach.
  - Practically, the combination of MLM + NSP with bidirectional context yields strong transferable representations; longer sequences improve learning at computational cost; hyperparameters are reasonably robust across tasks, particularly in data-rich regimes.