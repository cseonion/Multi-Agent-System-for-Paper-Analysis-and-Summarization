GLUE is a collection of diverse natural language understanding tasks. For fine-tuning, we represent the input sequence (single sentence or sentence pairs) as in Section 3 and use the final hidden vector C ∈ R^H from the [CLS] token as the aggregate representation. The new parameters introduced during fine-tuning are the classification weights W ∈ R^{K×H}, where K is the number of labels, and the loss is log(softmax(CW^T)). We use a batch size of 32 and fine-tune for 3 epochs on all GLUE tasks. For each task, the best fine-tuning learning rate is selected on the Dev set from {5e-5, 4e-5, 3e-5, 2e-5}. For BERT-LARGE, finetuning is sometimes unstable on small datasets, so several random restarts are performed with the same pre-trained checkpoint but different fine-tuning data shuffling and classifier initializations, selecting the best Dev model. Nine results are reported in Table 1. Both BERT-BASE and BERT-LARGE outperform all prior systems on all tasks by substantial margins, achieving 4.5% and 7.0% average accuracy improvements, respectively. BERT-BASE and OpenAI GPT are nearly identical architecturally apart from attention masking; for MNLI, BERT achieves a 4.6% absolute accuracy gain. On the official GLUE leaderboard, BERT-LARGE scores 80.5 versus 72.8 for OpenAI GPT. BERT-LARGE consistently outperforms BERT-BASE across tasks, especially those with limited data, with the model-size effect further explored in Section 5.2.