3.1 Pre-training BERT describes two unsupervised objectives used to train the model. Task 1 is Masked LM (MLM), where 15% of WordPiece tokens are masked at random and the model must predict the original tokens from the final hidden vectors of the masked positions. To mitigate pre-trainingâ€“fine-tuning mismatch caused by the [MASK] token, the masking procedure does not always replace the chosen token with [MASK]: 80% of the time it becomes [MASK], 10% of the time it becomes a random token, and 10% of the time it remains unchanged, with cross-entropy loss predicting the original token. Task 2 is Next Sentence Prediction (NSP), training the model to determine whether sentence B follows sentence A, with 50% IsNext and 50% NotNext; the [CLS] final vector C is used for NSP. The pre-training data consist of BooksCorpus (800M words) and English Wikipedia (2,500M words), with Wikipedia passages stripped of lists, tables, and headers, and a document-level corpus emphasized to capture long contiguous sequences rather than a shuffled sentence corpus like the Billion Word Benchmark. The procedure largely follows prior literature, and variations of MLM are discussed in Appendix C.2; NSP is shown to benefit QA and NLI (Section 5.1).