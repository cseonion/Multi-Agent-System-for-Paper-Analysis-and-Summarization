Summary of Section 3.1: Pre-training BERT

- Core idea: Pre-train BERT with two unsupervised tasks before fine-tuning on downstream tasks.
  - Task 1: Masked Language Modeling (MLM)
    - Rationale: A truly bidirectional model should be more powerful than left-to-right models.
    - Method: Randomly mask 15% of WordPiece tokens in each sequence and train to predict the original tokens.
    - Details to avoid "seeing itself" during pre-training: the i-th masked token is predicted using the final hidden vectors; the masking process is varied to reduce pre-training/fine-tuning mismatch.
    - Masking variants used during training: for a chosen token, replace with (1) [MASK] 80% of the time, (2) a random token 10% of the time, (3) unchanged 10% of the time.
    - Loss: cross-entropy over the vocabulary for the original token.
    - Note on mismatch with fine-tuning: [MASK] tokens do not appear during fine-tuning; to mitigate this, the model sometimes uses actual tokens instead of [MASK].
  - Task 2: Next Sentence Prediction (NSP)
    - Purpose: Teach the model to understand relationships between sentences, aiding tasks like QA and Natural Language Inference.
    - Setup: For each training example with sentences A and B, 50% of the time B is the actual next sentence (IsNext) and 50% of the time B is a random sentence (NotNext).
    - Benefit: Despite its simplicity, NSP improves performance on QA and NLI, as shown in the paper.
  - Position embeddings and sentence handling
    - NSP relates to learning representations across sentences; Unlike prior work that transfers only sentence embeddings, BERT transfers all parameters to initialize downstream models.
    - Input can handle single sentences or sentence pairs by packing two sentences with a [SEP] separator.
    - Token representations: WordPiece embeddings (vocabulary ~30,000), plus learned segment (sentence) embeddings.
    - Input embedding E is the sum of token, segment, and position embeddings.
    - The [CLS] token serves as the aggregate sequence representation for classification tasks; its final hidden state is used for downstream predictions.
- Training data and corpus
  - Pre-training corpus details: BooksCorpus (800 million words) and English Wikipedia (2.5 billion words).
  - For Wikipedia, only text passages are used (lists, tables, headers removed), focusing on long contiguous sequences.
  - Emphasis on using a document-level corpus (as opposed to a shuffled sentence-level corpus) to capture longer context.

Overall, this section describes the MLM and NSP pre-training tasks, how they mitigate pre-training/fine-tuning mismatches, and the input/embedding setup plus the data sources used for pre-training.