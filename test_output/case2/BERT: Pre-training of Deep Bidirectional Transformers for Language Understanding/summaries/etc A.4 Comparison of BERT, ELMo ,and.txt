This section conducts an ablation study on the masking strategies used with BERT’s masked language model (MLM) objective to reduce pre-training–finetuning mismatch, since the [MASK] token never appears during fine-tuning. Dev results for MNLI and NER are reported, with NER results given for both fine-tuning and a feature-based approach since mismatch is expected to be amplified for the latter; results are shown in Table 8. In the table, MASK replaces the target token with [MASK], SAME keeps the token unchanged, and RND replaces the target with a random token, with BERT using the 80/10/10 split for these strategies. For the feature-based NER, the last four layers of BERT are used as features, as was found optimal earlier. The findings show that fine-tuning is surprisingly robust to masking choices; however, using only MASK hurts the feature-based NER, and using only RND performs worse than the authors’ combined strategy.

In the OpenAI GPT discussion, the authors compare GPT with ELMo and BERT to isolate the impact of architecture and pre-training decisions. GPT is a left-to-right Transformer LM trained on BooksCorpus, whereas BERT uses BooksCorpus plus Wikipedia; GPT introduces [SEP] and [CLS] tokens only at fine-tuning, while BERT learns them during pre-training; GPT is trained for 1 million steps with a batch size of about 32,000 words, versus BERT’s 1 million steps with a batch size of about 128,000 words. GPT uses a fixed fine-tuning learning rate of 5e-5, whereas BERT uses task-specific fine-tuning learning rates that perform best on the development set. Ablation experiments cited in Section 5.1 show that most improvements arise from the two pre-training tasks and bidirectionality, rather than these other differences.

The appendix organization is threefold: Appendix A for additional implementation details, Appendix B for experimental details, and Appendix C for ablations, including Effects of Training Steps and Ablation for Different Masking Procedures. A.1 illustrates the pre-training tasks, detailing masked LM and the masking procedure; for example, randomly masking the 4th token in “my dog is hairy” and noting that random replacement occurs for 1.5% of all tokens (10% of 15%), so MLM predicts only 15% of tokens per batch, implying more pre-training steps may be required. This section also reiterates that the 15% token-prediction rate means more pre-training steps than standard language modeling. The GLUE fine-tuning discussion mirrors pre-training, with tasks including MNLI, QQP, QNLI, SST-2, CoLA, STS-B, MRPC, and RTE, while WINOGRAD NLI is excluded due to dataset issues and the tendency for GLUE submissions to predict the majority class. Hyperparameters for fine-tuning largely mirror pre-training, with batch sizes of 16 or 32, Adam learning rates of 5e-5, 3e-5, or 2e-5, and epochs of 2, 3, or 4, with dropout fixed at 0.1; larger datasets show less sensitivity, supporting an exhaustive search over these values to select the best development-set model.