BERT is presented with two phases: pre-training on unlabeled data using multiple pre-training tasks and fine-tuning where the pre-trained parameters are used to initialize task-specific models, each downstream task having its own fine-tuned model. A running question-answering example is used to illustrate the process. A distinctive feature is a unified architecture across tasks, with only minimal differences between pre-trained and downstream models. The model uses a multi-layer bidirectional Transformer encoder, following the original Vaswani et al. design, with L layers, hidden size H, and A self-attention heads; two sizes are reported: BERT BASE (L=12, H=768, A=12, total parameters 110M) and BERT LARGE (L=24, H=1024, A=16, total parameters 340M). BERT BASE is chosen to match OpenAI GPT in size, but unlike GPT, BERT employs bidirectional self-attention rather than left-to-right attention. For input/output, BERT can handle a single sentence or a pair of sentences by packing them into one sequence; WordPiece embeddings with a 30,000-token vocabulary are used. The first token [CLS] gives the aggregate sequence representation via its final hidden state, C ∈ R^H, for classification tasks. Sentence pairs are packed into a single sequence, separated by [SEP], and a learned embedding marks whether a token belongs to sentence A or B. The input embedding E is the sum of token, segment, and position embeddings, and the final hidden vector for the i-th token is its token representation, with the [CLS] token’s final vector C serving as the sequence representation.