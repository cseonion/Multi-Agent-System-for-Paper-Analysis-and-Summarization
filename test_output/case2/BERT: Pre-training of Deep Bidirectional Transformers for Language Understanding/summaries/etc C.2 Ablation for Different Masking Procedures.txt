Summary of Section C.2 Ablation for Different Masking Procedures

- Purpose and setup
  - Goal: Assess how different masking strategies during BERT’s MLM pre-training affect downstream fine-tuning, since the [MASK] token never appears during fine-tuning.
  - Strategies tested: MASK (replace target token with [MASK]), SAME (keep the original token), RND (replace with a random token).
  - BERT’s default pre-training mix is 80% MASK, 10% SAME, 10% RND.
  - Evaluation data: Dev results for MNLI and NER; for NER, both fine-tuning and feature-based approaches (the feature-based approach uses the last 4 BERT layers as features).

- Key findings
  - Fine-tuning is surprisingly robust to masking strategy changes.
  - For NER, using MASK-only masking is problematic when using the feature-based approach.
  - Using only RND masking performs worse than the mixed strategy, indicating the mixed approach is preferable.

- Implications
  - A mixed masking strategy (as in the default 80/10/10) helps reduce mismatch between pre-training and fine-tuning, especially for feature-based NER.
  - The results support the practice of not relying on a single masking method and caution against MASK-only masking in certain downstream setups.