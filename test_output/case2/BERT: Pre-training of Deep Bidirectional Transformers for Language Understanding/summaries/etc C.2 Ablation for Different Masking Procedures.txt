This ablation study (Section C.2) evaluates how different masking strategies during masked language model pre-training affect downstream performance, noting that the [MASK] symbol never appears during fine-tuning. The evaluation reports development results for MNLI and NER, with NER presented for both fine-tuning and a feature-based approach that concatenates the last four BERT layers, which was found optimal earlier. Masking strategies are defined as MASK (replace the target token with [MASK]), SAME (keep the token unchanged), and RND (replace with a random token), and the left part of the table shows the pre-training probabilities (BERT uses 80%, 10%, 10%). The right part shows the development results; the feature-based approach uses the last four layers as features. The findings indicate that fine-tuning is surprisingly robust to masking choices, but using only MASK is problematic for the feature-based NER, and using only RND performs substantially worse than the mixed strategy.