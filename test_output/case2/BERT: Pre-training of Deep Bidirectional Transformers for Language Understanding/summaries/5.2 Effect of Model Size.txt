Section 5.2 investigates how model size affects fine-tuning accuracy by training BERT variants with different numbers of layers, hidden units, and attention heads while keeping the same hyperparameters and training procedure as before. Table 6 shows the average development-set accuracy from five random restarts of fine-tuning on selected GLUE tasks, with larger models yielding strict accuracy improvements across all four datasets, including MRPC, which has only 3,600 labeled examples. The authors highlight that these gains occur atop already large models, noting that the largest Transformer in Vaswani et al. (2017) is L=6, H=1024, A=16 with 100M encoder parameters, while the largest known to date is L=64, H=512, A=2 with 235M parameters (Al-Rfou et al., 2018); in contrast, BERT BASE has 110M and BERT LARGE 340M parameters. They emphasize that while enlarging models has long been known to improve large-scale tasks (as reflected by LM perplexity in Table 6), this work convincingly shows that scaling to extreme sizes also yields substantial improvements on small-scale tasks given sufficient pre-training. The section contrasts these results with prior feature-based approaches (Peters et al., 2018b; Melamud et al., 2016), which reported mixed or limited gains when increasing pre-trained bi-LM size from two to four layers or from 200 to 600 hidden units, noting that those studies did not benefit from 1,000 hidden units; the authors argue that when fine-tuning relies on only a small number of randomly initialized task-specific parameters, larger, more expressive pre-trained representations can still boost performance even with very limited downstream data.