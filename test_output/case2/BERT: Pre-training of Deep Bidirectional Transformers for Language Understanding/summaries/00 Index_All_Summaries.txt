논문 제목: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
전체 섹션별 요약 인덱스
============================================================

섹션 1: Introduction
----------------------------------------
Please paste the raw text of Section 1 and, if available, the summary of any previous sections (for Section 1 there is none). I will produce a concise, paragraph-form summary that preserves all key points and numerical details, without bullet points. If there are specific length constraints or formatting preferences, let me know.

섹션 2: Related Work
----------------------------------------
The Related Work section acknowledges a long history of pre-training general language representations and briefly surveys the most widely used approaches in the field.

섹션 2.1: Unsupervised Feature-based Approaches
----------------------------------------
Unsupervised feature-based approaches to learning representations have evolved from early non-neural methods (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) to neural models (Mikolov et al., 2013; Pennington et al., 2014), with pre-trained word embeddings consistently delivering gains over scratch embeddings (Turian et al., 2010). Pretraining objectives include left-to-right language modeling (Mnih and Hinton, 2009) and discriminating correct versus incorrect words from left and right context (Mikolov et al., 2013). These ideas extend to coarser granularities, yielding sentence embeddings (Kiros et al., 2015; Logeswaran and Lee, 2018) and paragraph embeddings (Le and Mikolov, 2014). For training sentence representations, objectives have included ranking candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next-sentence words given the previous sentence representation (Kiros et al., 2015), and denoising autoencoder–derived objectives (Hill et al., 2016). ELMo and its predecessor (Peters et al., 2017; 2018) generalize embeddings by extracting context-sensitive features from both left-to-right and right-to-left language models; the contextual token representation is the concatenation of the two directions, and integrating these contextual embeddings with task-specific architectures advances state-of-the-art results on QA (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named-entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations by predicting a single word from both left and right contexts with LSTMs, a feature-based approach similar to ELMo but not deeply bidirectional. Fedus et al. (2018) shows that a cloze task can be used to improve the robustness of text generation models.

섹션 2.2: Unsupervised Fine-tuning Approaches
----------------------------------------
Like feature-based methods, the initial unsupervised fine-tuning work pre-trained word embedding parameters from unlabeled text (Collobert and Weston, 2008). More recently, sentence or document encoders that produce contextual token representations have been pre-trained on unlabeled text and then fine-tuned for supervised downstream tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The main advantage is that few parameters need to be learned from scratch. Consequently, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentence-level tasks in the GLUE benchmark (Wang et al., 2018a). The section also references GPT and BERT in the context of pre-training, contrasting left-to-right language models with bidirectional encoders (as exemplified by BERT) and notes that pre-training and auto-encoder objectives have been used for these models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).

섹션 2.3: Transfer Learning from Supervised Data
----------------------------------------
There is evidence that transferring from supervised tasks trained on large datasets yields strong results, notably natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). In computer vision, transfer learning is likewise effective, with a common recipe of fine-tuning models pre-trained on ImageNet (Deng et al., 2009; Yosinski et al., 2014).

섹션 3: BERT
----------------------------------------
BERT is presented with two phases: pre-training on unlabeled data using multiple pre-training tasks and fine-tuning where the pre-trained parameters are used to initialize task-specific models, each downstream task having its own fine-tuned model. A running question-answering example is used to illustrate the process. A distinctive feature is a unified architecture across tasks, with only minimal differences between pre-trained and downstream models. The model uses a multi-layer bidirectional Transformer encoder, following the original Vaswani et al. design, with L layers, hidden size H, and A self-attention heads; two sizes are reported: BERT BASE (L=12, H=768, A=12, total parameters 110M) and BERT LARGE (L=24, H=1024, A=16, total parameters 340M). BERT BASE is chosen to match OpenAI GPT in size, but unlike GPT, BERT employs bidirectional self-attention rather than left-to-right attention. For input/output, BERT can handle a single sentence or a pair of sentences by packing them into one sequence; WordPiece embeddings with a 30,000-token vocabulary are used. The first token [CLS] gives the aggregate sequence representation via its final hidden state, C ∈ R^H, for classification tasks. Sentence pairs are packed into a single sequence, separated by [SEP], and a learned embedding marks whether a token belongs to sentence A or B. The input embedding E is the sum of token, segment, and position embeddings, and the final hidden vector for the i-th token is its token representation, with the [CLS] token’s final vector C serving as the sequence representation.

섹션 3.1: Pre-training BERT
----------------------------------------
3.1 Pre-training BERT describes two unsupervised objectives used to train the model. Task 1 is Masked LM (MLM), where 15% of WordPiece tokens are masked at random and the model must predict the original tokens from the final hidden vectors of the masked positions. To mitigate pre-training–fine-tuning mismatch caused by the [MASK] token, the masking procedure does not always replace the chosen token with [MASK]: 80% of the time it becomes [MASK], 10% of the time it becomes a random token, and 10% of the time it remains unchanged, with cross-entropy loss predicting the original token. Task 2 is Next Sentence Prediction (NSP), training the model to determine whether sentence B follows sentence A, with 50% IsNext and 50% NotNext; the [CLS] final vector C is used for NSP. The pre-training data consist of BooksCorpus (800M words) and English Wikipedia (2,500M words), with Wikipedia passages stripped of lists, tables, and headers, and a document-level corpus emphasized to capture long contiguous sequences rather than a shuffled sentence corpus like the Billion Word Benchmark. The procedure largely follows prior literature, and variations of MLM are discussed in Appendix C.2; NSP is shown to benefit QA and NLI (Section 5.1).

섹션 3.2: Fine-tuning BERT
----------------------------------------
Fine-tuning is straightforward because the Transformer's self-attention lets BERT model many downstream tasks by swapping inputs and outputs. For text pairs, the common pattern is to independently encode the two texts before applying bidirectional cross attention, but BERT unifies these stages by concatenating the text pair and using self-attention, which effectively implements bidirectional cross attention between the two sentences. For each task, we plug in task-specific inputs and outputs and fine-tune all parameters end-to-end. At input, sentence A and sentence B correspond to (1) sentence pairs in paraphrasing, (2) hypothesis–premise pairs in entailment, (3) question–passage pairs in question answering, and (4) a degenerate text-∅ pair in text classification or sequence tagging. At output, token representations feed into an output layer for token-level tasks (sequence tagging or question answering), and the [CLS] representation feeds into an output layer for classification (entailment or sentiment analysis). Compared to pre-training, fine-tuning is relatively inexpensive; all results can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model. Task-specific details are described in Section 4, with additional details in Appendix A.5.

섹션 4: Experiments
----------------------------------------
This section presents the fine-tuning results of BERT on 11 natural language processing tasks.

섹션 4.1: GLUE
----------------------------------------
GLUE is a collection of diverse natural language understanding tasks. For fine-tuning, we represent the input sequence (single sentence or sentence pairs) as in Section 3 and use the final hidden vector C ∈ R^H from the [CLS] token as the aggregate representation. The new parameters introduced during fine-tuning are the classification weights W ∈ R^{K×H}, where K is the number of labels, and the loss is log(softmax(CW^T)). We use a batch size of 32 and fine-tune for 3 epochs on all GLUE tasks. For each task, the best fine-tuning learning rate is selected on the Dev set from {5e-5, 4e-5, 3e-5, 2e-5}. For BERT-LARGE, finetuning is sometimes unstable on small datasets, so several random restarts are performed with the same pre-trained checkpoint but different fine-tuning data shuffling and classifier initializations, selecting the best Dev model. Nine results are reported in Table 1. Both BERT-BASE and BERT-LARGE outperform all prior systems on all tasks by substantial margins, achieving 4.5% and 7.0% average accuracy improvements, respectively. BERT-BASE and OpenAI GPT are nearly identical architecturally apart from attention masking; for MNLI, BERT achieves a 4.6% absolute accuracy gain. On the official GLUE leaderboard, BERT-LARGE scores 80.5 versus 72.8 for OpenAI GPT. BERT-LARGE consistently outperforms BERT-BASE across tasks, especially those with limited data, with the model-size effect further explored in Section 5.2.

섹션 4.2: SQuAD v1.1
----------------------------------------
The SQuAD v1.1 section describes a dataset of 100k crowdsourced question/answer pairs where the task is to predict the exact answer span in a passage given a question and the passage embedded as a single packed sequence, with the question using an A embedding and the passage a B embedding. During fine-tuning, a start vector S and an end vector E are introduced; the probability that position i is the start of the answer is computed as a dot product S·T_i followed by a softmax over all positions, and the end probability is computed similarly with E. The score of a candidate span i..j is S·T_i + E·T_j with j ≥ i, and the model predicts the maximum-scoring valid span. The training objective is the sum of the log-likelihoods of the correct start and end positions. They fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32, and employ modest data augmentation by first fine-tuning on TriviaQA (TriviaQA-Wiki paragraphs formed of the first 400 tokens that contain at least one possible answer) before SQuAD. Their best system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system, with the single BERT model surpassing the top ensemble. For SQuAD v2.0, which adds the possibility of no answer, they extend the model by treating questions without an answer as having an answer span at the [CLS] token; the no-answer score is s_null = S·C + E·C, while the best non-null span ŝ_i,j = max_i≤j S·T_i + E·T_j, and a non-null answer is predicted when ŝ_i,j > s_null + τ, with τ chosen on the dev set to maximize F1. This v2.0 model does not use TriviaQA data, is fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48, and, when compared to prior leaderboard entries and top published work (excluding systems that use BERT as a component), achieves a +5.1 F1 improvement over the previous best.

섹션 4.4: SWAG
----------------------------------------
SWAG (Situations With Adversarial Generations) encompasses 113k sentence-pair completion examples that test grounded commonsense inference, where the task is to select the most plausible continuation among four choices for a given sentence A and continuation B. For fine-tuning, four input sequences are created, each combining A with one of the four possible B continuations; the model learns via a single task-specific parameter—a vector whose dot product with the [CLS] token representation C yields a score for each choice, normalized with softmax. The model is fine-tuned for 3 epochs with a learning rate of 2e-5 and a batch size of 16, and results (Table 4) show that BERT LARGE outperforms the baseline ESIM+ELMo by +27.1% and OpenAI GPT by 8.3%.

섹션 5: Ablation Studies
----------------------------------------
Section 5 reports ablations to assess the relative importance of BERT’s pre-training components. Using the BERT BASE architecture, the development-set results across MNLI-m, QNLI, MRPC, SST-2, and SQuAD are: BERTBASE 84.4, 88.4, 86.7, 92.7, 88.5; No NSP 83.9, 84.9, 86.5, 92.6, 87.9; LTR & No NSP 82.1, 84.3, 77.5, 92.1, 77.8; and + BiLSTM 82.1, 84.1, 75.7, 91.6, 84.9. Table 5 caption notes that this is an ablation over the pre-training tasks, and that No NSP means training without the next sentence prediction task, while LTR & No NSP denotes a left-to-right LM without NSP (like OpenAI GPT); + BiLSTM adds a randomly initialized BiLSTM on top of the LTR + No NSP model during fine-tuning. Ablation studies are available in Appendix C.

섹션 5.1: Effect of Pre-training Tasks
----------------------------------------
Section 5.1 investigates the impact of pre-training objectives by evaluating two schemes using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as BERT BASE: No NSP, a bidirectional model trained with masked LM but without the next sentence prediction task, and a left-context-only model trained with a standard left-to-right LM, with the left-context constraint also applied at fine-tuning to avoid pre-train/fine-tune mismatch, making it directly comparable to OpenAI GPT but with their data, inputs, and fine-tuning. They first assess the NSP contribution: Table 5 shows that removing NSP degrades performance significantly on QNLI, MNLI, and SQuAD 1.1. They then compare No NSP to LTR & No NSP: the LTR model underperforms the MLM model on all tasks, with large drops on MRPC and SQuAD. For SQuAD, token-level predictions lack right-side context, so performance suffers; adding a randomly initialized BiLSTM on top yields a notable improvement on SQuAD but still falls far short of pretrained bidirectional models, while the BiLSTM harms GLUE results. The authors note potential alternatives, such as training separate LTR and RTL models and concatenating representations (as ELMo does), but this is twice as expensive as a single bidirectional model, is not intuitive for QA since the RTL path cannot condition on the question, and is strictly less powerful than a deep bidirectional model that can use both left and right context at every layer.

섹션 5.2: Effect of Model Size
----------------------------------------
Section 5.2 investigates how model size affects fine-tuning accuracy by training BERT variants with different numbers of layers, hidden units, and attention heads while keeping the same hyperparameters and training procedure as before. Table 6 shows the average development-set accuracy from five random restarts of fine-tuning on selected GLUE tasks, with larger models yielding strict accuracy improvements across all four datasets, including MRPC, which has only 3,600 labeled examples. The authors highlight that these gains occur atop already large models, noting that the largest Transformer in Vaswani et al. (2017) is L=6, H=1024, A=16 with 100M encoder parameters, while the largest known to date is L=64, H=512, A=2 with 235M parameters (Al-Rfou et al., 2018); in contrast, BERT BASE has 110M and BERT LARGE 340M parameters. They emphasize that while enlarging models has long been known to improve large-scale tasks (as reflected by LM perplexity in Table 6), this work convincingly shows that scaling to extreme sizes also yields substantial improvements on small-scale tasks given sufficient pre-training. The section contrasts these results with prior feature-based approaches (Peters et al., 2018b; Melamud et al., 2016), which reported mixed or limited gains when increasing pre-trained bi-LM size from two to four layers or from 200 to 600 hidden units, noting that those studies did not benefit from 1,000 hidden units; the authors argue that when fine-tuning relies on only a small number of randomly initialized task-specific parameters, larger, more expressive pre-trained representations can still boost performance even with very limited downstream data.

섹션 5.3: Feature-based Approach with BERT
----------------------------------------
Section 5.3 contrasts fine-tuning with a feature-based approach for BERT by applying both to the CoNLL-2003 Named Entity Recognition task. It first notes that fixed, pre-computed representations can be advantageous when a task cannot be easily expressed by a Transformer encoder or when expensive representations are reusable across many experiments with cheaper downstream models. For the experiment, BERT is fed a case-preserving WordPiece input with maximal document context, treated as a tagging task without a CRF, to compare methods.

The ablation over model size (Table 6) shows that larger configurations improve both the masked LM perplexity and downstream development-set accuracy on MNLI-m, MRPC, and SST-2. Specifically, with 3 layers, 768 hidden units, and 12 attention heads the perplexity is 5.84 and dev accuracies are 77.9 (MNLI-m), 79.8 (MRPC), 88.4 (SST-2). Increasing to 6 layers with 3 heads yields 5.24, 80.6, 82.2, 90.7; with 6 layers and 12 heads: 4.68, 81.9, 84.8, 91.3. A 12-layer, 768/12 setup gives 3.99, 84.4, 86.7, 92.9; 12 layers with 1024 hidden and 16 heads improves to 3.54, 85.7, 86.9, 93.3; and 24 layers with 1024/16 yields 3.23, 86.6, 87.8, 93.7. These results demonstrate that larger, more capable pre-trained representations yield stronger performance across these tasks, including smaller datasets.

Table 7 compares fine-tuning versus a feature-based approach for CoNLL-2003. Using BERT-LARGE for fine-tuning attains Dev F1 96.6 and Test F1 92.8, while BERT-BASE yields 96.4 Dev F1 and 92.4 Test F1. In the feature-based setup with BERT-BASE, embeddings alone score 91.0 (Dev F1); representations from the second-to-last hidden layer reach 95.6; last hidden 94.9; a weighted sum of the last four hidden layers 95.9; concatenating the last four hidden layers 96.1; and a weighted sum of all 12 layers 95.5. The best feature-based configuration—concatenating the top four layers—achieves 96.1 Dev F1, only about 0.3 F1 behind full fine-tuning, illustrating that BERT is effective in both paradigms.

섹션 6: Conclusion
----------------------------------------
Recent empirical improvements from transfer learning with language models show that rich, unsupervised pre-training is integral to many language-understanding systems, enabling even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is to generalize these findings to deep bidirectional architectures, allowing the same pre-trained model to tackle a broad set of NLP tasks. In the MLM versus left-to-right comparison, MLM converges marginally slower than a left-to-right model, but its empirical gains far outweigh the extra training cost. The Next Sentence Prediction task is illustrated with examples showing how two spans are sampled from the corpus, with 50% of the time B being the actual next sentence and 50% a random sentence, provided the combined length is ≤ 512 tokens; the input is tokenized, masked at 15% with WordPiece, and the model uses a sum of the masked LM likelihood and NSP likelihood as the training loss. The A.2 Pre-training Procedure describes generating training sequences by sampling two spans A and B, labeling B as Next or NotNext accordingly; longer sequences are expensive due to quadratic attention, so to speed pre-training the model is first trained with sequence length 128 for 90% of steps, then with length 512 for the remaining 10% to learn positional embeddings. Training details include batch size 256, 1,000,000 steps (~40 epochs over a 3.3 billion word corpus), Adam with lr 1e-4, β1=0.9, β2=0.999, L2 weight decay 0.01, warmup for the first 10,000 steps, and linear decay, with dropout 0.1 and gelu activation. BERT BASE was trained on 4 Cloud TPUs (Pod configuration, 16 TPUs) for 4 days, and BERT LARGE on 16 Cloud TPUs (64 chips) for 4 days.

섹션 etc: A.3 Fine-tuning Procedure
----------------------------------------
Section A.3 reports an ablation study on masking strategies used with the MLM objective to reduce pre-training–finetuning mismatch, since [MASK] never appears during finetuning. Dev results for MNLI and NER show that finetuning is surprisingly robust to masking choices; however, using only the MASK strategy hurts the feature-based NER approach, and using only the RND strategy performs worse than the authors’ combined strategy. The OpenAI GPT comparison discusses how GPT and BERT differ beyond architecture: GPT is a left-to-right LM trained on BooksCorpus (800M words) with 1M steps at a batch size of about 32,000 words and a fixed fine-tuning learning rate of 5e-5, whereas BERT is trained on BooksCorpus plus Wikipedia (2,500M words) with 1M steps at a batch size of about 128,000 words and task-specific fine-tuning learning rates; BERT also learns [SEP] and [CLS] tokens and sentence A/B embeddings during pre-training. Ablation experiments in Section 5.1 indicate that most improvements arise from the two pre-training tasks and bidirectionality rather than other differences.

The Appendix outline clarifies organization into three parts: Appendix A with additional implementation details, Appendix B with experimental details, and Appendix C with ablations, including Effects of Training Steps and Ablation for Different Masking Procedures. A.1 illustrates pre-training tasks and Masked LM masking: for example, with the sentence my dog is hairy and random masking selects the 4th token (hairy), the masking procedure forces the encoder to maintain contextual representations while the random replacement occurs for 1.5% of all tokens (10% of 15%), implying more pre-training steps may be needed since masked LM predicts only 15% of tokens per batch. The illustration also notes that MLM uses 15% token predictions, making pre-training longer than standard LM.

The Appendix also discusses fine-tuning on diverse tasks, with GLUE covering MNLI, QQP, QNLI, SST-2, CoLA, STS-B, MRPC, RTE, and WINOGRAD NLI; however, GLUE reports issues with WINOGRAD NLI, leading to its exclusion to avoid unfair advantage for GPT, and in GLUE submissions the majority class is predicted. For fine-tuning, hyperparameters largely mirror pre-training with adjustments: batch size, learning rate (Adam) in the ranges of 5e-5, 3e-5, 2e-5, and epochs in {2, 3, 4}, with dropout fixed at 0.1; larger data sets are less sensitive to these choices, so an exhaustive search over these values is recommended.

섹션 etc: A.4 Comparison of BERT, ELMo ,and
----------------------------------------
This section conducts an ablation study on the masking strategies used with BERT’s masked language model (MLM) objective to reduce pre-training–finetuning mismatch, since the [MASK] token never appears during fine-tuning. Dev results for MNLI and NER are reported, with NER results given for both fine-tuning and a feature-based approach since mismatch is expected to be amplified for the latter; results are shown in Table 8. In the table, MASK replaces the target token with [MASK], SAME keeps the token unchanged, and RND replaces the target with a random token, with BERT using the 80/10/10 split for these strategies. For the feature-based NER, the last four layers of BERT are used as features, as was found optimal earlier. The findings show that fine-tuning is surprisingly robust to masking choices; however, using only MASK hurts the feature-based NER, and using only RND performs worse than the authors’ combined strategy.

In the OpenAI GPT discussion, the authors compare GPT with ELMo and BERT to isolate the impact of architecture and pre-training decisions. GPT is a left-to-right Transformer LM trained on BooksCorpus, whereas BERT uses BooksCorpus plus Wikipedia; GPT introduces [SEP] and [CLS] tokens only at fine-tuning, while BERT learns them during pre-training; GPT is trained for 1 million steps with a batch size of about 32,000 words, versus BERT’s 1 million steps with a batch size of about 128,000 words. GPT uses a fixed fine-tuning learning rate of 5e-5, whereas BERT uses task-specific fine-tuning learning rates that perform best on the development set. Ablation experiments cited in Section 5.1 show that most improvements arise from the two pre-training tasks and bidirectionality, rather than these other differences.

The appendix organization is threefold: Appendix A for additional implementation details, Appendix B for experimental details, and Appendix C for ablations, including Effects of Training Steps and Ablation for Different Masking Procedures. A.1 illustrates the pre-training tasks, detailing masked LM and the masking procedure; for example, randomly masking the 4th token in “my dog is hairy” and noting that random replacement occurs for 1.5% of all tokens (10% of 15%), so MLM predicts only 15% of tokens per batch, implying more pre-training steps may be required. This section also reiterates that the 15% token-prediction rate means more pre-training steps than standard language modeling. The GLUE fine-tuning discussion mirrors pre-training, with tasks including MNLI, QQP, QNLI, SST-2, CoLA, STS-B, MRPC, and RTE, while WINOGRAD NLI is excluded due to dataset issues and the tendency for GLUE submissions to predict the majority class. Hyperparameters for fine-tuning largely mirror pre-training, with batch sizes of 16 or 32, Adam learning rates of 5e-5, 3e-5, or 2e-5, and epochs of 2, 3, or 4, with dropout fixed at 0.1; larger datasets show less sensitivity, supporting an exhaustive search over these values to select the best development-set model.

섹션 etc: A.5 Illustrations of Fine-tuning on Different Tasks
----------------------------------------
This ablation study evaluates masking strategies used with BERT’s masked language modeling (MLM) objective to reduce pre-training–finetuning mismatch because the [MASK] token never appears during fine-tuning. Dev results are reported for MNLI and NER; for NER, both fine-tuning and a feature-based approach (concatenating the last four BERT layers) are shown, with results in Table 8. The MLM masking schemes are MASK (replace with [MASK]), SAME (keep the token), and RND (random token), with BERT’s pre-training using an 80/10/10 split; for the feature-based NER, the last four layers were found optimal. The results indicate that fine-tuning is robust to masking choices, but MASK-only hurts the feature-based NER, and RND-only underperforms relative to the mixed strategy. In the OpenAI GPT comparison, GPT is a left-to-right Transformer LM trained on BooksCorpus, whereas BERT uses BooksCorpus plus Wikipedia; GPT introduces [SEP] and [CLS] tokens only at fine-tuning, while BERT learns them during pre-training; GPT is trained for 1 million steps with a batch size of about 32,000 words, versus BERT’s 1 million steps with a batch size of about 128,000 words; GPT’s fine-tuning learning rate is fixed at 5e-5, whereas BERT uses task-specific fine-tuning learning rates that are tuned on the development set. Ablation experiments cited show that most improvements derive from the two pre-training tasks and bidirectionality, rather than these other design differences. The Appendix is organized into three sections (A for implementation details, B for experimental details, C for ablations including Effect of Training Steps and Ablation for Different Masking Procedures). A.1 illustrates the pre-training tasks with examples of masked LM; for random masking, selecting the 4th token in “my dog is hairy” and replacing 1.5% of all tokens (10% of 15%) means MLM predicts only 15% of tokens per batch, implying more pre-training steps are needed. This 15% token-prediction rate motivates longer pre-training than standard language modeling. The GLUE fine-tuning discussion mirrors pre-training, with datasets MNLI, QQP, QNLI, SST-2, CoLA, STS-B, MRPC, and RTE (WINOGRAD NLI excluded due to dataset issues and GLUE’s majority-class baseline; for GLUE submissions they predict the majority class). Hyperparameters for fine-tuning largely mirror pre-training, with batch sizes of 16 or 32, Adam learning rates of 5e-5, 3e-5, or 2e-5, epochs of 2, 3, or 4, and dropout fixed at 0.1; larger datasets show less sensitivity, justifying an exhaustive search over these values to select the dev-best model. The illustration of fine-tuning on different tasks is given in Figure 4, and the GLUE tasks are as listed above, with the stated hyperparameter ranges commonly effective across tasks.

섹션 etc: STS-B
----------------------------------------
This section presents an ablation study of masking strategies used with BERT’s masked language modeling objective to reduce pre-training–finetuning mismatch, noting that the [MASK] token never appears during fine-tuning. Dev results are reported for MNLI and NER, with NER results shown for both fine-tuning and a feature-based approach that concatenates the last four BERT layers. The masking schemes evaluated are MASK (replace with [MASK]), SAME (keep the token), and RND (random token), with the pre-training masking distribution of 80%, 10%, 10% as in the left part of Table 8. The right part shows dev results, and for the feature-based NER the last four layers are used as features, which was found optimal earlier. The results indicate that fine-tuning is robust to masking choices, but using only MASK hurts the feature-based NER, and using only RND underperforms compared with the mixed strategy.

In the OpenAI GPT comparison, the section contrasts GPT with BERT to isolate the impact of architectural and training differences. GPT is a left-to-right Transformer LM trained on BooksCorpus (800M words), while BERT uses BooksCorpus plus Wikipedia (2,500M words) and learns special tokens like [SEP] and [CLS] during pre-training, whereas GPT introduces them only at fine-tuning. GPT is trained for 1M steps with a batch size of about 32,000 words; BERT uses 1M steps with a batch size of about 128,000 words. GPT fine-tuning uses a fixed learning rate of 5e-5, whereas BERT uses task-specific fine-tuning learning rates tuned on the development set. Ablation experiments show that most improvements arise from the two pre-training tasks and bidirectionality rather than these design differences.

The Appendix for the BERT paper is organized into three sections: A for implementation details, B for experimental details, and C for ablations, including “Effect of Number of Training Steps” and “Ablation for Different Masking Procedures.” A.1 illustrates the pre-training tasks with examples of masked LM and masking procedures. It explains that random masking selects the 4th token in an example and replaces 1.5% of all tokens (10% of 15%), meaning the MLM predicts only 15% of tokens per batch, which motivates longer pre-training. This justification underpins the statement that the masked LM predicts 15% of tokens per batch, suggesting more pre-training steps are required. The section reiterates that BERT uses the mixed masking strategy with 80/10/10 proportions, and that results for MNLI, QQP, QNLI, SST-2, CoLA, STS-B, MRPC, and RTE are used for GLUE fine-tuning, with Winograd NLI excluded due to data issues. Fine-tuning hyperparameters largely mirror pre-training, with batch sizes of 16 or 32, Adam learning rates of 5e-5, 3e-5, or 2e-5, epochs of 2, 3, or 4, and dropout fixed at 0.1; larger datasets show less sensitivity, justifying an exhaustive dev-based search to select the best model.

섹션 etc: A Additional Details for BERT
----------------------------------------
This section presents an ablation on masking strategies for BERT’s masked language model (MLM) pre-training to reduce pre-training–finetuning mismatch, noting that the [MASK] symbol never appears during fine-tuning; dev results are reported for MNLI and NER, with NER results shown for both fine-tuning and a feature-based approach that concatenates the last four BERT layers, since that was found optimal earlier, and the table indicates that while fine-tuning is robust to masking choices, using only MASK hurts the feature-based NER, and using only RND underperforms compared with the mixed strategy (80/10/10). The OpenAI GPT comparison isolates architectural and training differences, showing GPT is a left-to-right Transformer LM trained on BooksCorpus (800M words) whereas BERT uses BooksCorpus plus Wikipedia (2,500M words) and learns special tokens like [SEP] and [CLS] during pre-training (unlike GPT, which introduces them at fine-tuning); GPT is trained for 1M steps with a batch size of about 32,000 words, while BERT uses 1M steps with a batch size of about 128,000 words, and GPT fine-tuning uses a fixed learning rate of 5e-5 whereas BERT uses task-specific fine-tuning learning rates; ablations show that most improvements arise from the two pre-training tasks and bidirectionality rather than these design differences. The Appendix is organized into three sections: A for implementation details, B for experimental details, and C for ablations, including “Effect of Number of Training Steps” and “Ablation for Different Masking Procedures.” A.1 illustrates the pre-training tasks with masked LM and the masking procedure, explaining that random masking selects the 4th token and replaces 1.5% of all tokens (10% of 15%), so MLM predicts 15% of tokens per batch, which motivates longer pre-training; this underpins the statement that masked LM predicts 15% per batch and justifies more pre-training steps, reiterating that BERT uses the 80/10/10 mixed masking distribution, and that fine-tuning GLUE results (MNLI, QQP, QNLI, SST-2, CoLA, STS-B, MRPC, and RTE; Winograd NLI excluded due to data issues) are obtained with fine-tuning hyperparameters largely mirroring pre-training, with batch sizes 16 or 32, Adam learning rates 5e-5, 3e-5, or 2e-5, epochs 2, 3, or 4, and dropout fixed at 0.1; larger datasets show less sensitivity, justifying an exhaustive dev-based search, and a note that GLUE submissions predict the majority class for Winograd NLI.

섹션 etc: C.2 Ablation for Different Masking Procedures
----------------------------------------
This ablation study (Section C.2) evaluates how different masking strategies during masked language model pre-training affect downstream performance, noting that the [MASK] symbol never appears during fine-tuning. The evaluation reports development results for MNLI and NER, with NER presented for both fine-tuning and a feature-based approach that concatenates the last four BERT layers, which was found optimal earlier. Masking strategies are defined as MASK (replace the target token with [MASK]), SAME (keep the token unchanged), and RND (replace with a random token), and the left part of the table shows the pre-training probabilities (BERT uses 80%, 10%, 10%). The right part shows the development results; the feature-based approach uses the last four layers as features. The findings indicate that fine-tuning is surprisingly robust to masking choices, but using only MASK is problematic for the feature-based NER, and using only RND performs substantially worse than the mixed strategy.

============================================================
총 섹션 수: 23
생성 시간: 2025-08-19 08:32:43
