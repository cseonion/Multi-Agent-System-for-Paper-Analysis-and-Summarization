Here is a focused summary of the current section (A.5 Illustrations of Fine-tuning on Different Tasks and the related appendix material):

- Masking strategy ablation (pre-training vs. fine-tuning)
  - Purpose: evaluate how different MLM masking strategies during pre-training affect downstream fine-tuning, since fine-tuning never sees the [MASK] token.
  - Strategies tested: MASK (replace with [MASK]), SAME (keep token), RND (random token).
  - BERT’s pre-training mix: 80% MASK, 10% SAME, 10% RND.
  - Findings:
    - Fine-tuning is surprisingly robust to masking variations.
    - MASK-only is problematic for NER when using the feature-based approach.
    - RND-only performs worse than the mixed strategy.
  - Implication: a mixed masking strategy is preferable to reduce pre-training/fine-tuning mismatch, with caveats for feature-based NER.

- OpenAI GPT vs BERT comparison
  - Positioning: GPT (OpenAI) and BERT are fine-tuning based; ELMo is feature-based.
  - Key differences (highlights):
    - Data: GPT uses BooksCorpus (800M words); BERT uses BooksCorpus (800M) + Wikipedia (2.5B words).
    - Special tokens: GPT uses SEP/CLS only at fine-tuning; BERT learns SEP/CLS and sentence embeddings during pre-training.
    - Training regime: GPT ~1M steps with large batch (32,000 words); BERT ~1M steps with 128,000-word batches.
    - Fine-tuning learning rate: GPT uses a fixed 5e-5; BERT uses task-specific fine-tuning learning rates.
  - Main takeaway: most improvements come from the two pre-training tasks (MLM and NSP) and bidirectionality, not architectural/training differences between GPT and BERT. Section 5.1 ablations support this.

- Appendix structure and purpose
  - Organization: Appendix A (implementation details), Appendix B (experiment details), Appendix C (additional ablations).
  - Focus of additional ablations: effect of training steps; ablation for different masking procedures.
  - Goal: provide concrete methodological details to support the main results.

- A.1 Illustration of the Pre-training Tasks
  - Content: concrete examples of MLM and masking procedures.
  - Example used: “my dog is hairy,” masking the 4th token (hairy).
  - Rationale: masking/random replacement forces the Transformer to maintain distributional contextual representations without knowing which words will be predicted; random replacements occur for 1.5% of all tokens (i.e., 10% of the 15% masked tokens) to avoid harming language understanding.
  - Implication: MLM predicts about 15% of tokens per batch, suggesting more pre-training steps may be needed for convergence.
  - Note: reinforces why the 80/10/10 masking mix is examined via ablations.

- Fine-tuning on GLUE tasks and practical details (Appendix content)
  - Fine-tuning setup largely preserves pre-training hyperparameters, with task-specific adjustments to batch size, learning rate, and number of epochs; dropout fixed at 0.1.
  - Hyperparameter ranges that work well across tasks:
    - Batch size: 16, 32
    - Adam learning rate: 5e-5, 3e-5, 2e-5
    - Number of epochs: 2, 3, 4
  - Observations: large datasets (≥100k labeled examples) are less sensitive to hyperparameters than small datasets.
  - GLUE tasks covered: MNLI, QQP, QNLI, SST-2, CoLA, STS, MRPC, RTE; WNLI is excluded from GLUE submissions (majority-class baseline used for those cases).
  - Training scale and hardware:
    - Pre-training data: ~3.3 billion words
    - Duration: about 4 days per run
    - Hardware: BERT-Base on 4 Cloud TPUs; BERT-Large on 16 Cloud TPUs
    - Setup: ~1,000,000 pre-training steps (~40 epochs); batch size 256; sequence length 512 (with a mix of shorter/longer sequence steps)
  - Practical conclusion: MLM + NSP with bidirectionality yields strong, transferable representations; longer sequences help learning at higher compute cost; hyperparameters are reasonably robust, especially with larger datasets.

- Overall takeaways for this section
  - Masking ablations show fine-tuning is robust to masking choices, with caveats for feature-based NER using MASK-only.
  - The GPT vs. BERT discussion emphasizes that gains come mainly from the pre-training objectives (MLM/NSP) and bidirectionality, not just architectural differences.
  - The appendix provides concrete methodological detail on masking illustrations, GLUE fine-tuning setups, hyperparameters, dataset scope, and hardware scale, underscoring practical feasibility and generalizability of BERT.
  - Practically, MLM + NSP with bidirectional context yields strong transferable representations; longer sequences improve learning at a computational cost; hyperparameters are robust across tasks, particularly in data-rich regimes.