This section presents an ablation study of masking strategies used with BERT’s masked language modeling objective to reduce pre-training–finetuning mismatch, noting that the [MASK] token never appears during fine-tuning. Dev results are reported for MNLI and NER, with NER results shown for both fine-tuning and a feature-based approach that concatenates the last four BERT layers. The masking schemes evaluated are MASK (replace with [MASK]), SAME (keep the token), and RND (random token), with the pre-training masking distribution of 80%, 10%, 10% as in the left part of Table 8. The right part shows dev results, and for the feature-based NER the last four layers are used as features, which was found optimal earlier. The results indicate that fine-tuning is robust to masking choices, but using only MASK hurts the feature-based NER, and using only RND underperforms compared with the mixed strategy.

In the OpenAI GPT comparison, the section contrasts GPT with BERT to isolate the impact of architectural and training differences. GPT is a left-to-right Transformer LM trained on BooksCorpus (800M words), while BERT uses BooksCorpus plus Wikipedia (2,500M words) and learns special tokens like [SEP] and [CLS] during pre-training, whereas GPT introduces them only at fine-tuning. GPT is trained for 1M steps with a batch size of about 32,000 words; BERT uses 1M steps with a batch size of about 128,000 words. GPT fine-tuning uses a fixed learning rate of 5e-5, whereas BERT uses task-specific fine-tuning learning rates tuned on the development set. Ablation experiments show that most improvements arise from the two pre-training tasks and bidirectionality rather than these design differences.

The Appendix for the BERT paper is organized into three sections: A for implementation details, B for experimental details, and C for ablations, including “Effect of Number of Training Steps” and “Ablation for Different Masking Procedures.” A.1 illustrates the pre-training tasks with examples of masked LM and masking procedures. It explains that random masking selects the 4th token in an example and replaces 1.5% of all tokens (10% of 15%), meaning the MLM predicts only 15% of tokens per batch, which motivates longer pre-training. This justification underpins the statement that the masked LM predicts 15% of tokens per batch, suggesting more pre-training steps are required. The section reiterates that BERT uses the mixed masking strategy with 80/10/10 proportions, and that results for MNLI, QQP, QNLI, SST-2, CoLA, STS-B, MRPC, and RTE are used for GLUE fine-tuning, with Winograd NLI excluded due to data issues. Fine-tuning hyperparameters largely mirror pre-training, with batch sizes of 16 or 32, Adam learning rates of 5e-5, 3e-5, or 2e-5, epochs of 2, 3, or 4, and dropout fixed at 0.1; larger datasets show less sensitivity, justifying an exhaustive dev-based search to select the best model.