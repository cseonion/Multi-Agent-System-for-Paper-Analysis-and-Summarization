Summary of Section 5.1. Effect of Pre-training Tasks

- Objective and setup
  - Goal: quantify how pretraining objectives—especially NSP—and bidirectionality affect performance, while keeping the pretraining data, fine-tuning scheme, and hyperparameters identical to BERT BASE.
  - Models compared:
    - No NSP: bidirectional MLM pretraining without the next sentence prediction task.
    - LTR & No NSP: left-to-right language modeling without NSP (GPT-like); fine-tuning also constrained to left-to-right to avoid pretrain/fine-tune mismatch.
    - + BiLSTM: same as LTR & No NSP, but with a randomly initialized BiLSTM added on top during fine-tuning to try to bolster token-level predictions (not a true bidirectional pretraining).
  - Rationale: NSP removal isolates the effect of the NSP objective; replacing MLM bidirectionality with an LTR model tests the value of bidirectional context; the BiLSTM tests whether a post-hoc augmentation can compensate.

- Key findings
  - NSP impact: Removing NSP hurts performance on QNLI, MNLI, and SQuAD 1.1, indicating NSP contributes positively to downstream results.
  - Bidirectionality vs. LTR: The LTR & No NSP setup underperforms the MLM baseline across all tasks, with notably large declines on MRPC and SQuAD, highlighting the importance of deep bidirectional representations.
  - BiLSTM augmentation: Adding a BiLSTM on top of LTR+No NSP improves SQuAD results substantially, but overall performance remains far below pretrained bidirectional models, and it can hurt performance on some GLUE tasks (MRPC, MNLI, QNLI, SST-2).
  - Efficiency and design considerations: While it’s possible to concatenate separate LTR and RTL models (as in ELMo), this approach is twice as expensive as a single bidirectional model, is less intuitive for QA (RTL cannot condition on questions), and is strictly less powerful than a true deep bidirectional model.

- Takeaway
  - NSP contributes to performance; removing it harms several tasks.
  - Replacing bidirectional MLM with left-to-right pretraining (without NSP) is detrimental across tasks.
  - A BiLSTM on top of LTR can help in specific tasks (notably SQuAD) but does not achieve the gains of genuine bidirectional pretraining and can harm some GLUE tasks.
  - Overall, deep bidirectional pretraining is superior to both MLM-without-NSP and LTR-only approaches.

- Note: Additional ablation details are provided in Appendix C.