Section 5.1 investigates the impact of pre-training objectives by evaluating two schemes using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as BERT BASE: No NSP, a bidirectional model trained with masked LM but without the next sentence prediction task, and a left-context-only model trained with a standard left-to-right LM, with the left-context constraint also applied at fine-tuning to avoid pre-train/fine-tune mismatch, making it directly comparable to OpenAI GPT but with their data, inputs, and fine-tuning. They first assess the NSP contribution: Table 5 shows that removing NSP degrades performance significantly on QNLI, MNLI, and SQuAD 1.1. They then compare No NSP to LTR & No NSP: the LTR model underperforms the MLM model on all tasks, with large drops on MRPC and SQuAD. For SQuAD, token-level predictions lack right-side context, so performance suffers; adding a randomly initialized BiLSTM on top yields a notable improvement on SQuAD but still falls far short of pretrained bidirectional models, while the BiLSTM harms GLUE results. The authors note potential alternatives, such as training separate LTR and RTL models and concatenating representations (as ELMo does), but this is twice as expensive as a single bidirectional model, is not intuitive for QA since the RTL path cannot condition on the question, and is strictly less powerful than a deep bidirectional model that can use both left and right context at every layer.