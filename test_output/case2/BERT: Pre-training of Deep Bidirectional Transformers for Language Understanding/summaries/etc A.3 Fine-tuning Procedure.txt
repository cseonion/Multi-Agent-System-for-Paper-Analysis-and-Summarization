Section A.3 reports an ablation study on masking strategies used with the MLM objective to reduce pre-training–finetuning mismatch, since [MASK] never appears during finetuning. Dev results for MNLI and NER show that finetuning is surprisingly robust to masking choices; however, using only the MASK strategy hurts the feature-based NER approach, and using only the RND strategy performs worse than the authors’ combined strategy. The OpenAI GPT comparison discusses how GPT and BERT differ beyond architecture: GPT is a left-to-right LM trained on BooksCorpus (800M words) with 1M steps at a batch size of about 32,000 words and a fixed fine-tuning learning rate of 5e-5, whereas BERT is trained on BooksCorpus plus Wikipedia (2,500M words) with 1M steps at a batch size of about 128,000 words and task-specific fine-tuning learning rates; BERT also learns [SEP] and [CLS] tokens and sentence A/B embeddings during pre-training. Ablation experiments in Section 5.1 indicate that most improvements arise from the two pre-training tasks and bidirectionality rather than other differences.

The Appendix outline clarifies organization into three parts: Appendix A with additional implementation details, Appendix B with experimental details, and Appendix C with ablations, including Effects of Training Steps and Ablation for Different Masking Procedures. A.1 illustrates pre-training tasks and Masked LM masking: for example, with the sentence my dog is hairy and random masking selects the 4th token (hairy), the masking procedure forces the encoder to maintain contextual representations while the random replacement occurs for 1.5% of all tokens (10% of 15%), implying more pre-training steps may be needed since masked LM predicts only 15% of tokens per batch. The illustration also notes that MLM uses 15% token predictions, making pre-training longer than standard LM.

The Appendix also discusses fine-tuning on diverse tasks, with GLUE covering MNLI, QQP, QNLI, SST-2, CoLA, STS-B, MRPC, RTE, and WINOGRAD NLI; however, GLUE reports issues with WINOGRAD NLI, leading to its exclusion to avoid unfair advantage for GPT, and in GLUE submissions the majority class is predicted. For fine-tuning, hyperparameters largely mirror pre-training with adjustments: batch size, learning rate (Adam) in the ranges of 5e-5, 3e-5, 2e-5, and epochs in {2, 3, 4}, with dropout fixed at 0.1; larger data sets are less sensitive to these choices, so an exhaustive search over these values is recommended.