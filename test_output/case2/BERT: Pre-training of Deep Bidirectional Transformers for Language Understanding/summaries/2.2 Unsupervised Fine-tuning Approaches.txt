Like feature-based methods, the initial unsupervised fine-tuning work pre-trained word embedding parameters from unlabeled text (Collobert and Weston, 2008). More recently, sentence or document encoders that produce contextual token representations have been pre-trained on unlabeled text and then fine-tuned for supervised downstream tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The main advantage is that few parameters need to be learned from scratch. Consequently, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentence-level tasks in the GLUE benchmark (Wang et al., 2018a). The section also references GPT and BERT in the context of pre-training, contrasting left-to-right language models with bidirectional encoders (as exemplified by BERT) and notes that pre-training and auto-encoder objectives have been used for these models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).