Summary of Section 2.2: Unsupervised Fine-tuning Approaches

- Progression: From pre-trained word embeddings to sentence/document encoders pre-trained on unlabeled text, which are then fine-tuned on downstream supervised tasks. This reduces the amount of learning required from scratch.

- Key advantage: Large, pre-trained encoders provide strong representations that can be adapted with relatively few parameters during task-specific fine-tuning.

- Notable models and results:
  - OpenAI GPT (Radford et al., 2018) uses a left-to-right language model and achieved state-of-the-art results on multiple sentence-level tasks in GLUE.
  - BERT and related contextual models illustrate the effectiveness of contextualized representations, with input formats incorporating [CLS], [SEP], and span-based QA (start/end positions) representations.

- Pretraining objectives: Both autoregressive language modeling (left-to-right) and autoencoder-type objectives have been used to pre-train these models (as cited: Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).

- Takeaway: Unsupervised pretraining of sentence/document encoders enables powerful, transferable representations that, when fine-tuned, yield strong performance on a range of downstream NLP tasks.