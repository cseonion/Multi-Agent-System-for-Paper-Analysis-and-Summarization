Section 5.3 contrasts fine-tuning with a feature-based approach for BERT by applying both to the CoNLL-2003 Named Entity Recognition task. It first notes that fixed, pre-computed representations can be advantageous when a task cannot be easily expressed by a Transformer encoder or when expensive representations are reusable across many experiments with cheaper downstream models. For the experiment, BERT is fed a case-preserving WordPiece input with maximal document context, treated as a tagging task without a CRF, to compare methods.

The ablation over model size (Table 6) shows that larger configurations improve both the masked LM perplexity and downstream development-set accuracy on MNLI-m, MRPC, and SST-2. Specifically, with 3 layers, 768 hidden units, and 12 attention heads the perplexity is 5.84 and dev accuracies are 77.9 (MNLI-m), 79.8 (MRPC), 88.4 (SST-2). Increasing to 6 layers with 3 heads yields 5.24, 80.6, 82.2, 90.7; with 6 layers and 12 heads: 4.68, 81.9, 84.8, 91.3. A 12-layer, 768/12 setup gives 3.99, 84.4, 86.7, 92.9; 12 layers with 1024 hidden and 16 heads improves to 3.54, 85.7, 86.9, 93.3; and 24 layers with 1024/16 yields 3.23, 86.6, 87.8, 93.7. These results demonstrate that larger, more capable pre-trained representations yield stronger performance across these tasks, including smaller datasets.

Table 7 compares fine-tuning versus a feature-based approach for CoNLL-2003. Using BERT-LARGE for fine-tuning attains Dev F1 96.6 and Test F1 92.8, while BERT-BASE yields 96.4 Dev F1 and 92.4 Test F1. In the feature-based setup with BERT-BASE, embeddings alone score 91.0 (Dev F1); representations from the second-to-last hidden layer reach 95.6; last hidden 94.9; a weighted sum of the last four hidden layers 95.9; concatenating the last four hidden layers 96.1; and a weighted sum of all 12 layers 95.5. The best feature-based configuration—concatenating the top four layers—achieves 96.1 Dev F1, only about 0.3 F1 behind full fine-tuning, illustrating that BERT is effective in both paradigms.