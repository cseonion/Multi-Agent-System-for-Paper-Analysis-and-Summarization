Summary of Section 5.3: Feature-based Approach with BERT

- Objective and setup
  - This section directly compares the fine-tuning approach (adding a task-specific classifier and updating all parameters) with a feature-based approach (using fixed representations from BERT).
  - The comparison is applied to CoNLL-2003 NER. BERT uses a case-preserving WordPiece vocabulary and leverages the full document context. The task is formulated as tagging, and no CRF is used.
  - For the feature-based setting, the representation from one or more BERT layers is extracted without updating BERT’s parameters, and this fixed representation is fed into a downstream model (a randomly initialized two-layer BiLSTM with 768 hidden units) followed by the classification layer. The representation of the first sub-token is used as input to the token-level classifier.

- Experimental details (models and ablations)
  - Table 6 (ablation over model size) shows a size-based study: larger models (more layers, larger hidden sizes, more attention heads) yield better language-model perplexity and better dev/test accuracy on tasks (MNLI, MRPC, SST-2). This provides context for why scaling BERT can help downstream tasks.
  - For the NER evaluation (Table 7), results are reported as Dev F1 and Test F1, averaged over 5 random restarts with the same hyperparameters (which were chosen on the development set).

- Key results
  - Fine-tuning vs feature-based on CoNLL-2003 NER:
    - Fine-tuned BERTLARGE achieves Dev F1 96.6 and Test F1 92.8.
    - Fine-tuned BERTBASE achieves Dev F1 96.4 and Test F1 92.4.
    - Feature-based results (using BERTBASE features with a downstream BiLSTM) vary by how the BERT representations are combined:
      - Embeddings: Dev F1 91.0
      - Second-to-Last Hidden: Dev F1 95.6
      - Last Hidden: Dev F1 94.9
      - Weighted Sum of Last Four Hidden: Dev F1 95.9
      - Concat of Last Four Hidden: Dev F1 96.1
      - Weighted Sum of All 12 Layers: Dev F1 95.5
  - The best feature-based method is the Concat of the last four hidden layers (with BERTBASE features), achieving Dev F1 96.1. This is described by the authors as being very close to the performance of fine-tuning the entire model (the text notes it is “only 0.3 F1 behind” the fully fine-tuned model).

- Interpretation and implications
  - BERT is effective in both regimes: full fine-tuning and as a fixed feature extractor.
  - The strongest feature-based result nearly matches the fine-tuned model, illustrating that high-quality representations from a pre-trained transformer can be very powerful even when the backbone is not updated.
  - The results reinforce the broader conclusion that larger pre-trained models provide substantial downstream gains, even when task data are limited.

- Practical takeaway
  - If computational or task-architecture constraints prevent fine-tuning, a high-quality feature-based approach using top-layer representations (especially concatenating the top four hidden layers) can yield near state-of-the-art performance on NER.

- Notes
  - Hyperparameters were tuned on the development set; Dev/Test scores reported are averaged over 5 random restarts.
  - The section also includes a case note on the token representation strategy (using the first sub-token) and that a CRF is not used in this setup.