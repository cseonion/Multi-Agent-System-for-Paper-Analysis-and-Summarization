This section presents an ablation on masking strategies for BERT’s masked language model (MLM) pre-training to reduce pre-training–finetuning mismatch, noting that the [MASK] symbol never appears during fine-tuning; dev results are reported for MNLI and NER, with NER results shown for both fine-tuning and a feature-based approach that concatenates the last four BERT layers, since that was found optimal earlier, and the table indicates that while fine-tuning is robust to masking choices, using only MASK hurts the feature-based NER, and using only RND underperforms compared with the mixed strategy (80/10/10). The OpenAI GPT comparison isolates architectural and training differences, showing GPT is a left-to-right Transformer LM trained on BooksCorpus (800M words) whereas BERT uses BooksCorpus plus Wikipedia (2,500M words) and learns special tokens like [SEP] and [CLS] during pre-training (unlike GPT, which introduces them at fine-tuning); GPT is trained for 1M steps with a batch size of about 32,000 words, while BERT uses 1M steps with a batch size of about 128,000 words, and GPT fine-tuning uses a fixed learning rate of 5e-5 whereas BERT uses task-specific fine-tuning learning rates; ablations show that most improvements arise from the two pre-training tasks and bidirectionality rather than these design differences. The Appendix is organized into three sections: A for implementation details, B for experimental details, and C for ablations, including “Effect of Number of Training Steps” and “Ablation for Different Masking Procedures.” A.1 illustrates the pre-training tasks with masked LM and the masking procedure, explaining that random masking selects the 4th token and replaces 1.5% of all tokens (10% of 15%), so MLM predicts 15% of tokens per batch, which motivates longer pre-training; this underpins the statement that masked LM predicts 15% per batch and justifies more pre-training steps, reiterating that BERT uses the 80/10/10 mixed masking distribution, and that fine-tuning GLUE results (MNLI, QQP, QNLI, SST-2, CoLA, STS-B, MRPC, and RTE; Winograd NLI excluded due to data issues) are obtained with fine-tuning hyperparameters largely mirroring pre-training, with batch sizes 16 or 32, Adam learning rates 5e-5, 3e-5, or 2e-5, epochs 2, 3, or 4, and dropout fixed at 0.1; larger datasets show less sensitivity, justifying an exhaustive dev-based search, and a note that GLUE submissions predict the majority class for Winograd NLI.