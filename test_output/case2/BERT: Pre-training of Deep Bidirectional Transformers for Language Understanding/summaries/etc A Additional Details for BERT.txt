Summary of Section: A. Additional Details for BERT

- Masking strategy ablation (Section 3.1)
  - Purpose: Evaluate how different MLM masking strategies during pre-training affect downstream fine-tuning, since fine-tuning never sees the [MASK] token.
  - Setup and data: Dev results reported for MNLI and NER; for NER, both fine-tuning and feature-based approaches are included. Results are shown in Table 8; masking strategies are MASK (replace with [MASK]), SAME (keep token), and RND (random token). BERT’s pre-training mix is 80% [MASK], 10% SAME, 10% RND.
  - Findings:
    - Fine-tuning is surprisingly robust to masking variations.
    - MASK-only masking is problematic for NER when using the feature-based approach.
    - RND-only masking performs worse than the mixed strategy.
  - Implication: A mixed masking strategy is preferable to reduce pre-training/fine-tuning mismatch, with caveats for feature-based NER.

- OpenAI GPT vs BERT comparison
  - Positioning: GPT and BERT are both fine-tuning based; ELMo is feature-based.
  - Key differences highlighted:
    - Data: GPT uses BooksCorpus (800M words); BERT uses BooksCorpus (800M) + Wikipedia (2.5B words).
    - Special tokens: GPT uses [SEP] and [CLS] only at fine-tuning; BERT learns [SEP], [CLS], and sentence embeddings during pre-training.
    - Training regime: GPT ~1M steps with batch size corresponding to 32,000 words; BERT ~1M steps with 128,000-word batches.
    - Fine-tuning learning rate: GPT uses a fixed 5e-5; BERT uses task-specific fine-tuning learning rates.
  - Main takeaway: Most empirical gains come from the two pre-training tasks (MLM and NSP) and bidirectionality rather than architectural/training differences between GPT and BERT. Section 5.1 ablations support this.

- Appendix structure and purpose
  - Organization: Appendix A (additional implementation details), Appendix B (additional experimental details), Appendix C (additional ablations).
  - Focus of additional ablations: effect of the number of training steps; ablation for different masking procedures.
  - Purpose: provide concrete methodological details to support the main results.

- A.1 Illustration of the Pre-training Tasks
  - Content: concrete examples of MLM and masking procedures.
  - Example: “my dog is hairy,” masking the 4th token (hairy).
  - Rationale: masking/random replacement forces the Transformer to maintain distributional contextual representations without knowing which words will be predicted; random replacements occur for 1.5% of all tokens (10% of the 15% masked tokens) to avoid harming language understanding.
  - Additional points: compared to standard LM training, the masked LM predicts about 15% of tokens per batch, suggesting more pre-training steps may be required for convergence. The section reinforces why the 80/10/10 masking mix is examined via ablations.

- Fine-tuning on GLUE tasks and practical details (Appendix content)
  - Fine-tuning setup: largely preserves pre-training hyperparameters, with task-specific adjustments to batch size, learning rate, and number of training epochs; dropout fixed at 0.1.
  - Hyperparameter ranges that work well across tasks:
    - Batch size: 16, 32
    - Adam learning rate: 5e-5, 3e-5, 2e-5
    - Number of epochs: 2, 3, 4
  - Observations: large datasets (≥100k labeled examples) are less sensitive to hyperparameter choices than small datasets.
  - GLUE tasks covered: MNLI, QQP, QNLI, SST-2, CoLA, STS, MRPC, RTE; WNLI is excluded from GLUE submissions (majority class baseline used for those cases).
  - Training scale and hardware:
    - Pre-training data: ~3.3 billion words
    - Duration: about 4 days per run
    - Hardware: BERT-Base on 4 Cloud TPUs; BERT-Large on 16 Cloud TPUs
    - Setup: ~1,000,000 pre-training steps (~40 epochs); batch size 256; sequence length 512 (with a mix of shorter/longer sequence steps)
  - Practical conclusion: MLM + NSP with bidirectionality yields strong, transferable representations; longer sequences help learning at higher compute cost; hyperparameters are reasonably robust, especially with larger datasets.

- Overall takeaways for this section
  - Masking ablations show fine-tuning is robust to masking choices, with caveats for feature-based NER using MASK-only.
  - The GPT vs. BERT discussion emphasizes that gains come mainly from the pre-training objectives (MLM/NSP) and bidirectionality, not just architectural differences.
  - The appendix provides concrete methodological detail on masking illustrations, GLUE fine-tuning setups, hyperparameters, dataset scope, and hardware scale, underscoring practical feasibility and generalizability of BERT.
  - Practically, MLM + NSP with bidirectional context yields strong transferable representations; longer sequences improve learning at a computational cost; hyperparameters are robust across tasks, particularly in data-rich regimes.