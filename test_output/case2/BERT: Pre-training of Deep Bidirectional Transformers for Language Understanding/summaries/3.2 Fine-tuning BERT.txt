Fine-tuning is straightforward because the Transformer's self-attention lets BERT model many downstream tasks by swapping inputs and outputs. For text pairs, the common pattern is to independently encode the two texts before applying bidirectional cross attention, but BERT unifies these stages by concatenating the text pair and using self-attention, which effectively implements bidirectional cross attention between the two sentences. For each task, we plug in task-specific inputs and outputs and fine-tune all parameters end-to-end. At input, sentence A and sentence B correspond to (1) sentence pairs in paraphrasing, (2) hypothesis–premise pairs in entailment, (3) question–passage pairs in question answering, and (4) a degenerate text-∅ pair in text classification or sequence tagging. At output, token representations feed into an output layer for token-level tasks (sequence tagging or question answering), and the [CLS] representation feeds into an output layer for classification (entailment or sentiment analysis). Compared to pre-training, fine-tuning is relatively inexpensive; all results can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model. Task-specific details are described in Section 4, with additional details in Appendix A.5.