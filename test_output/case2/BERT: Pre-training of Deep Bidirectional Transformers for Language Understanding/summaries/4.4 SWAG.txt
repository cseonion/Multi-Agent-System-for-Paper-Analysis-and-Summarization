SWAG (Situations With Adversarial Generations) encompasses 113k sentence-pair completion examples that test grounded commonsense inference, where the task is to select the most plausible continuation among four choices for a given sentence A and continuation B. For fine-tuning, four input sequences are created, each combining A with one of the four possible B continuations; the model learns via a single task-specific parameterâ€”a vector whose dot product with the [CLS] token representation C yields a score for each choice, normalized with softmax. The model is fine-tuned for 3 epochs with a learning rate of 2e-5 and a batch size of 16, and results (Table 4) show that BERT LARGE outperforms the baseline ESIM+ELMo by +27.1% and OpenAI GPT by 8.3%.