Section 5 reports ablations to assess the relative importance of BERTâ€™s pre-training components. Using the BERT BASE architecture, the development-set results across MNLI-m, QNLI, MRPC, SST-2, and SQuAD are: BERTBASE 84.4, 88.4, 86.7, 92.7, 88.5; No NSP 83.9, 84.9, 86.5, 92.6, 87.9; LTR & No NSP 82.1, 84.3, 77.5, 92.1, 77.8; and + BiLSTM 82.1, 84.1, 75.7, 91.6, 84.9. Table 5 caption notes that this is an ablation over the pre-training tasks, and that No NSP means training without the next sentence prediction task, while LTR & No NSP denotes a left-to-right LM without NSP (like OpenAI GPT); + BiLSTM adds a randomly initialized BiLSTM on top of the LTR + No NSP model during fine-tuning. Ablation studies are available in Appendix C.