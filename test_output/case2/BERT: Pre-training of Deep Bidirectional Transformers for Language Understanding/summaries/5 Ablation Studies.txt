Section 5 summary:

- Purpose: The authors conduct ablation experiments to assess how different pre-training facets of BERT influence performance, using the BERT BASE architecture.

- Experimental setups (Table 5):
  - BERTBASE: Baseline with all pre-training tasks (NSP included).
  - No NSP: Trained without the Next Sentence Prediction task.
  - LTR & No NSP: Trained as a left-to-right language model without NSP (GPT-like).
  - + BiLSTM: A randomly initialized BiLSTM is added on top of the LTR+No NSP model during fine-tuning.

- Results (Dev Set) across tasks MNLI-m, QNLI, MRPC, SST-2, SQuAD (SQuAD listed as F1):
  - BERTBASE: 84.4; 88.4; 86.7; 92.7; 88.5
  - No NSP: 83.9; 84.9; 86.5; 92.6; 87.9
  - LTR & No NSP: 82.1; 84.3; 77.5; 92.1; 77.8
  - + BiLSTM: 82.1; 84.1; 75.7; 91.6; 84.9

- Key takeaways:
  - Removing NSP (No NSP) yields a modest drop across most tasks (e.g., MNLI-m and QNLI decrease; SQuAD drops from 88.5 to 87.9).
  - Replacing NSP with a left-to-right LM (LTR & No NSP) causes a larger degradation, especially on MRPC (77.5) and SQuAD (77.8), with overall lower performance on several tasks.
  - Adding a BiLSTM on top of LTR+No NSP improves SQuAD performance substantially (84.9) but does not recover performance on most other tasks (MRPC, MNLI, QNLI, SST-2 remain relatively limited).
  - Overall, NSP contributes positively to performance; left-to-right pretraining without NSP is detrimental, and the BiLSTM gain is task-specific.

- Note: Additional ablation details are provided in Appendix C.