Recent empirical improvements from transfer learning with language models show that rich, unsupervised pre-training is integral to many language-understanding systems, enabling even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is to generalize these findings to deep bidirectional architectures, allowing the same pre-trained model to tackle a broad set of NLP tasks. In the MLM versus left-to-right comparison, MLM converges marginally slower than a left-to-right model, but its empirical gains far outweigh the extra training cost. The Next Sentence Prediction task is illustrated with examples showing how two spans are sampled from the corpus, with 50% of the time B being the actual next sentence and 50% a random sentence, provided the combined length is ≤ 512 tokens; the input is tokenized, masked at 15% with WordPiece, and the model uses a sum of the masked LM likelihood and NSP likelihood as the training loss. The A.2 Pre-training Procedure describes generating training sequences by sampling two spans A and B, labeling B as Next or NotNext accordingly; longer sequences are expensive due to quadratic attention, so to speed pre-training the model is first trained with sequence length 128 for 90% of steps, then with length 512 for the remaining 10% to learn positional embeddings. Training details include batch size 256, 1,000,000 steps (~40 epochs over a 3.3 billion word corpus), Adam with lr 1e-4, β1=0.9, β2=0.999, L2 weight decay 0.01, warmup for the first 10,000 steps, and linear decay, with dropout 0.1 and gelu activation. BERT BASE was trained on 4 Cloud TPUs (Pod configuration, 16 TPUs) for 4 days, and BERT LARGE on 16 Cloud TPUs (64 chips) for 4 days.