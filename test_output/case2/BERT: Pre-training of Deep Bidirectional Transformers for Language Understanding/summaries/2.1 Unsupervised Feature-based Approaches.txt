Summary of Section 2.1: Unsupervised Feature-based Approaches

- Context: Traces the development of pre-trained representations from traditional non-neural and neural word embeddings to the widespread use of pre-trained vectors in modern NLP, highlighting their performance gains over learning from scratch.

- Pretraining objectives: Early work used left-to-right language modeling and discrimination of correct vs. incorrect words within contexts. These ideas were extended to larger textual units (sentences and paragraphs) with objectives such as next-sentence ranking, left-to-right generation conditioned on prior context, and denoising autoencoder–based objectives.

- Sentence/paragraph representations: The literature moved beyond word-level embeddings to coarse-grained representations (sentence and paragraph embeddings) using corresponding pretraining tasks.

- ELMo and contextualization: ELMo (and its predecessors) produces context-sensitive token representations by combining forward and backward language models; the token representation is the concatenation of the two directional components. When integrated into task-specific architectures, ELMo achieved state-of-the-art results on multiple benchmarks (e.g., question answering, sentiment analysis, and named entity recognition).

- Related contextual approaches: Melamud et al. (2016) propose context representations by predicting a word from both left and right contexts using LSTMs, a method similar in spirit to ELMo but not deeply bidirectional. Fedus et al. (2018) show that cloze-style tasks can improve robustness in text generation models.

- Takeaway: The section situates contextual, unsupervised pretraining as a key evolution in representation learning, highlighting ELMo’s contribution as a practical and effective way to inject contextual information into downstream NLP systems.