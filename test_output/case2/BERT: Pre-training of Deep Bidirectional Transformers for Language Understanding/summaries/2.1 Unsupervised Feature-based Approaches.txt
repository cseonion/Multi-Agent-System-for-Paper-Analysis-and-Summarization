Unsupervised feature-based approaches to learning representations have evolved from early non-neural methods (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) to neural models (Mikolov et al., 2013; Pennington et al., 2014), with pre-trained word embeddings consistently delivering gains over scratch embeddings (Turian et al., 2010). Pretraining objectives include left-to-right language modeling (Mnih and Hinton, 2009) and discriminating correct versus incorrect words from left and right context (Mikolov et al., 2013). These ideas extend to coarser granularities, yielding sentence embeddings (Kiros et al., 2015; Logeswaran and Lee, 2018) and paragraph embeddings (Le and Mikolov, 2014). For training sentence representations, objectives have included ranking candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next-sentence words given the previous sentence representation (Kiros et al., 2015), and denoising autoencoderâ€“derived objectives (Hill et al., 2016). ELMo and its predecessor (Peters et al., 2017; 2018) generalize embeddings by extracting context-sensitive features from both left-to-right and right-to-left language models; the contextual token representation is the concatenation of the two directions, and integrating these contextual embeddings with task-specific architectures advances state-of-the-art results on QA (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named-entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations by predicting a single word from both left and right contexts with LSTMs, a feature-based approach similar to ELMo but not deeply bidirectional. Fedus et al. (2018) shows that a cloze task can be used to improve the robustness of text generation models.