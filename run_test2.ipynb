{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test case 2: Multi-Paper Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 08:21:32 | main                 | INFO     | ğŸš€ Task Paper Processing Started. Cache directory: cache/20250818_082132/\n",
      "2025-08-18 08:21:32 | main                 | INFO     | ğŸ“‹ Process logs are being recorded\n",
      "2025-08-18 08:21:32 | main                 | INFO     | ğŸ“‚ Input path: ['test/case2/']\n",
      "2025-08-18 08:21:32 | main                 | INFO     | ğŸ“‹ Log file: cache/20250818_082132/process.log\n",
      "2025-08-18 08:21:32 | src.tracking         | INFO     | ğŸ“Š ì‹¤í–‰ ì¶”ì  ì‹œìŠ¤í…œ ì´ˆê¸°í™”: cache/20250818_082132/\n",
      "2025-08-18 08:21:32 | src.graph            | INFO     | ğŸ”§ Send API ê¸°ë°˜ ë³‘ë ¬ ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì‹œì‘...\n",
      "2025-08-18 08:21:32 | src.graph            | INFO     | âœ… Send API ê¸°ë°˜ ë³‘ë ¬ ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì™„ë£Œ\n",
      "2025-08-18 08:21:32 | main                 | INFO     | âš™ï¸  Starting workflow execution...\n",
      "2025-08-18 08:21:32 | src.load_doc         | INFO     | ğŸ”„ Document extraction process started\n",
      "2025-08-18 08:21:32 | src.load_doc         | INFO     | ğŸ“‚ Processing paths: ['test/case2/']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d854e3a703694ce0b8b4a74676245642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 08:21:53 | src.load_doc         | INFO     | ğŸ“„ Loaded 269 documents\n",
      "2025-08-18 08:21:53 | src.load_doc         | INFO     | ğŸ“Š Found papers: ['Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need', 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'RoBERTa: A Robustly Optimized BERT Pretraining Approach']\n",
      "2025-08-18 08:21:53 | src.load_doc         | INFO     | ğŸ”§ Starting vectorstore creation\n",
      "/Users/onion/Documents/á„á…±á„‹á…¥á†¸/á„Œá…µá„‹á…¯á†«/á„‹á…µá†«á„á…¥á†«_á„ƒá…µá†¸á„‹á…©á„á…©/á„‹á…µá†¸á„‰á…¡á„€á…ªá„Œá…¦/code/v3(final)/task_paper/src/load_doc.py:206: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(\n",
      "2025-08-18 08:21:58 | sentence_transformers.SentenceTransformer | INFO     | Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-08-18 08:22:02 | src.load_doc         | INFO     | âœ… Embedder initialized with model: all-MiniLM-L6-v2\n",
      "2025-08-18 08:22:02 | src.load_doc         | INFO     | ğŸ”„ Creating vectorstore for: Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need\n",
      "2025-08-18 08:22:02 | faiss.loader         | INFO     | Loading faiss.\n",
      "2025-08-18 08:22:02 | faiss.loader         | INFO     | Successfully loaded faiss.\n",
      "2025-08-18 08:22:02 | faiss                | INFO     | Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
      "2025-08-18 08:22:02 | src.load_doc         | INFO     | ğŸ’¾ Vectorstore for 'Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need' saved to cache/20250818_082132/Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need/vectorstore/\n",
      "2025-08-18 08:22:02 | src.load_doc         | INFO     | ğŸ”„ Creating vectorstore for: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "2025-08-18 08:22:02 | src.load_doc         | INFO     | ğŸ’¾ Vectorstore for 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' saved to cache/20250818_082132/BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding/vectorstore/\n",
      "2025-08-18 08:22:02 | src.load_doc         | INFO     | ğŸ”„ Creating vectorstore for: RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
      "2025-08-18 08:22:03 | src.load_doc         | INFO     | ğŸ’¾ Vectorstore for 'RoBERTa: A Robustly Optimized BERT Pretraining Approach' saved to cache/20250818_082132/RoBERTa: A Robustly Optimized BERT Pretraining Approach/vectorstore/\n",
      "2025-08-18 08:22:03 | src.load_doc         | INFO     | âœ… All vectorstores created successfully (3 papers)\n",
      "2025-08-18 08:22:03 | src.load_doc         | INFO     | âœ… Document extraction completed successfully\n",
      "2025-08-18 08:22:03 | src.graph            | INFO     | ğŸš€ 3ê°œ ë…¼ë¬¸ì— ëŒ€í•œ ë³‘ë ¬ ìš”ì•½ ì‘ì—… ì‹œì‘...\n",
      "2025-08-18 08:22:03 | src.graph            | INFO     |   ğŸ“¤ 'Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need' â†’ Summary Subgraphë¡œ ì „ì†¡ (ì„¹ì…˜ 22ê°œ)\n",
      "2025-08-18 08:22:03 | src.graph            | INFO     |   ğŸ“¤ 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' â†’ Summary Subgraphë¡œ ì „ì†¡ (ì„¹ì…˜ 23ê°œ)\n",
      "2025-08-18 08:22:03 | src.graph            | INFO     |   ğŸ“¤ 'RoBERTa: A Robustly Optimized BERT Pretraining Approach' â†’ Summary Subgraphë¡œ ì „ì†¡ (ì„¹ì…˜ 22ê°œ)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 'Provided proper attribution is provided, Google he...' ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ (22ê°œ ì„¹ì…˜)\n",
      "â³ 'Provided proper attribution is provided, Google he...' ì„¹ì…˜ ìš”ì•½ ì¤‘... (22ê°œ ì„¹ì…˜)\n",
      "âœ… 'RoBERTa: A Robustly Optimized BERT Pretraining App...' ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ (22ê°œ ì„¹ì…˜)\n",
      "â³ 'RoBERTa: A Robustly Optimized BERT Pretraining App...' ì„¹ì…˜ ìš”ì•½ ì¤‘... (22ê°œ ì„¹ì…˜)\n",
      "âœ… 'BERT: Pre-training of Deep Bidirectional Transform...' ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ (23ê°œ ì„¹ì…˜)\n",
      "â³ 'BERT: Pre-training of Deep Bidirectional Transform...' ì„¹ì…˜ ìš”ì•½ ì¤‘... (23ê°œ ì„¹ì…˜)\n",
      "      ğŸ“‘ ì „ì²´ ìš”ì•½ ì¸ë±ìŠ¤ ì €ì¥ë¨: 00 Index_All_Summaries.txt\n",
      "âœ… 'Provided proper attribution is provided, Google he...' ì„¹ì…˜ë³„ ìš”ì•½ ì™„ë£Œ (22ê°œ ì„¹ì…˜)\n",
      "      ğŸ’¾ ìµœì¢… ìš”ì•½ ì €ì¥ë¨: Final_Summary.txt\n",
      "âœ… 'Provided proper attribution is provided, Google he...' ìµœì¢… ìš”ì•½ ì™„ë£Œ\n",
      "      ğŸ“‘ ì „ì²´ ìš”ì•½ ì¸ë±ìŠ¤ ì €ì¥ë¨: 00 Index_All_Summaries.txt\n",
      "âœ… 'BERT: Pre-training of Deep Bidirectional Transform...' ì„¹ì…˜ë³„ ìš”ì•½ ì™„ë£Œ (23ê°œ ì„¹ì…˜)\n",
      "      ğŸ“‘ ì „ì²´ ìš”ì•½ ì¸ë±ìŠ¤ ì €ì¥ë¨: 00 Index_All_Summaries.txt\n",
      "âœ… 'RoBERTa: A Robustly Optimized BERT Pretraining App...' ì„¹ì…˜ë³„ ìš”ì•½ ì™„ë£Œ (22ê°œ ì„¹ì…˜)\n",
      "      ğŸ’¾ ìµœì¢… ìš”ì•½ ì €ì¥ë¨: Final_Summary.txt\n",
      "âœ… 'BERT: Pre-training of Deep Bidirectional Transform...' ìµœì¢… ìš”ì•½ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 08:25:02 | agents.domain_agent  | INFO     | ğŸ” ë„ë©”ì¸ ë¶„ì„ ì‹œì‘... ë¶„ì„ ëŒ€ìƒ: 3ê°œ ë…¼ë¬¸\n",
      "2025-08-18 08:25:02 | agents.domain_agent  | INFO     | ğŸ“„ ë¶„ì„ ë…¼ë¬¸: ['Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need', 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'RoBERTa: A Robustly Optimized BERT Pretraining Approach']\n",
      "2025-08-18 08:25:02 | agents.domain_agent  | INFO     | ğŸ“‘ ë„ë©”ì¸ ë¶„ì„ ì¤‘...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ğŸ’¾ ìµœì¢… ìš”ì•½ ì €ì¥ë¨: Final_Summary.txt\n",
      "âœ… 'RoBERTa: A Robustly Optimized BERT Pretraining App...' ìµœì¢… ìš”ì•½ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 08:25:03 | agents.domain_agent  | INFO     |    âœ… Provided proper attribution is... ë¶„ì•¼: {'main_field': ['Computer Science'], 'sub_field': ['Natural Language Processing', 'Deep Learning', 'Neural Networks']}\n",
      "2025-08-18 08:25:03 | agents.domain_agent  | INFO     | ğŸ“‘ ë„ë©”ì¸ ë¶„ì„ ì¤‘...\n",
      "2025-08-18 08:25:04 | agents.domain_agent  | INFO     |    âœ… BERT: Pre-training of Deep Bid... ë¶„ì•¼: {'main_field': ['Computer Science'], 'sub_field': ['Natural Language Processing', 'Machine Learning', 'Deep Learning']}\n",
      "2025-08-18 08:25:04 | agents.domain_agent  | INFO     | ğŸ“‘ ë„ë©”ì¸ ë¶„ì„ ì¤‘...\n",
      "2025-08-18 08:25:04 | agents.domain_agent  | INFO     |    âœ… RoBERTa: A Robustly Optimized ... ë¶„ì•¼: {'main_field': ['Computer Science'], 'sub_field': ['Natural Language Processing', 'Machine Learning']}\n",
      "2025-08-18 08:25:04 | agents.domain_agent  | INFO     | âœ… ë„ë©”ì¸ ë¶„ì„ ì™„ë£Œ!\n",
      "2025-08-18 08:25:04 | agents.analysis_plan_router | INFO     | ğŸ¯ ë¶„ì„ ê³„íš ìƒì„± ì¤‘... ë…¼ë¬¸ ìˆ˜: 3\n",
      "2025-08-18 08:25:04 | agents.analysis_plan_router | INFO     | ğŸ“š ë‹¤ì¤‘ ë…¼ë¬¸ ë¶„ì„: 3ê°œ ë…¼ë¬¸\n",
      "2025-08-18 08:25:05 | agents.analysis_plan_router | INFO     | âœ… ë¶„ì„ ê³„íš ê²°ì •: ë¹„êµ ë¶„ì„ (comparison)\n",
      "2025-08-18 08:25:05 | src.graph            | INFO     | ğŸ§­ ë¶„ì„ í”Œëœ í™•ì •: comparison\n",
      "2025-08-18 08:25:05 | agents.analysis_comparison_agent | INFO     | ğŸ§® ë¹„êµ ë¶„ì„ ì‹œì‘: ëŒ€ìƒ ë…¼ë¬¸ ìˆ˜ 3\n",
      "2025-08-18 08:25:05 | agents.tools.web_search | INFO     | Web Search Tool enabled: DuckDuckGoSearchResults\n",
      "2025-08-18 08:25:05 | agents.analysis_comparison_agent | INFO     | ğŸ”§ Web Search Tool í™œì„±í™”\n",
      "2025-08-18 08:25:05 | agents.analysis_comparison_agent | INFO     | ğŸ”§ Vectorstore Tools ì¶”ê°€ë¨: 3ê°œ\n",
      "2025-08-18 08:25:05 | agents.analysis_comparison_agent | INFO     | ğŸ” ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ (tool ì‚¬ìš© ê°€ëŠ¥, ë‹¤ì¤‘ ë…¼ë¬¸ ë¹„êµ)...\n",
      "2025-08-18 08:25:46 | agents.analysis_comparison_agent | INFO     | âœ… ë¹„êµ ë¶„ì„ ì™„ë£Œ\n",
      "2025-08-18 08:25:46 | agents.tools.python_repl | INFO     | Python REPL Tool enabled\n",
      "2025-08-18 08:25:46 | agents.write_agent   | INFO     | ğŸ”§ Python REPL Tool í™œì„±í™”\n",
      "2025-08-18 08:25:46 | agents.write_agent   | INFO     | ğŸ“ Write agent ì‹¤í–‰ (ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼ ìƒì„±, Markdown ì¶œë ¥)...\n",
      "2025-08-18 08:26:45 | agents.write_agent   | INFO     | âœ… ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ\n",
      "2025-08-18 08:26:45 | src.tracking         | INFO     | ğŸ¯ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ\n",
      "2025-08-18 08:26:45 | src.tracking         | INFO     | â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: 312.88ì´ˆ\n",
      "2025-08-18 08:26:45 | src.tracking         | INFO     | ğŸ’° ì´ í† í° ì‚¬ìš©: 0ê°œ\n",
      "2025-08-18 08:26:45 | src.tracking         | INFO     | ğŸ’³ ì´ ì˜ˆìƒ ë¹„ìš©: $0.0000\n",
      "2025-08-18 08:26:45 | main                 | INFO     | ğŸ“Š ì‹¤í–‰ ì¶”ì  ì™„ë£Œ - ì—ì´ì „íŠ¸ 0ê°œ, ì´ ì‹œê°„ 312.88ì´ˆ\n",
      "2025-08-18 08:26:45 | main                 | INFO     | ğŸ’¾ ì €ì¥ë¨: cache/20250818_082132/comparison_analysis_report_20250818_082645.md\n",
      "2025-08-18 08:26:45 | main                 | INFO     | ğŸ’¾ ì €ì¥ë¨: cache/20250818_082132/comparison_final_report_20250818_082645.md\n",
      "2025-08-18 08:26:45 | main                 | INFO     | âœ… Task Paper Processing Completed Successfully\n"
     ]
    }
   ],
   "source": [
    "from main import run\n",
    "\n",
    "pdf_path = [\"test/case2/\"]\n",
    "result, app, workflow = run(pdf_path, print_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Transformers, BERT, and RoBERTa: What Changed, What Matters, and When to Use Each\n",
      "\n",
      "Based on the multi-paper analysis report\n",
      "\n",
      "Introduction\n",
      "Modern NLP is built on three pivotal ideas: attention-only sequence modeling (Transformer), bidirectional encoder pretraining (BERT), and â€œitâ€™s the training recipe, not the architectureâ€ (RoBERTa). In this post, youâ€™ll learn how these models differ in goals, training, and practical use, and youâ€™ll leave with a small code snippet to try dynamic masking (the trick that helped RoBERTa pull ahead).\n",
      "\n",
      "## Executive snapshot (one paragraph each)\n",
      "\n",
      "- Transformer (Attention Is All You Need): Introduces the encoderâ€“decoder architecture powered entirely by multi-head self-attention. It replaces recurrence/convolution, boosts parallelism, models long-range dependencies better, and achieves state-of-the-art machine translation with faster training.\n",
      "\n",
      "- BERT: Repurposes the Transformer encoder for self-supervised pretraining (Masked Language Modeling + Next Sentence Prediction). It learns deeply bidirectional representations that fine-tune well on diverse NLU tasks with minimal task-specific engineering.\n",
      "\n",
      "- RoBERTa: Keeps BERTâ€™s encoder but shows that scale and training strategy dominate performance. It removes NSP, uses dynamic masking, increases data, batch size, and training time, and delivers stronger NLU results without changing the architecture.\n",
      "\n",
      "## Problem settings and goals\n",
      "\n",
      "- Transformer (AIAIY): Supervised sequence-to-sequence (e.g., WMTâ€™14 machine translation). Goals: parallel training, long-range modeling, faster convergence vs. RNN/CNN.\n",
      "\n",
      "- BERT: General-purpose language understanding via self-supervised pretraining, then fine-tuning for GLUE, SQuAD, SWAG. Goal: exploit deep bidirectional context.\n",
      "\n",
      "- RoBERTa: Re-evaluates BERTâ€™s pretraining to isolate what truly matters. Goal: maximize downstream accuracy via optimized data and training, not architectural tweaks.\n",
      "\n",
      "## Whatâ€™s inside: architectures and objectives\n",
      "\n",
      "- Transformer (encoderâ€“decoder)\n",
      "  - Fixed sinusoidal positional encodings.\n",
      "  - Multi-head attention + residuals + feed-forward layers.\n",
      "  - Decoder uses masked self-attention; cross-attention connects encoder to decoder.\n",
      "  - Objective: supervised token-level cross-entropy with autoregressive decoding.\n",
      "\n",
      "- BERT (encoder-only)\n",
      "  - Learned positional embeddings; WordPiece tokenization.\n",
      "  - Special tokens: [CLS], [SEP]; segment embeddings.\n",
      "  - Objectives: Masked Language Modeling (15% tokens; mixed replacement) + Next Sentence Prediction.\n",
      "\n",
      "- RoBERTa (encoder-only, BERT-compatible)\n",
      "  - Same architecture as BERT; byte-level BPE (50k) instead of WordPiece.\n",
      "  - Objective: MLM only; no NSP.\n",
      "  - Dynamic masking (new masks each epoch), larger batches, longer training, more diverse data.\n",
      "\n",
      "Positional handling: Transformer uses fixed sinusoidal; BERT/RoBERTa use learned positions.\n",
      "\n",
      "## Data, scale, and compute (why training matters)\n",
      "\n",
      "- Transformer: Trained on WMTâ€™14 Enâ€“De/Enâ€“Fr with 8Ã—P100; hoursâ€“days; efficient vs. RNN/CNN.\n",
      "\n",
      "- BERT: BooksCorpus + Wikipedia; BASE and LARGE variants; heavy pretraining (for its time).\n",
      "\n",
      "- RoBERTa: ~160GB of text (BookCorpus, Wikipedia, CC-News, OpenWebText, Stories), very large batches (up to ~8k sequences), long schedules (up to ~500k steps), heavy distributed training.\n",
      "\n",
      "## Results at a glance\n",
      "\n",
      "- Transformer: New SOTA BLEU on WMTâ€™14 Enâ€“De (28.4) and Enâ€“Fr (41.0); strong transfer to parsing.\n",
      "\n",
      "- BERT: SOTA across GLUE, SQuAD v1.1, SWAG; simple fine-tuning works broadly.\n",
      "\n",
      "- RoBERTa: Improves over BERT on GLUE (dev/test), SQuAD v1.1/v2.0, RACE; competitive with XLNet despite no NSP and unchanged architecture.\n",
      "\n",
      "## Strengths and limitations\n",
      "\n",
      "- Transformer (AIAIY)\n",
      "  - Strengths: Attention-only seq2seq with massive parallelism; efficient training; interpretable heads; great for generative transduction.\n",
      "  - Limits: O(n^2) attention cost; needs parallel data; autoregressive decoding is sequential at inference.\n",
      "\n",
      "- BERT\n",
      "  - Strengths: Universal bidirectional representations; strong fine-tuning baselines; minimal task-specific work.\n",
      "  - Limits: Expensive pretraining; NSP may not help; domain mismatch without continued pretraining; MLM is sample-inefficient.\n",
      "\n",
      "- RoBERTa\n",
      "  - Strengths: Shows data/compute and dynamic masking drive gains; robust NLU baselines; removes NSP without loss.\n",
      "  - Limits: Higher compute/data demands; English-centric; still O(n^2); not tailored for generative seq2seq.\n",
      "\n",
      "## Conceptual impact\n",
      "\n",
      "- Transformer: Establishes attention-only as a general backbone across NLP, vision, speech; encoder/decoder foundations for later models.\n",
      "\n",
      "- BERT: Popularizes pretrain-then-finetune; proves importance of bidirectionality; standardizes [CLS]/[SEP] formatting.\n",
      "\n",
      "- RoBERTa: Recalibrates the field toward training/data optimization; challenges NSP; provides stronger, reproducible baselines.\n",
      "\n",
      "## Assumptions and trade-offs\n",
      "\n",
      "- Transformer: Trades quadratic memory for global receptive field and throughput; best fit for seq2seq generation.\n",
      "\n",
      "- BERT: Assumes masked-token recovery yields transferable semantics; trades generative ability for discriminative strength.\n",
      "\n",
      "- RoBERTa: Assumes scaling and recipe refinements beat architectural novelty; trades accessibility for peak accuracy.\n",
      "\n",
      "## Practical guidance: when to use which\n",
      "\n",
      "- Need machine translation, summarization, data-to-text, or any seq2seq with cross-attention? Use a Transformer encoderâ€“decoder (AIAIY-style). Modern variants add relative positions and better decoding, but the core holds.\n",
      "\n",
      "- Need strong NLU (classification, NER, extractive QA) with little task-specific engineering? Use a pretrained encoder. Prefer RoBERTa for stronger results if you can afford it; choose BERT for compatibility, smaller checkpoints, or tighter resource budgets.\n",
      "\n",
      "- Domain-specific tasks (biomedical, legal, code): Continue pretraining (domain-adaptive pretraining) on in-domain data for BERT/RoBERTa before fine-tuning.\n",
      "\n",
      "## Hands-on: dynamic masking for MLM in practice (RoBERTa vs. BERT)\n",
      "\n",
      "The snippet below shows a minimal masked language modeling setup using Hugging Face Transformers. Switching the model name flips between BERT and RoBERTa; DataCollatorForLanguageModeling enables dynamic masking (RoBERTa-style).\n",
      "\n",
      "```python\n",
      "# pip install transformers datasets accelerate -q\n",
      "from datasets import load_dataset\n",
      "from transformers import (AutoTokenizer, AutoModelForMaskedLM,\n",
      "                          DataCollatorForLanguageModeling, Trainer, TrainingArguments)\n",
      "\n",
      "# Toggle between 'roberta-base' (byte-level BPE, no NSP) and 'bert-base-uncased' (WordPiece)\n",
      "MODEL_NAME = \"roberta-base\"  # or \"bert-base-uncased\"\n",
      "\n",
      "# 1) Load a small text corpus (subset for a quick demo)\n",
      "raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
      "\n",
      "def tokenize(examples):\n",
      "    return tokenizer(examples[\"text\"], truncation=True, padding=False)\n",
      "\n",
      "tok = raw.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
      "\n",
      "# 2) Dynamic masking: new masks each batch/epoch (key to RoBERTaâ€™s gains)\n",
      "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
      "\n",
      "# 3) Model and quick training loop (few steps for illustration)\n",
      "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
      "\n",
      "args = TrainingArguments(\n",
      "    output_dir=\"mlm-demo\",\n",
      "    per_device_train_batch_size=8,\n",
      "    learning_rate=5e-5,\n",
      "    num_train_epochs=1,\n",
      "    max_steps=100,       # keep tiny for demo\n",
      "    logging_steps=20,\n",
      "    report_to=\"none\",\n",
      ")\n",
      "\n",
      "trainer = Trainer(model=model, args=args, train_dataset=tok, data_collator=collator)\n",
      "trainer.train()\n",
      "\n",
      "# Tip:\n",
      "# - Using 'roberta-base' mirrors RoBERTa-style MLM (no NSP) with dynamic masking.\n",
      "# - Using 'bert-base-uncased' still benefits from dynamic masking here,\n",
      "#   but original BERT pretraining also included Next Sentence Prediction.\n",
      "```\n",
      "\n",
      "Notes:\n",
      "- NSP is not part of RoBERTaâ€™s recipe. The Trainer above uses MLM only, which aligns with RoBERTa.\n",
      "- For task fine-tuning (e.g., SST-2, MRPC), swap the model head to AutoModelForSequenceClassification and load a GLUE dataset.\n",
      "\n",
      "## Reproducibility and resources\n",
      "\n",
      "- All three released code/models; RoBERTa also shared data resources and detailed training recipes.\n",
      "- Exact RoBERTa reproduction needs substantial hardware. Even so, smaller-scale replicas benefit from its recipe: dynamic masking, longer training, and larger batches (within your limits).\n",
      "\n",
      "## Open problems and extensions\n",
      "\n",
      "- Long-context efficiency: O(n^2) attention spurs sparse/linear attention and memory-augmented approaches.\n",
      "- Multilingual and low-resource transfer: mBERT and XLM-R extend the recipe; domain shifts remain tricky.\n",
      "- Generative pretraining: Encoder-only excels at NLU; encoderâ€“decoder or decoder-only pretraining (e.g., T5, GPT) better suit generationâ€”bridging both is active research.\n",
      "- Interpretability and robustness: Attention helps, but robust generalization under distribution shifts and adversarial inputs is still an open challenge.\n",
      "\n",
      "## Key takeaways\n",
      "\n",
      "- Transformer (AIAIY) unlocked parallel, attention-only seq2seq and set the blueprint for modern architectures.\n",
      "- BERT proved bidirectional encoder pretraining plus simple fine-tuning yields strong, general NLU performance.\n",
      "- RoBERTa showed that training/data scale and dynamic masking, not architectural changes, drove further NLU gains.\n",
      "\n",
      "## Potential applications\n",
      "\n",
      "- Transformer: Machine translation, abstractive summarization, code generation, speech recognition with seq2seq.\n",
      "- BERT/RoBERTa: Text classification, NER, extractive QA, sentence similarity, retrieval re-ranking.\n",
      "- With domain-adaptive pretraining: Biomedical/clinical NLP, legal document understanding, code intelligence, financial analysis.\n"
     ]
    }
   ],
   "source": [
    "# check `cache` folder\n",
    "print(result['final_report'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
