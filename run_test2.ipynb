{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test case 2: Multi-Paper Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 08:21:32 | main                 | INFO     | 🚀 Task Paper Processing Started. Cache directory: cache/20250818_082132/\n",
      "2025-08-18 08:21:32 | main                 | INFO     | 📋 Process logs are being recorded\n",
      "2025-08-18 08:21:32 | main                 | INFO     | 📂 Input path: ['test/case2/']\n",
      "2025-08-18 08:21:32 | main                 | INFO     | 📋 Log file: cache/20250818_082132/process.log\n",
      "2025-08-18 08:21:32 | src.tracking         | INFO     | 📊 실행 추적 시스템 초기화: cache/20250818_082132/\n",
      "2025-08-18 08:21:32 | src.graph            | INFO     | 🔧 Send API 기반 병렬 워크플로우 구성 시작...\n",
      "2025-08-18 08:21:32 | src.graph            | INFO     | ✅ Send API 기반 병렬 워크플로우 구성 완료\n",
      "2025-08-18 08:21:32 | main                 | INFO     | ⚙️  Starting workflow execution...\n",
      "2025-08-18 08:21:32 | src.load_doc         | INFO     | 🔄 Document extraction process started\n",
      "2025-08-18 08:21:32 | src.load_doc         | INFO     | 📂 Processing paths: ['test/case2/']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d854e3a703694ce0b8b4a74676245642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 08:21:53 | src.load_doc         | INFO     | 📄 Loaded 269 documents\n",
      "2025-08-18 08:21:53 | src.load_doc         | INFO     | 📊 Found papers: ['Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need', 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'RoBERTa: A Robustly Optimized BERT Pretraining Approach']\n",
      "2025-08-18 08:21:53 | src.load_doc         | INFO     | 🔧 Starting vectorstore creation\n",
      "/Users/onion/Documents/취업/지원/인턴_딥오토/입사과제/code/v3(final)/task_paper/src/load_doc.py:206: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(\n",
      "2025-08-18 08:21:58 | sentence_transformers.SentenceTransformer | INFO     | Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-08-18 08:22:02 | src.load_doc         | INFO     | ✅ Embedder initialized with model: all-MiniLM-L6-v2\n",
      "2025-08-18 08:22:02 | src.load_doc         | INFO     | 🔄 Creating vectorstore for: Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need\n",
      "2025-08-18 08:22:02 | faiss.loader         | INFO     | Loading faiss.\n",
      "2025-08-18 08:22:02 | faiss.loader         | INFO     | Successfully loaded faiss.\n",
      "2025-08-18 08:22:02 | faiss                | INFO     | Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
      "2025-08-18 08:22:02 | src.load_doc         | INFO     | 💾 Vectorstore for 'Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need' saved to cache/20250818_082132/Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need/vectorstore/\n",
      "2025-08-18 08:22:02 | src.load_doc         | INFO     | 🔄 Creating vectorstore for: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "2025-08-18 08:22:02 | src.load_doc         | INFO     | 💾 Vectorstore for 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' saved to cache/20250818_082132/BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding/vectorstore/\n",
      "2025-08-18 08:22:02 | src.load_doc         | INFO     | 🔄 Creating vectorstore for: RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
      "2025-08-18 08:22:03 | src.load_doc         | INFO     | 💾 Vectorstore for 'RoBERTa: A Robustly Optimized BERT Pretraining Approach' saved to cache/20250818_082132/RoBERTa: A Robustly Optimized BERT Pretraining Approach/vectorstore/\n",
      "2025-08-18 08:22:03 | src.load_doc         | INFO     | ✅ All vectorstores created successfully (3 papers)\n",
      "2025-08-18 08:22:03 | src.load_doc         | INFO     | ✅ Document extraction completed successfully\n",
      "2025-08-18 08:22:03 | src.graph            | INFO     | 🚀 3개 논문에 대한 병렬 요약 작업 시작...\n",
      "2025-08-18 08:22:03 | src.graph            | INFO     |   📤 'Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need' → Summary Subgraph로 전송 (섹션 22개)\n",
      "2025-08-18 08:22:03 | src.graph            | INFO     |   📤 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' → Summary Subgraph로 전송 (섹션 23개)\n",
      "2025-08-18 08:22:03 | src.graph            | INFO     |   📤 'RoBERTa: A Robustly Optimized BERT Pretraining Approach' → Summary Subgraph로 전송 (섹션 22개)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'Provided proper attribution is provided, Google he...' 섹션 추출 완료 (22개 섹션)\n",
      "⏳ 'Provided proper attribution is provided, Google he...' 섹션 요약 중... (22개 섹션)\n",
      "✅ 'RoBERTa: A Robustly Optimized BERT Pretraining App...' 섹션 추출 완료 (22개 섹션)\n",
      "⏳ 'RoBERTa: A Robustly Optimized BERT Pretraining App...' 섹션 요약 중... (22개 섹션)\n",
      "✅ 'BERT: Pre-training of Deep Bidirectional Transform...' 섹션 추출 완료 (23개 섹션)\n",
      "⏳ 'BERT: Pre-training of Deep Bidirectional Transform...' 섹션 요약 중... (23개 섹션)\n",
      "      📑 전체 요약 인덱스 저장됨: 00 Index_All_Summaries.txt\n",
      "✅ 'Provided proper attribution is provided, Google he...' 섹션별 요약 완료 (22개 섹션)\n",
      "      💾 최종 요약 저장됨: Final_Summary.txt\n",
      "✅ 'Provided proper attribution is provided, Google he...' 최종 요약 완료\n",
      "      📑 전체 요약 인덱스 저장됨: 00 Index_All_Summaries.txt\n",
      "✅ 'BERT: Pre-training of Deep Bidirectional Transform...' 섹션별 요약 완료 (23개 섹션)\n",
      "      📑 전체 요약 인덱스 저장됨: 00 Index_All_Summaries.txt\n",
      "✅ 'RoBERTa: A Robustly Optimized BERT Pretraining App...' 섹션별 요약 완료 (22개 섹션)\n",
      "      💾 최종 요약 저장됨: Final_Summary.txt\n",
      "✅ 'BERT: Pre-training of Deep Bidirectional Transform...' 최종 요약 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 08:25:02 | agents.domain_agent  | INFO     | 🔍 도메인 분석 시작... 분석 대상: 3개 논문\n",
      "2025-08-18 08:25:02 | agents.domain_agent  | INFO     | 📄 분석 논문: ['Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need', 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'RoBERTa: A Robustly Optimized BERT Pretraining Approach']\n",
      "2025-08-18 08:25:02 | agents.domain_agent  | INFO     | 📑 도메인 분석 중...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      💾 최종 요약 저장됨: Final_Summary.txt\n",
      "✅ 'RoBERTa: A Robustly Optimized BERT Pretraining App...' 최종 요약 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 08:25:03 | agents.domain_agent  | INFO     |    ✅ Provided proper attribution is... 분야: {'main_field': ['Computer Science'], 'sub_field': ['Natural Language Processing', 'Deep Learning', 'Neural Networks']}\n",
      "2025-08-18 08:25:03 | agents.domain_agent  | INFO     | 📑 도메인 분석 중...\n",
      "2025-08-18 08:25:04 | agents.domain_agent  | INFO     |    ✅ BERT: Pre-training of Deep Bid... 분야: {'main_field': ['Computer Science'], 'sub_field': ['Natural Language Processing', 'Machine Learning', 'Deep Learning']}\n",
      "2025-08-18 08:25:04 | agents.domain_agent  | INFO     | 📑 도메인 분석 중...\n",
      "2025-08-18 08:25:04 | agents.domain_agent  | INFO     |    ✅ RoBERTa: A Robustly Optimized ... 분야: {'main_field': ['Computer Science'], 'sub_field': ['Natural Language Processing', 'Machine Learning']}\n",
      "2025-08-18 08:25:04 | agents.domain_agent  | INFO     | ✅ 도메인 분석 완료!\n",
      "2025-08-18 08:25:04 | agents.analysis_plan_router | INFO     | 🎯 분석 계획 생성 중... 논문 수: 3\n",
      "2025-08-18 08:25:04 | agents.analysis_plan_router | INFO     | 📚 다중 논문 분석: 3개 논문\n",
      "2025-08-18 08:25:05 | agents.analysis_plan_router | INFO     | ✅ 분석 계획 결정: 비교 분석 (comparison)\n",
      "2025-08-18 08:25:05 | src.graph            | INFO     | 🧭 분석 플랜 확정: comparison\n",
      "2025-08-18 08:25:05 | agents.analysis_comparison_agent | INFO     | 🧮 비교 분석 시작: 대상 논문 수 3\n",
      "2025-08-18 08:25:05 | agents.tools.web_search | INFO     | Web Search Tool enabled: DuckDuckGoSearchResults\n",
      "2025-08-18 08:25:05 | agents.analysis_comparison_agent | INFO     | 🔧 Web Search Tool 활성화\n",
      "2025-08-18 08:25:05 | agents.analysis_comparison_agent | INFO     | 🔧 Vectorstore Tools 추가됨: 3개\n",
      "2025-08-18 08:25:05 | agents.analysis_comparison_agent | INFO     | 🔍 ReAct 에이전트 실행 (tool 사용 가능, 다중 논문 비교)...\n",
      "2025-08-18 08:25:46 | agents.analysis_comparison_agent | INFO     | ✅ 비교 분석 완료\n",
      "2025-08-18 08:25:46 | agents.tools.python_repl | INFO     | Python REPL Tool enabled\n",
      "2025-08-18 08:25:46 | agents.write_agent   | INFO     | 🔧 Python REPL Tool 활성화\n",
      "2025-08-18 08:25:46 | agents.write_agent   | INFO     | 📝 Write agent 실행 (블로그 스타일 생성, Markdown 출력)...\n",
      "2025-08-18 08:26:45 | agents.write_agent   | INFO     | ✅ 블로그 포스트 생성 완료\n",
      "2025-08-18 08:26:45 | src.tracking         | INFO     | 🎯 전체 워크플로우 완료\n",
      "2025-08-18 08:26:45 | src.tracking         | INFO     | ⏱️  총 실행 시간: 312.88초\n",
      "2025-08-18 08:26:45 | src.tracking         | INFO     | 💰 총 토큰 사용: 0개\n",
      "2025-08-18 08:26:45 | src.tracking         | INFO     | 💳 총 예상 비용: $0.0000\n",
      "2025-08-18 08:26:45 | main                 | INFO     | 📊 실행 추적 완료 - 에이전트 0개, 총 시간 312.88초\n",
      "2025-08-18 08:26:45 | main                 | INFO     | 💾 저장됨: cache/20250818_082132/comparison_analysis_report_20250818_082645.md\n",
      "2025-08-18 08:26:45 | main                 | INFO     | 💾 저장됨: cache/20250818_082132/comparison_final_report_20250818_082645.md\n",
      "2025-08-18 08:26:45 | main                 | INFO     | ✅ Task Paper Processing Completed Successfully\n"
     ]
    }
   ],
   "source": [
    "from main import run\n",
    "\n",
    "pdf_path = [\"test/case2/\"]\n",
    "result, app, workflow = run(pdf_path, print_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Transformers, BERT, and RoBERTa: What Changed, What Matters, and When to Use Each\n",
      "\n",
      "Based on the multi-paper analysis report\n",
      "\n",
      "Introduction\n",
      "Modern NLP is built on three pivotal ideas: attention-only sequence modeling (Transformer), bidirectional encoder pretraining (BERT), and “it’s the training recipe, not the architecture” (RoBERTa). In this post, you’ll learn how these models differ in goals, training, and practical use, and you’ll leave with a small code snippet to try dynamic masking (the trick that helped RoBERTa pull ahead).\n",
      "\n",
      "## Executive snapshot (one paragraph each)\n",
      "\n",
      "- Transformer (Attention Is All You Need): Introduces the encoder–decoder architecture powered entirely by multi-head self-attention. It replaces recurrence/convolution, boosts parallelism, models long-range dependencies better, and achieves state-of-the-art machine translation with faster training.\n",
      "\n",
      "- BERT: Repurposes the Transformer encoder for self-supervised pretraining (Masked Language Modeling + Next Sentence Prediction). It learns deeply bidirectional representations that fine-tune well on diverse NLU tasks with minimal task-specific engineering.\n",
      "\n",
      "- RoBERTa: Keeps BERT’s encoder but shows that scale and training strategy dominate performance. It removes NSP, uses dynamic masking, increases data, batch size, and training time, and delivers stronger NLU results without changing the architecture.\n",
      "\n",
      "## Problem settings and goals\n",
      "\n",
      "- Transformer (AIAIY): Supervised sequence-to-sequence (e.g., WMT’14 machine translation). Goals: parallel training, long-range modeling, faster convergence vs. RNN/CNN.\n",
      "\n",
      "- BERT: General-purpose language understanding via self-supervised pretraining, then fine-tuning for GLUE, SQuAD, SWAG. Goal: exploit deep bidirectional context.\n",
      "\n",
      "- RoBERTa: Re-evaluates BERT’s pretraining to isolate what truly matters. Goal: maximize downstream accuracy via optimized data and training, not architectural tweaks.\n",
      "\n",
      "## What’s inside: architectures and objectives\n",
      "\n",
      "- Transformer (encoder–decoder)\n",
      "  - Fixed sinusoidal positional encodings.\n",
      "  - Multi-head attention + residuals + feed-forward layers.\n",
      "  - Decoder uses masked self-attention; cross-attention connects encoder to decoder.\n",
      "  - Objective: supervised token-level cross-entropy with autoregressive decoding.\n",
      "\n",
      "- BERT (encoder-only)\n",
      "  - Learned positional embeddings; WordPiece tokenization.\n",
      "  - Special tokens: [CLS], [SEP]; segment embeddings.\n",
      "  - Objectives: Masked Language Modeling (15% tokens; mixed replacement) + Next Sentence Prediction.\n",
      "\n",
      "- RoBERTa (encoder-only, BERT-compatible)\n",
      "  - Same architecture as BERT; byte-level BPE (50k) instead of WordPiece.\n",
      "  - Objective: MLM only; no NSP.\n",
      "  - Dynamic masking (new masks each epoch), larger batches, longer training, more diverse data.\n",
      "\n",
      "Positional handling: Transformer uses fixed sinusoidal; BERT/RoBERTa use learned positions.\n",
      "\n",
      "## Data, scale, and compute (why training matters)\n",
      "\n",
      "- Transformer: Trained on WMT’14 En–De/En–Fr with 8×P100; hours–days; efficient vs. RNN/CNN.\n",
      "\n",
      "- BERT: BooksCorpus + Wikipedia; BASE and LARGE variants; heavy pretraining (for its time).\n",
      "\n",
      "- RoBERTa: ~160GB of text (BookCorpus, Wikipedia, CC-News, OpenWebText, Stories), very large batches (up to ~8k sequences), long schedules (up to ~500k steps), heavy distributed training.\n",
      "\n",
      "## Results at a glance\n",
      "\n",
      "- Transformer: New SOTA BLEU on WMT’14 En–De (28.4) and En–Fr (41.0); strong transfer to parsing.\n",
      "\n",
      "- BERT: SOTA across GLUE, SQuAD v1.1, SWAG; simple fine-tuning works broadly.\n",
      "\n",
      "- RoBERTa: Improves over BERT on GLUE (dev/test), SQuAD v1.1/v2.0, RACE; competitive with XLNet despite no NSP and unchanged architecture.\n",
      "\n",
      "## Strengths and limitations\n",
      "\n",
      "- Transformer (AIAIY)\n",
      "  - Strengths: Attention-only seq2seq with massive parallelism; efficient training; interpretable heads; great for generative transduction.\n",
      "  - Limits: O(n^2) attention cost; needs parallel data; autoregressive decoding is sequential at inference.\n",
      "\n",
      "- BERT\n",
      "  - Strengths: Universal bidirectional representations; strong fine-tuning baselines; minimal task-specific work.\n",
      "  - Limits: Expensive pretraining; NSP may not help; domain mismatch without continued pretraining; MLM is sample-inefficient.\n",
      "\n",
      "- RoBERTa\n",
      "  - Strengths: Shows data/compute and dynamic masking drive gains; robust NLU baselines; removes NSP without loss.\n",
      "  - Limits: Higher compute/data demands; English-centric; still O(n^2); not tailored for generative seq2seq.\n",
      "\n",
      "## Conceptual impact\n",
      "\n",
      "- Transformer: Establishes attention-only as a general backbone across NLP, vision, speech; encoder/decoder foundations for later models.\n",
      "\n",
      "- BERT: Popularizes pretrain-then-finetune; proves importance of bidirectionality; standardizes [CLS]/[SEP] formatting.\n",
      "\n",
      "- RoBERTa: Recalibrates the field toward training/data optimization; challenges NSP; provides stronger, reproducible baselines.\n",
      "\n",
      "## Assumptions and trade-offs\n",
      "\n",
      "- Transformer: Trades quadratic memory for global receptive field and throughput; best fit for seq2seq generation.\n",
      "\n",
      "- BERT: Assumes masked-token recovery yields transferable semantics; trades generative ability for discriminative strength.\n",
      "\n",
      "- RoBERTa: Assumes scaling and recipe refinements beat architectural novelty; trades accessibility for peak accuracy.\n",
      "\n",
      "## Practical guidance: when to use which\n",
      "\n",
      "- Need machine translation, summarization, data-to-text, or any seq2seq with cross-attention? Use a Transformer encoder–decoder (AIAIY-style). Modern variants add relative positions and better decoding, but the core holds.\n",
      "\n",
      "- Need strong NLU (classification, NER, extractive QA) with little task-specific engineering? Use a pretrained encoder. Prefer RoBERTa for stronger results if you can afford it; choose BERT for compatibility, smaller checkpoints, or tighter resource budgets.\n",
      "\n",
      "- Domain-specific tasks (biomedical, legal, code): Continue pretraining (domain-adaptive pretraining) on in-domain data for BERT/RoBERTa before fine-tuning.\n",
      "\n",
      "## Hands-on: dynamic masking for MLM in practice (RoBERTa vs. BERT)\n",
      "\n",
      "The snippet below shows a minimal masked language modeling setup using Hugging Face Transformers. Switching the model name flips between BERT and RoBERTa; DataCollatorForLanguageModeling enables dynamic masking (RoBERTa-style).\n",
      "\n",
      "```python\n",
      "# pip install transformers datasets accelerate -q\n",
      "from datasets import load_dataset\n",
      "from transformers import (AutoTokenizer, AutoModelForMaskedLM,\n",
      "                          DataCollatorForLanguageModeling, Trainer, TrainingArguments)\n",
      "\n",
      "# Toggle between 'roberta-base' (byte-level BPE, no NSP) and 'bert-base-uncased' (WordPiece)\n",
      "MODEL_NAME = \"roberta-base\"  # or \"bert-base-uncased\"\n",
      "\n",
      "# 1) Load a small text corpus (subset for a quick demo)\n",
      "raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
      "\n",
      "def tokenize(examples):\n",
      "    return tokenizer(examples[\"text\"], truncation=True, padding=False)\n",
      "\n",
      "tok = raw.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
      "\n",
      "# 2) Dynamic masking: new masks each batch/epoch (key to RoBERTa’s gains)\n",
      "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
      "\n",
      "# 3) Model and quick training loop (few steps for illustration)\n",
      "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
      "\n",
      "args = TrainingArguments(\n",
      "    output_dir=\"mlm-demo\",\n",
      "    per_device_train_batch_size=8,\n",
      "    learning_rate=5e-5,\n",
      "    num_train_epochs=1,\n",
      "    max_steps=100,       # keep tiny for demo\n",
      "    logging_steps=20,\n",
      "    report_to=\"none\",\n",
      ")\n",
      "\n",
      "trainer = Trainer(model=model, args=args, train_dataset=tok, data_collator=collator)\n",
      "trainer.train()\n",
      "\n",
      "# Tip:\n",
      "# - Using 'roberta-base' mirrors RoBERTa-style MLM (no NSP) with dynamic masking.\n",
      "# - Using 'bert-base-uncased' still benefits from dynamic masking here,\n",
      "#   but original BERT pretraining also included Next Sentence Prediction.\n",
      "```\n",
      "\n",
      "Notes:\n",
      "- NSP is not part of RoBERTa’s recipe. The Trainer above uses MLM only, which aligns with RoBERTa.\n",
      "- For task fine-tuning (e.g., SST-2, MRPC), swap the model head to AutoModelForSequenceClassification and load a GLUE dataset.\n",
      "\n",
      "## Reproducibility and resources\n",
      "\n",
      "- All three released code/models; RoBERTa also shared data resources and detailed training recipes.\n",
      "- Exact RoBERTa reproduction needs substantial hardware. Even so, smaller-scale replicas benefit from its recipe: dynamic masking, longer training, and larger batches (within your limits).\n",
      "\n",
      "## Open problems and extensions\n",
      "\n",
      "- Long-context efficiency: O(n^2) attention spurs sparse/linear attention and memory-augmented approaches.\n",
      "- Multilingual and low-resource transfer: mBERT and XLM-R extend the recipe; domain shifts remain tricky.\n",
      "- Generative pretraining: Encoder-only excels at NLU; encoder–decoder or decoder-only pretraining (e.g., T5, GPT) better suit generation—bridging both is active research.\n",
      "- Interpretability and robustness: Attention helps, but robust generalization under distribution shifts and adversarial inputs is still an open challenge.\n",
      "\n",
      "## Key takeaways\n",
      "\n",
      "- Transformer (AIAIY) unlocked parallel, attention-only seq2seq and set the blueprint for modern architectures.\n",
      "- BERT proved bidirectional encoder pretraining plus simple fine-tuning yields strong, general NLU performance.\n",
      "- RoBERTa showed that training/data scale and dynamic masking, not architectural changes, drove further NLU gains.\n",
      "\n",
      "## Potential applications\n",
      "\n",
      "- Transformer: Machine translation, abstractive summarization, code generation, speech recognition with seq2seq.\n",
      "- BERT/RoBERTa: Text classification, NER, extractive QA, sentence similarity, retrieval re-ranking.\n",
      "- With domain-adaptive pretraining: Biomedical/clinical NLP, legal document understanding, code intelligence, financial analysis.\n"
     ]
    }
   ],
   "source": [
    "# check `cache` folder\n",
    "print(result['final_report'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
